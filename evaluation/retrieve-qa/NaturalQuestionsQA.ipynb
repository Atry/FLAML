{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install flaml[retrievechat]~=2.0.0rc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/FLAML/docs/reference/autogen/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models to use:  ['gpt-4']\n"
     ]
    }
   ],
   "source": [
    "from flaml import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\".config.local\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4\",\n",
    "            \"gpt4\",\n",
    "            \"gpt-4-32k\",\n",
    "            \"gpt-4-32k-0314\",\n",
    "            \"gpt-35-turbo\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "assert len(config_list) > 0\n",
    "print(\"models to use: \", [config_list[i][\"model\"] for i in range(len(config_list))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct agents for RetrieveChat\n",
    "\n",
    "We start by initialzing the `RetrieveAssistantAgent` and `RetrieveUserProxyAgent`. The system message needs to be set to \"You are a helpful assistant.\" for RetrieveAssistantAgent. The detailed instructions are given in the user message. Later we will use the `RetrieveUserProxyAgent.generate_init_prompt` to combine the instructions and a math problem for an initial prompt to be sent to the LLM assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml.autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from flaml.autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "import chromadb\n",
    "\n",
    "# 1. create an RetrieveAssistantAgent instance named \"assistant\"\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\", \n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 600,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. create the RetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
    "corpus_file = \"https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/corpus.txt\"\n",
    "\n",
    "# Create a new collection for NaturalQuestions dataset\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    retrieve_config={\n",
    "        \"docs_path\": corpus_file,\n",
    "        \"chunk_token_size\": 4900,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"natural-questions\",\n",
    "        \"chunk_mode\": \"one_line\",\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Questions QA\n",
    "\n",
    "Use RetrieveChat to answer questions for [NaturalQuestion](https://ai.google.com/research/NaturalQuestions) dataset.\n",
    "\n",
    "We'll first create a new document collection based on all the context corpus, then we select some questions and answer them with RetrieveChat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2023-08-09 14:44:36--  https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/queries.jsonl\n",
      "Resolving huggingface.co (huggingface.co)... 13.32.50.53, 13.32.50.80, 13.32.50.129, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.32.50.53|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1380571 (1.3M) [text/plain]\n",
      "Saving to: ‘/tmp/chromadb/queries.jsonl’\n",
      "\n",
      "/tmp/chromadb/queri 100%[===================>]   1.32M  1.06MB/s    in 1.2s    \n",
      "\n",
      "2023-08-09 14:44:38 (1.06 MB/s) - ‘/tmp/chromadb/queries.jsonl’ saved [1380571/1380571]\n",
      "\n",
      "['what is non controlling interest on balance sheet', 'how many episodes are in chicago fire season 4', 'who sings love will keep us alive by the eagles', 'who is the leader of the ontario pc party', 'where did the last name keith come from']\n",
      "[[\"the portion of a subsidiary corporation 's stock that is not owned by the parent corporation\"], ['23'], ['Timothy B. Schmit'], ['Patrick Walter Brown'], ['from Keith in East Lothian , Scotland', \"from a nickname , derived from the Middle High German kīt , a word meaning `` sprout '' , `` offspring ''\"]]\n",
      "Number of questions: 6775\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "queries_file = \"https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/queries.jsonl\"\n",
    "!wget -O /tmp/chromadb/queries.jsonl $queries_file\n",
    "queries = [json.loads(line) for line in open(\"/tmp/chromadb/queries.jsonl\").readlines() if line]\n",
    "questions = [q[\"text\"] for q in queries]\n",
    "answers = [q[\"metadata\"][\"answer\"] for q in queries]\n",
    "print(questions[:5])\n",
    "print(answers[:5])\n",
    "print(\"Number of questions:\", len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO \n",
    "import sys\n",
    "\n",
    "class Capturing(list):\n",
    "    def __enter__(self):\n",
    "        self._stdout = sys.stdout\n",
    "        sys.stdout = self._stringio = StringIO()\n",
    "        return self\n",
    "    def __exit__(self, *args):\n",
    "        self.extend(self._stringio.getvalue().splitlines())\n",
    "        del self._stringio    # free up some memory\n",
    "        sys.stdout = self._stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_answers = []\n",
    "for qa_problem in questions[:200]:\n",
    "    assistant.reset()\n",
    "    with Capturing() as print_output:\n",
    "        ragproxyagent.initiate_chat(assistant, problem=qa_problem, n_results=3)\n",
    "    retrieve_answers.append(print_output[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Non-controlling interest, also known as minority interest, on a balance sheet refers to the portion of a subsidiary corporation's stock that is not owned by the parent corporation. It often makes up less than 50% of outstanding shares, as ownership above that threshold would generally cause the company to cease being a subsidiary.\", 'The fourth season of Chicago Fire contains 23 episodes.', '\"Love Will Keep Us Alive\" by the Eagles is sung by their bassist, Timothy B. Schmit.', 'Patrick Brown is the leader of the Ontario PC Party.', 'The last name Keith is of Scottish origin, derived from a place name in East Lothian, Scotland.']\n",
      "len(retrieve_answers): 145\n"
     ]
    }
   ],
   "source": [
    "print(retrieve_answers[:5])\n",
    "print(\"len(retrieve_answers):\", len(retrieve_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#F1\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1_recall(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens), int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec), rec\n",
    "\n",
    "def get_gold_answers(example):\n",
    "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\n",
    "    \n",
    "    gold_answers = [answer[\"text\"] for answer in example.answers if answer[\"text\"]]\n",
    "\n",
    "    # if gold_answers doesn't exist it's because this is a negative example - \n",
    "    # the only correct answer is an empty string\n",
    "    if not gold_answers:\n",
    "        gold_answers = [\"\"]\n",
    "        \n",
    "    return gold_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is non controlling interest on balance sheet\n",
      "Prediction: Non-controlling interest, also known as minority interest, on a balance sheet refers to the portion of a subsidiary corporation's stock that is not owned by the parent corporation. It often makes up less than 50% of outstanding shares, as ownership above that threshold would generally cause the company to cease being a subsidiary.\n",
      "True Answers: [\"the portion of a subsidiary corporation 's stock that is not owned by the parent corporation\"]\n",
      "EM: 0 \t F1: 0.36666666666666675 \t Recall: 0.8461538461538461\n",
      "Question: where did the last name keith come from\n",
      "Prediction: The last name Keith is of Scottish origin, derived from a place name in East Lothian, Scotland.\n",
      "True Answers: [\"a nickname , derived from the Middle High German kīt , a word meaning `` sprout '' , `` offspring ''\"]\n",
      "EM: 0 \t F1: 0.15384615384615383 \t Recall: 0.18181818181818182\n",
      "Question: converting stereo signal to mono signal is called\n",
      "Prediction: Converting stereo signal to mono signal is called \"downmixing.\"\n",
      "True Answers: ['Panning']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: who were the three elves who got rings\n",
      "Prediction: The three elves who got rings in Tolkien's legendarium are Galadriel, Gil-galad, and Círdan.\n",
      "True Answers: ['Gil - galad', 'Círdan', 'Galadriel']\n",
      "EM: 0 \t F1: 0.14285714285714288 \t Recall: 1.0\n",
      "Question: when is the new tappan zee bridge going to be finished\n",
      "Prediction: The new Tappan Zee Bridge, now known as the Governor Mario M. Cuomo Bridge, was completed in 2017.\n",
      "True Answers: ['by June 15 , 2018']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: who has been chosen as the brand ambassador of the campaign ' beti bachao-beti padhao\n",
      "Prediction: Madhuri Dixit has been chosen as the brand ambassador for the campaign 'Beti Bachao-Beti Padhao'.\n",
      "True Answers: ['Sakshi Malik']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: when is the new tappan zee bridge going to be finished\n",
      "Prediction: The new Tappan Zee Bridge, now known as the Governor Mario M. Cuomo Bridge, was completed in 2017.\n",
      "True Answers: ['June 15 , 2018']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: who has been chosen as the brand ambassador of the campaign ' beti bachao-beti padhao\n",
      "Prediction: Madhuri Dixit has been chosen as the brand ambassador for the campaign 'Beti Bachao-Beti Padhao'.\n",
      "True Answers: ['Olympics 2016 bronze medallist Sakshi Malik']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: who makes the decisions about what to produce in a market economy\n",
      "Prediction: In a market economy, decisions about what to produce are made by individuals and businesses based on supply and demand factors.\n",
      "True Answers: ['capital and financial markets']\n",
      "EM: 0 \t F1: 0.08333333333333334 \t Recall: 0.25\n",
      "Question: in order to prove disparate impact you first must establish\n",
      "Prediction: In order to prove disparate impact, you first must establish a prima facie case by showing that a specific policy or practice has a disproportionately adverse effect on members of a protected group.\n",
      "True Answers: ['practices in employment , housing , and other areas that adversely affect one group of people of a protected characteristic more than another , even though rules applied by employers or landlords are formally neutral']\n",
      "EM: 0 \t F1: 0.23333333333333334 \t Recall: 0.22580645161290322\n",
      "Question: where do characters live in this is us\n",
      "Prediction: The characters in \"This Is Us\" primarily live in various locations within the United States, but the provided context does not mention specific locations for the show.\n",
      "True Answers: ['Pittsburgh', 'Los Angeles', 'New Jersey', 'New York City']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: who is the leader of the ontario pc party\n",
      "Prediction: Patrick Brown is the leader of the Ontario PC Party.\n",
      "True Answers: ['Patrick Brown']\n",
      "EM: 0 \t F1: 0.4 \t Recall: 1.0\n",
      "Question: how many seasons of prison break are on netflix\n",
      "Prediction: There are 5 seasons of Prison Break available on Netflix.\n",
      "True Answers: ['five']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: when did they stop cigarette advertising on television\n",
      "Prediction: Cigarette advertising on television was banned in the United States on January 2, 1971.\n",
      "True Answers: ['1970']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: who is the number one ranked golfer in the world right now\n",
      "Prediction: \n",
      "True Answers: ['Dustin Johnson']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: latest season on keeping up with the kardashians\n",
      "Prediction: The latest season of Keeping Up with the Kardashians is not mentioned in the provided context. Please update the context or provide more information.\n",
      "True Answers: ['fourteen']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: when did the us take over wake island\n",
      "Prediction: The US took over Wake Island on January 17, 1899.\n",
      "True Answers: ['July 4 , 1898']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: who has the oldest team in the nba\n",
      "Prediction: \n",
      "True Answers: ['New York Knickerbockers']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: latest season on keeping up with the kardashians\n",
      "Prediction: The latest season of Keeping Up with the Kardashians is not mentioned in the provided context. Please update the context or provide more information.\n",
      "True Answers: ['14']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: how dose the poet present death as a voyage in crossing the bar\n",
      "Prediction: In \"Crossing the Bar\", the poet presents death as a voyage by using metaphors of sailing and tides. The lines \"Sunset and evening star, And one clear call for me! And may there be no moaning of the bar, When I put out to sea\" demonstrate the poet's depiction of death as a journey across the sea, with the evening star symbolizing the end of life and the \"moaning of the bar\" referring to the reluctance of leaving this world.\n",
      "True Answers: [\"uses an extended metaphor to compare death with crossing the `` sandbar '' between river of life , with its outgoing `` flood '' , and the ocean that lies beyond ( death ) , the `` boundless deep '' , to which we return\"]\n",
      "EM: 0 \t F1: 0.14583333333333331 \t Recall: 0.2413793103448276\n",
      "Question: who were farmers who kept a small portion of their crops & gave the rest to the landowners\n",
      "Prediction: Farmers who kept a small portion of their crops and gave the rest to the landowners were called sharecroppers.\n",
      "True Answers: ['Sharecropping']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: what is the meaning of the name comanche\n",
      "Prediction: The meaning of the name Comanche is not directly related to the provided context. The Comanche are a Native American tribe whose name comes from the Ute word \"Kimantsi,\" meaning \"enemy\" or \"stranger.\"\n",
      "True Answers: [\"`` Comanche '' is from the Ute name for them , kɨmantsi ( enemy )\"]\n",
      "EM: 0 \t F1: 0.3333333333333333 \t Recall: 0.6666666666666666\n",
      "Question: 2 . what are the reasons states impose protectionists policies on other countries\n",
      "Prediction: 5. To maintain national security by protecting strategically important industries like defense, energy, and agriculture.\n",
      "True Answers: ['shield the producers , businesses , and workers of the import - competing sector in the country from foreign competitors']\n",
      "EM: 0 \t F1: 0.0689655172413793 \t Recall: 0.07142857142857142\n",
      "Question: what event provoked congress to propose the eleventh amendment and the states to ratify it\n",
      "Prediction: The Eleventh Amendment was proposed by Congress in response to the U.S. Supreme Court's decision in Chisholm v. Georgia (1793), which allowed citizens of one state to sue another state in federal court. The Eleventh Amendment limited the jurisdiction of federal courts by preventing them from hearing cases against a state in which the state is sued by citizens of another state or a foreign nation.\n",
      "True Answers: [\"the Supreme Court 's ruling in Chisholm v. Georgia , 2 U.S. 419 ( 1793 )\"]\n",
      "EM: 0 \t F1: 0.22535211267605634 \t Recall: 0.6666666666666666\n",
      "Question: what is the meaning of the dragon boat festival\n",
      "Prediction: The Dragon Boat Festival is a traditional Chinese holiday that commemorates the life and death of the famous Chinese scholar Qu Yuan. The festival occurs on the fifth day of the fifth month of the lunar calendar and involves boat races in the shape of dragons and eating zongzi, a kind of sticky rice dumpling.\n",
      "True Answers: ['commemorating fealty and filial piety']\n",
      "EM: 0 \t F1: 0.039999999999999994 \t Recall: 0.2\n",
      "Question: who is edmund on days of our lives\n",
      "Prediction: Edmund is not a character on Days of Our Lives. The table in the context provided appears to be about Australian politicians, not related to the soap opera.\n",
      "True Answers: ['Adam Caine']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: what are bulls used for on a farm\n",
      "Prediction: Bulls are used on a farm for breeding purposes, helping to produce offspring and maintain the livestock population.\n",
      "True Answers: ['herd maintenance']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: royal society for the protection of birds number of members\n",
      "Prediction: From my available information, the Royal Society for the Protection of Birds (RSPB) has over 1 million members.\n",
      "True Answers: ['more than a million members ( including 195,000 youth members )']\n",
      "EM: 0 \t F1: 0.16666666666666666 \t Recall: 0.25\n",
      "Question: who sings war do n 't let me down\n",
      "Prediction: The song \"War\" with the lyrics \"Don't Let Me Down\" is not mentioned in the provided context. Please provide more information or clarify the question.\n",
      "True Answers: ['American production duo The Chainsmokers', 'vocals of American singer Daya']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: where was the movie jeremiah johnson filmed at\n",
      "Prediction: The movie \"Jeremiah Johnson\" was filmed in various locations in Utah, including Ashley National Forest, Uinta Mountains, and the Zion National Park.\n",
      "True Answers: ['in nearly 100 locations across Utah , including : Mount Timpanogos , Ashley National Forest , Leeds , Snow Canyon State Park , St. George , Sundance Resort , Uinta National Forest , Wasatch - Cache National Forest , and Zion National Park']\n",
      "EM: 0 \t F1: 0.42307692307692313 \t Recall: 0.34375\n",
      "Question: when does jenny humphrey come back to gossip girl\n",
      "Prediction: Jenny Humphrey, played by Taylor Momsen, returns to Gossip Girl in Season 4, Episode 10.\n",
      "True Answers: ['the final episode of the series']\n",
      "EM: 0 \t F1: 0.10526315789473685 \t Recall: 0.25\n",
      "Question: when was theme from a summer place released\n",
      "Prediction: \"Theme from A Summer Place\" was released in September 1959 as a single by Percy Faith and his Orchestra.\n",
      "True Answers: ['1960']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: what degree is a crock pot on low\n",
      "Prediction: A crock pot on low is typically around 190°F (88°C).\n",
      "True Answers: ['71 -- 74 ° C ( 160 -- 165 ° F )']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: who performed the first c section in 1794\n",
      "Prediction: The context provided does not include information about the first C-section performed in 1794. Please provide additional context or relevant information.\n",
      "True Answers: ['Dr. Jesse Bennett']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: how long prime minister stay in office canada\n",
      "Prediction: In Canada, there is no fixed term for a prime minister. They can stay in office as long as their party maintains the confidence of the House of Commons and they remain the leader of the party.\n",
      "True Answers: ['until he or she resigns , is dismissed , or dies']\n",
      "EM: 0 \t F1: 0.04878048780487805 \t Recall: 0.1111111111111111\n",
      "Question: how long prime minister stay in office canada\n",
      "Prediction: In Canada, there is no fixed term for a prime minister. They can stay in office as long as their party maintains the confidence of the House of Commons and they remain the leader of the party.\n",
      "True Answers: ['four years']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: when does the new season of lost in space come out\n",
      "Prediction: \n",
      "True Answers: ['April 13 , 2018']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: where did aeneas go when he left carthage\n",
      "Prediction: After leaving Carthage, Aeneas went to Italy, where he eventually founded the city of Rome.\n",
      "True Answers: ['Sicily']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: when did you have to have a driver 's license\n",
      "Prediction: \n",
      "True Answers: ['to operate one or more types of motorized vehicles , such as a motorcycle , car , truck , or bus on a public road']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: who plays general hux in the last jedi\n",
      "Prediction: Domhnall Gleeson plays General Hux in The Last Jedi.\n",
      "True Answers: ['Domhnall Gleeson']\n",
      "EM: 0 \t F1: 0.4 \t Recall: 1.0\n",
      "Question: who has participated in the most super bowls\n",
      "Prediction: Bill Belichick has participated in the most Super Bowls, with a total of 7 appearances.\n",
      "True Answers: ['New England Patriots']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: who sang in the movie i walk the line\n",
      "Prediction: John C. Reilly sang on all the tracks and played guitar on most of them in the movie \"I Walk the Line.\"\n",
      "True Answers: ['Joaquin Phoenix', 'Reese Witherspoon', 'Waylon Payne', 'Johnathan Rice', 'Tyler Hilton', 'Shooter Jennings']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: when was the seat moved to the capitol\n",
      "Prediction: The Australian Capital Territory's capital, Canberra, became the capital in 1913.\n",
      "True Answers: ['December 1800']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: how many students attend university of texas at austin\n",
      "Prediction: There are 51,331 students enrolled at the University of Texas at Austin in Fall 2016.\n",
      "True Answers: ['over 50,000 undergraduate and graduate students']\n",
      "EM: 0 \t F1: 0.1 \t Recall: 0.16666666666666666\n",
      "Question: when was the last time the military drafted\n",
      "Prediction: The last time the US military drafted was during the Vietnam War, ending in 1973.\n",
      "True Answers: ['1972']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: when does isle of dogs come out in us\n",
      "Prediction: Isle of Dogs was released in the US on March 23, 2018.\n",
      "True Answers: ['March 23 , 2018']\n",
      "EM: 0 \t F1: 0.42857142857142855 \t Recall: 1.0\n",
      "Question: where was the capital of the habsburg empire located\n",
      "Prediction: The capital of the Habsburg Empire was located in Vienna (1526-1583) and (1611-1804), and in Prague (1583-1611).\n",
      "True Answers: ['Vienna , except from 1583 to 1611 , when it was moved to Prague']\n",
      "EM: 0 \t F1: 0.22222222222222224 \t Recall: 0.25\n",
      "Question: where was the flag placed on the moon\n",
      "Prediction: The flag you are referring to is the United States flag, which was placed on the moon during the Apollo 11 mission in 1969.\n",
      "True Answers: ['Sea of Tranquility']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: when was the seat moved to the capitol\n",
      "Prediction: The Australian Capital Territory's capital, Canberra, became the capital in 1913.\n",
      "True Answers: ['1800']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: where was the flag placed on the moon\n",
      "Prediction: The flag you are referring to is the United States flag, which was placed on the moon during the Apollo 11 mission in 1969.\n",
      "True Answers: ['Sea of Tranquility', 'Ocean of Storms', 'Fra Mauro formation', 'Hadley -- Apennine', 'Descartes Highlands', 'Taurus -- Littrow']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: when does the new gotham season come out\n",
      "Prediction: The context provided does not have information about when the new Gotham season is coming out. Please provide more context or the specific season you are referring to.\n",
      "True Answers: ['September 21 , 2017']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: when was the seat moved to the capitol\n",
      "Prediction: The Australian Capital Territory's capital, Canberra, became the capital in 1913.\n",
      "True Answers: ['November 17 , 1800']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: when was the last time the military drafted\n",
      "Prediction: The last time the US military drafted was during the Vietnam War, ending in 1973.\n",
      "True Answers: ['December 1972']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: how many students attend university of texas at austin\n",
      "Prediction: There are 51,331 students enrolled at the University of Texas at Austin in Fall 2016.\n",
      "True Answers: ['over 50,000']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: who starred in the remake of true grit\n",
      "Prediction: The remake of \"True Grit\" (2010) starred Jeff Bridges as Rooster Cogburn, Hailee Steinfeld as Mattie Ross, Matt Damon as LaBoeuf, and Josh Brolin as Tom Chaney.\n",
      "True Answers: ['Hailee Steinfeld', 'Jeff Bridges', 'Matt Damon', 'Josh Brolin', 'Barry Pepper']\n",
      "EM: 0 \t F1: 0.14285714285714288 \t Recall: 1.0\n",
      "Question: who played tom on as the world turns\n",
      "Prediction: I am unable to answer your question about \"who played Tom on As the World Turns\" using the provided context. Please UPDATE CONTEXT or provide more information.\n",
      "True Answers: ['Peter Galman ( 1969 -- 73 )', 'C. David Colson ( 1973 -- 78 )', 'Tom Tammi ( 1979 -- 80 )', 'Justin Deas ( 1980 -- 84 )', 'Jason Kincaid ( 1984 )', 'Gregg Marx ( 1984 -- 86 )', 'Scott Holmes ( 1987 -- 2010 )']\n",
      "EM: 0 \t F1: 0.06896551724137932 \t Recall: 0.25\n",
      "Question: which country has won maximum number of gold medal in asian game 2014\n",
      "Prediction: The context provided does not contain information on the country with the maximum number of gold medals in the 2014 Asian Games. Please UPDATE CONTEXT with the information related to the 2014 Asian Games.\n",
      "True Answers: ['China']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: when do willow and tara get back together\n",
      "Prediction: Willow and Tara get back together in Buffy the Vampire Slayer in Season 6, Episode 19, \"Seeing Red.\"\n",
      "True Answers: ['Entropy']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: what year did the price is right first air\n",
      "Prediction: The Price is Right first aired in 1956.\n",
      "True Answers: ['1956']\n",
      "EM: 0 \t F1: 0.25 \t Recall: 1.0\n",
      "Question: what is the longest panic at the disco song title\n",
      "Prediction: The longest Panic! at the Disco song title is \"Lying Is the Most Fun a Girl Can Have Without Taking Her Clothes Off.\"\n",
      "True Answers: ['Bohemian Rhapsody']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: which best describes timbuktu under the mali empire\n",
      "Prediction: Timbuktu under the Mali Empire is best described as a prosperous and significant cultural, educational, and trade center.\n",
      "True Answers: ['a place of trade , entertainment , and education']\n",
      "EM: 0 \t F1: 0.18181818181818182 \t Recall: 0.3333333333333333\n",
      "Question: which government had more power under the articles of confederation\n",
      "Prediction: Under the Articles of Confederation, state governments had more power than the federal government.\n",
      "True Answers: ['states']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: how many super bowl games has the patriots played in\n",
      "Prediction: The New England Patriots have played in 11 Super Bowl games.\n",
      "True Answers: ['ten']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: who played junior on in the heat of the night\n",
      "Prediction: Alan Autry played the role of Captain Bubba Skinner's nephew (Lorraine Toussaint), a.k.a. Junior, in the TV show \"In the Heat of the Night.\"\n",
      "True Answers: ['Christian LeBlanc']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "=======================================\n",
      "Average EM: 0.0\n",
      "Average F1: 0.17113575754109359\n",
      "Average Recall: 0.5484944253290597\n"
     ]
    }
   ],
   "source": [
    "all_em_scores = []\n",
    "all_f1_scores = []\n",
    "all_recall_scores = []\n",
    "for i in range(len(retrieve_answers)):\n",
    "    prediction = retrieve_answers[i]\n",
    "    gold_answers = answers[i]\n",
    "\n",
    "    em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
    "    f1_score = max((compute_f1_recall(prediction, answer)[0]) for answer in gold_answers)\n",
    "    recall_score = max((compute_f1_recall(prediction, answer)[1]) for answer in gold_answers)\n",
    "\n",
    "    all_em_scores.append(em_score)\n",
    "    all_f1_scores.append(f1_score)\n",
    "    all_recall_scores.append(recall_score)\n",
    "\n",
    "    if i % 10 == 0 or recall_score < 0.3:\n",
    "        print(f\"Question: {questions[i]}\")\n",
    "        print(f\"Prediction: {prediction}\")\n",
    "        print(f\"True Answers: {gold_answers}\")\n",
    "        print(f\"EM: {em_score} \\t F1: {f1_score} \\t Recall: {recall_score}\")\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(f\"Average EM: {sum(all_em_scores) / len(all_em_scores)}\")\n",
    "print(f\"Average F1: {sum(all_f1_scores) / len(all_f1_scores)}\")\n",
    "print(f\"Average Recall: {sum(all_recall_scores) / len(all_recall_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
