{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"pyautogen[retrievechat]~=0.2.0b5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/FLAML/docs/reference/autogen/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijiang1/anaconda3/envs/autogen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models to use:  ['gpt-35-turbo']\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\".config.local\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4\",\n",
    "            \"gpt4\",\n",
    "            \"gpt-4-32k\",\n",
    "            \"gpt-4-32k-0314\",\n",
    "            \"gpt-35-turbo\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "assert len(config_list) > 0\n",
    "config_list[0]['model'] = 'gpt-35-turbo'\n",
    "print(\"models to use: \", [config_list[i][\"model\"] for i in range(len(config_list))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct agents for RetrieveChat\n",
    "\n",
    "We start by initialzing the `RetrieveAssistantAgent` and `RetrieveUserProxyAgent`. The system message needs to be set to \"You are a helpful assistant.\" for RetrieveAssistantAgent. The detailed instructions are given in the user message. Later we will use the `RetrieveUserProxyAgent.generate_init_prompt` to combine the instructions and a math problem for an initial prompt to be sent to the LLM assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lijiang1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lijiang1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_common_words(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Get the list of English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Filter out common words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Reconstruct the text without common words\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijiang1/anaconda3/envs/autogen/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "from autogen.retrieve_utils import create_vector_db_from_dir, query_vector_db\n",
    "import chromadb\n",
    "\n",
    "# # Use this class to clean the question before searching\n",
    "# class CleanRetrieveUserProxyAgent(RetrieveUserProxyAgent):\n",
    "#     def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\"):\n",
    "#         problem = remove_common_words(problem)\n",
    "#         if not self._collection or not self._get_or_create:\n",
    "#             print(\"Trying to create collection.\")\n",
    "#             self._client = create_vector_db_from_dir(\n",
    "#                 dir_path=self._docs_path,\n",
    "#                 max_tokens=self._chunk_token_size,\n",
    "#                 client=self._client,\n",
    "#                 collection_name=self._collection_name,\n",
    "#                 chunk_mode=self._chunk_mode,\n",
    "#                 must_break_at_empty_line=self._must_break_at_empty_line,\n",
    "#                 embedding_model=self._embedding_model,\n",
    "#                 get_or_create=self._get_or_create,\n",
    "#                 embedding_function=self._embedding_function,\n",
    "#                 custom_text_split_function=self.custom_text_split_function,\n",
    "#                 custom_text_types=self._custom_text_types,\n",
    "#                 recursive=self._recursive,\n",
    "#             )\n",
    "#             self._collection = True\n",
    "#             self._get_or_create = True\n",
    "\n",
    "#         results = query_vector_db(\n",
    "#             query_texts=[problem],\n",
    "#             n_results=n_results,\n",
    "#             search_string=search_string,\n",
    "#             client=self._client,\n",
    "#             collection_name=self._collection_name,\n",
    "#             embedding_model=self._embedding_model,\n",
    "#             embedding_function=self._embedding_function,\n",
    "#         )\n",
    "#         self._search_string = search_string\n",
    "#         self._results = results\n",
    "#         print(\"doc_ids: \", results[\"ids\"])\n",
    "\n",
    "\n",
    "# 1. create an RetrieveAssistantAgent instance named \"assistant\"\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\", \n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"timeout\": 60,\n",
    "        \"cache_seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. create the RetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
    "corpus_file = \"https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/corpus.txt\"\n",
    "\n",
    "# Create a new collection for NaturalQuestions dataset\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": corpus_file,\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"natural-questions\",\n",
    "        \"chunk_mode\": \"one_line\",\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        \"get_or_create\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Questions QA\n",
    "\n",
    "Use RetrieveChat to answer questions for [NaturalQuestion](https://ai.google.com/research/NaturalQuestions) dataset.\n",
    "\n",
    "We'll first create a new document collection based on all the context corpus, then we select some questions and answer them with RetrieveChat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-21 17:37:13--  https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/queries.jsonl\n",
      "Resolving huggingface.co (huggingface.co)... 99.84.108.129, 99.84.108.55, 99.84.108.70, ...\n",
      "Connecting to huggingface.co (huggingface.co)|99.84.108.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1380571 (1.3M) [text/plain]\n",
      "Saving to: ‘/tmp/chromadb/queries.jsonl’\n",
      "\n",
      "/tmp/chromadb/queri 100%[===================>]   1.32M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2023-11-21 17:37:13 (91.2 MB/s) - ‘/tmp/chromadb/queries.jsonl’ saved [1380571/1380571]\n",
      "\n",
      "['non controlling interest balance sheet', 'many episodes chicago fire season 4', 'sings love keep us alive eagles', 'leader ontario pc party', 'last name keith come']\n",
      "[[\"the portion of a subsidiary corporation 's stock that is not owned by the parent corporation\"], ['23'], ['Timothy B. Schmit'], ['Patrick Walter Brown'], ['from Keith in East Lothian , Scotland', \"from a nickname , derived from the Middle High German kīt , a word meaning `` sprout '' , `` offspring ''\"]]\n",
      "Number of questions: 6775\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "queries_file = \"https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/queries.jsonl\"\n",
    "!wget -O /tmp/chromadb/queries.jsonl $queries_file\n",
    "queries = [json.loads(line) for line in open(\"/tmp/chromadb/queries.jsonl\").readlines() if line]\n",
    "questions = [remove_common_words(q[\"text\"]) for q in queries]\n",
    "answers = [q[\"metadata\"][\"answer\"] for q in queries]\n",
    "print(questions[:5])\n",
    "print(answers[:5])\n",
    "print(\"Number of questions:\", len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO \n",
    "import sys\n",
    "\n",
    "class Capturing(list):\n",
    "    def __enter__(self):\n",
    "        self._stdout = sys.stdout\n",
    "        sys.stdout = self._stringio = StringIO()\n",
    "        return self\n",
    "    def __exit__(self, *args):\n",
    "        self.extend(self._stringio.getvalue().splitlines())\n",
    "        del self._stringio    # free up some memory\n",
    "        sys.stdout = self._stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 0.00%, Time Used 0.00 hours\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Film </Th> <Th> Year </Th> <Th> Fuck count </Th> <Th> Minutes </Th> <Th> Uses / mi ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Character </Th> <Th> Ultimate Avengers </Th> <Th> Ultimate Avengers 2 </Th> <Th> I ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Position </Th> <Th> Country </Th> <Th> Town / City </Th> <Th> PM2. 5 </Th> <Th> PM ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Country ( or dependent territory ) </Th> <Th> Population </Th> <Th ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> State </Th> <Th> Gross collections ( in thousands ) </Th> <Th> Rev ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Date </Th> <Th> Province </Th> <Th> Mag . </Th> <Th> MMI </Th> <Th> Deaths </Th> < ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> City </Th> <Th> River </Th> <Th> State </Th> </Tr> <Tr> <Td> Gangakhed </Td> <Td>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Player </Th> <Th> Pos . </Th> <Th> Team </Th> <Th> Career start </Th> <Th> Career  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> ABO and Rh blood type distribution by country ( population averages ) <Tr> <Th> Country </Th ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> </Th> <Th colspan=\"3\"> Total area </Th> <Th colspan=\"4\"> Land area </Th> <Th colsp ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> Performance in the European Cup and UEFA Champions League by club <Tr> <Th> <Ul> <Li> </Li>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> City </Th> <Th> State </Th> <Th> Land area ( sq mi ) </Th> <Th> La ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> # </Th> <Th> Country </Th> <Th> Name </Th> <Th> International goals </Th> <Th> Cap ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> City </Th> <Th> Image </Th> <Th> Population </Th> <Th> Definition  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Team </Th> <Th> Won </Th> <Th> Lost </Th> <Th> Tied </Th> <Th> Pct ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Territory </Th> <Th> Rights holder </Th> <Th> Ref </Th> </Tr> <Tr> <Td> Asia </Td> ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> ( hide ) Rank </Th> <Th> Nat </Th> <Th> Name </Th> <Th> Years </Th> <Th> Goals </T ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> </Th> <Th colspan=\"3\"> Total area </Th> <Th colspan=\"4\"> Land area </Th> <Th colsp ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th colspan=\"2\"> Bids by school </Th> <Th colspan=\"2\"> Most recent </Th> <Th colspan=\"2 ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Name </Th> <Th> Nation </Th> <Th> TP </Th> <Th colspan=\"2\"> SP </T ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> 2014 Rank </Th> <Th> City </Th> <Th> 2014 Estimate </Th> <Th> 2010 Census </Th> <T ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> S.No . </Th> <Th> Year </Th> <Th> Name </Th> </Tr> <Tr> <Td> </Td> <Td> 1961 </Td> ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> Densities of various materials covering a range of values <Tr> <Th> Material </Th> <Th> ρ (  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Club </Th> <Th> Season </Th> <Th colspan=\"3\"> League </Th> <Th colspan=\"2\"> Nation ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank ( 2016 ) </Th> <Th> Airports ( large hubs ) </Th> <Th> IATA Code </Th> <Th> M ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> City </Th> <Th> Region / State </Th> <Th> Country </Th> <Th> Park name </Th> <Th>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Year </Th> <Th> Winner ( nationally ) </Th> <Th> Votes </Th> <Th> Percent </Th> <T ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Compound </Th> <Th> SERT </Th> <Th> NET </Th> <Th> DAT </Th> <Th> 5 - HT </Th> <Th ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Name </Th> <Th> Industry </Th> <Th> Revenue ( USD millions ) </Th> ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Name </Th> <Th> Name in Georgian </Th> <Th> Population 1989 </Th>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Country </Th> <Th colspan=\"2\"> The World Factbook </Th> <Th colspan=\"2\"> World Res ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Country </Th> <Th> Area ( km2 ) </Th> <Th> Notes </Th> </Tr> <Tr>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Country </Th> <Th> Area ( km2 ) </Th> <Th> Notes </Th> </Tr> <Tr>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Date </Th> <Th> State ( s ) </Th> <Th> Magnitude </Th> <Th> Fatalities </Th> <Th>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Artist </Th> <Th> # Gold </Th> <Th> # Platinum </Th> <Th> # Multi-Platinum </Th> < ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> </Th> <Th colspan=\"2\"> Name </Th> <Th> Number of locations </Th> <Th> Revenue </Th ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> </Th> <Th> Name </Th> <Th> Country </Th> <Th> Region </Th> <Th> Depth ( meters ) < ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Player ( 2017 HRs ) </Th> <Th> HR </Th> </Tr> <Tr> <Td> </Td> <Td> ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> No . </Th> <Th> Athlete </Th> <Th> Nation </Th> <Th> Sport </Th> <Th> Years </Th>  ...\n",
      "INFO:autogen.retrieve_utils:Found 5369 chunks.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "retrieve_answers = []\n",
    "questions_sample = []\n",
    "answers_sample = []\n",
    "num_questions = 100\n",
    "st = time.time()\n",
    "for idx, qa_problem in enumerate(questions[:num_questions]):\n",
    "    if idx % 100 == 0:\n",
    "        ct = time.time()\n",
    "        print(f\"\\nProgress {idx/num_questions*100:.2f}%, Time Used {(ct-st)/3600:.2f} hours\\n\")\n",
    "    assistant.reset()\n",
    "    try:\n",
    "        with Capturing() as print_output:\n",
    "            ragproxyagent.initiate_chat(assistant, problem=qa_problem, n_results=10)\n",
    "        retrieve_answers.append(print_output[-3])\n",
    "        questions_sample.append(qa_problem)\n",
    "        answers_sample.append(answers[:num_questions][idx])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error in problem: \", qa_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Non controlling interest balance sheet refers to the portion of a subsidiary corporation's stock that is not owned by the parent corporation shown in the balance sheet as an equity item.\", 'There are 23 episodes in season 4 of Chicago Fire.', '\"Love Will Keep Us Alive\" is a song performed by the Eagles.', \"The leader of the Ontario PC Party and Ontario's Leader of the Official Opposition is Patrick Brown.\", 'Duffy.']\n",
      "len(retrieve_answers): 100\n",
      "len(answers_sample): 100\n",
      "len(questions_sample): 100\n"
     ]
    }
   ],
   "source": [
    "print(retrieve_answers[:5])\n",
    "print(\"len(retrieve_answers):\", len(retrieve_answers))\n",
    "print(\"len(answers_sample):\", len(answers_sample))\n",
    "print(\"len(questions_sample):\", len(questions_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#F1\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1_recall(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens), int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec), rec\n",
    "\n",
    "def get_gold_answers(example):\n",
    "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\n",
    "    \n",
    "    gold_answers = [answer[\"text\"] for answer in example.answers if answer[\"text\"]]\n",
    "\n",
    "    # if gold_answers doesn't exist it's because this is a negative example - \n",
    "    # the only correct answer is an empty string\n",
    "    if not gold_answers:\n",
    "        gold_answers = [\"\"]\n",
    "        \n",
    "    return gold_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: non controlling interest balance sheet\n",
      "Prediction: Non controlling interest balance sheet refers to the portion of a subsidiary corporation's stock that is not owned by the parent corporation shown in the balance sheet as an equity item.\n",
      "True Answers: [\"the portion of a subsidiary corporation 's stock that is not owned by the parent corporation\"]\n",
      "EM: 0 \t F1: 0.5641025641025641 \t Recall: 0.8461538461538461\n",
      "Question: many episodes chicago fire season 4\n",
      "Prediction: There are 23 episodes in season 4 of Chicago Fire.\n",
      "True Answers: ['23']\n",
      "EM: 0 \t F1: 0.18181818181818182 \t Recall: 1.0\n",
      "Question: sings love keep us alive eagles\n",
      "Prediction: \"Love Will Keep Us Alive\" is a song performed by the Eagles.\n",
      "True Answers: ['Timothy B. Schmit']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: leader ontario pc party\n",
      "Prediction: The leader of the Ontario PC Party and Ontario's Leader of the Official Opposition is Patrick Brown.\n",
      "True Answers: ['Patrick Walter Brown']\n",
      "EM: 0 \t F1: 0.23529411764705882 \t Recall: 0.6666666666666666\n",
      "Question: last name keith come\n",
      "Prediction: Duffy.\n",
      "True Answers: ['from Keith in East Lothian , Scotland', \"from a nickname , derived from the Middle High German kīt , a word meaning `` sprout '' , `` offspring ''\"]\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: last name keith come\n",
      "Prediction: Duffy.\n",
      "True Answers: [\"a nickname , derived from the Middle High German kīt , a word meaning `` sprout '' , `` offspring ''\"]\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: leader ontario pc party\n",
      "Prediction: The leader of the Ontario PC Party and Ontario's Leader of the Official Opposition is Patrick Brown.\n",
      "True Answers: ['Patrick Walter Brown MPP']\n",
      "EM: 0 \t F1: 0.22222222222222224 \t Recall: 0.5\n",
      "Question: converting stereo signal mono signal called\n",
      "Prediction: Converting a stereo signal to a mono signal is called panning straight up, which creates a dual mono signal.\n",
      "True Answers: ['Panning']\n",
      "EM: 0 \t F1: 0.11764705882352941 \t Recall: 1.0\n",
      "Question: plays max voice goofy movie\n",
      "Prediction: I'm sorry, I cannot find a question in the provided context. Please provide a question for me to answer.\n",
      "True Answers: ['Jason Marsden']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: sings love keep us alive eagles\n",
      "Prediction: \"Love Will Keep Us Alive\" is a song performed by the Eagles.\n",
      "True Answers: ['lead vocals by bassist Timothy B. Schmit']\n",
      "EM: 0 \t F1: 0.11764705882352941 \t Recall: 0.14285714285714285\n",
      "Question: three elves got rings\n",
      "Prediction: Three elves received Rings of Power from Celebrimbor: Gil-galad got Narya and Vilya, while Cirdan got Narya. Galadriel received Nenya from the start.\n",
      "True Answers: ['Gil - galad', 'Círdan', 'Galadriel']\n",
      "EM: 0 \t F1: 0.08695652173913045 \t Recall: 1.0\n",
      "Question: new tappan zee bridge going finished\n",
      "Prediction: Both spans of the new Tappan Zee Bridge are expected to be operational by June 15, 2018.\n",
      "True Answers: ['by June 15 , 2018']\n",
      "EM: 0 \t F1: 0.4 \t Recall: 1.0\n",
      "Question: chosen brand ambassador campaign ' beti bachao-beti padhao\n",
      "Prediction: Sakshi Malik was made brand ambassador for BBBP campaign.\n",
      "True Answers: ['Sakshi Malik']\n",
      "EM: 0 \t F1: 0.3636363636363636 \t Recall: 1.0\n",
      "Question: new tappan zee bridge going finished\n",
      "Prediction: Both spans of the new Tappan Zee Bridge are expected to be operational by June 15, 2018.\n",
      "True Answers: ['June 15 , 2018']\n",
      "EM: 0 \t F1: 0.3157894736842105 \t Recall: 1.0\n",
      "Question: chosen brand ambassador campaign ' beti bachao-beti padhao\n",
      "Prediction: Sakshi Malik was made brand ambassador for BBBP campaign.\n",
      "True Answers: ['Olympics 2016 bronze medallist Sakshi Malik']\n",
      "EM: 0 \t F1: 0.26666666666666666 \t Recall: 0.3333333333333333\n",
      "Question: makes decisions produce market economy\n",
      "Prediction: In a market economy, decisions regarding investment, production, and distribution are based on the interplay of supply and demand, which determines the prices of goods and services.\n",
      "True Answers: ['capital and financial markets']\n",
      "EM: 0 \t F1: 0.07142857142857142 \t Recall: 0.25\n",
      "Question: order prove disparate impact first must establish\n",
      "Prediction: To establish a prove of disparate impact, it is necessary to identify a neutral policy that has a disproportionate impact on a protected group.\n",
      "True Answers: ['practices in employment , housing , and other areas that adversely affect one group of people of a protected characteristic more than another , even though rules applied by employers or landlords are formally neutral']\n",
      "EM: 0 \t F1: 0.19607843137254902 \t Recall: 0.16129032258064516\n",
      "Question: plays doc back future\n",
      "Prediction: I'm sorry, I couldn't understand your question. Please rephrase it with additional details.\n",
      "True Answers: ['Christopher Allen Lloyd']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: nitty gritty dirt band fishin dark album\n",
      "Prediction: The Nitty Gritty Dirt Band's \"Fishin' in the Dark\" is a song from their album \"Hold On\".\n",
      "True Answers: ['Hold On']\n",
      "EM: 0 \t F1: 0.25 \t Recall: 1.0\n",
      "Question: characters live us\n",
      "Prediction: I'm sorry, but I cannot find a proper question related to the given context. Please provide a new user's request.\n",
      "True Answers: ['Pittsburgh', 'Los Angeles', 'New Jersey', 'New York City']\n",
      "EM: 0 \t F1: 0.10526315789473684 \t Recall: 0.5\n",
      "Question: leader ontario pc party\n",
      "Prediction: The leader of the Ontario PC Party and Ontario's Leader of the Official Opposition is Patrick Brown.\n",
      "True Answers: ['Patrick Brown']\n",
      "EM: 0 \t F1: 0.25 \t Recall: 1.0\n",
      "Question: many seasons prison break netflix\n",
      "Prediction: Prison Break has 5 seasons, and all 5 are available on Netflix.\n",
      "True Answers: ['five']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: recorded ca n 't help falling love\n",
      "Prediction: Can't Help Falling in Love was originally recorded by Elvis Presley and has been covered by numerous other artists.\n",
      "True Answers: ['Elvis Presley']\n",
      "EM: 0 \t F1: 0.1904761904761905 \t Recall: 1.0\n",
      "Question: name atom bomb dropped usa hiroshima\n",
      "Prediction: The atom bombs dropped on Hiroshima and Nagasaki were dropped by the United States.\n",
      "True Answers: ['Little Boy']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: us take wake island\n",
      "Prediction: The United States formally took possession of Wake Island on January 17, 1899, under orders from President William McKinley.\n",
      "True Answers: ['January 17 , 1899']\n",
      "EM: 0 \t F1: 0.2857142857142857 \t Recall: 1.0\n",
      "Question: stop cigarette advertising television\n",
      "Prediction: The Public Health Cigarette Smoking Act banned cigarette advertising on television and radio starting on January 2, 1971.\n",
      "True Answers: ['January 2 , 1971']\n",
      "EM: 0 \t F1: 0.3 \t Recall: 1.0\n",
      "Question: stop cigarette advertising television\n",
      "Prediction: The Public Health Cigarette Smoking Act banned cigarette advertising on television and radio starting on January 2, 1971.\n",
      "True Answers: ['1970']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: many lines symmetry equilateral triangle\n",
      "Prediction: An equilateral triangle has 3 lines of reflection and rotational symmetry of order 3 about its center.\n",
      "True Answers: ['3']\n",
      "EM: 0 \t F1: 0.11764705882352941 \t Recall: 1.0\n",
      "Question: many lines symmetry equilateral triangle\n",
      "Prediction: An equilateral triangle has 3 lines of reflection and rotational symmetry of order 3 about its center.\n",
      "True Answers: ['3 lines of reflection and rotational symmetry']\n",
      "EM: 0 \t F1: 0.6086956521739131 \t Recall: 1.0\n",
      "Question: number one ranked golfer world right\n",
      "Prediction: Dustin Johnson is the number one ranked golfer in the world.\n",
      "True Answers: ['Dustin Johnson']\n",
      "EM: 0 \t F1: 0.3636363636363636 \t Recall: 1.0\n",
      "Question: latest season keeping kardashians\n",
      "Prediction: The latest season of Keeping Up with the Kardashians aired 14 seasons and is scheduled to be released on March 8, 2018.\n",
      "True Answers: ['fourteen']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: mitochondria located sperm\n",
      "Prediction: The mitochondria in an animal sperm is located in the midpiece, at the base of the sperm head.\n",
      "True Answers: ['midpiece']\n",
      "EM: 0 \t F1: 0.14285714285714288 \t Recall: 1.0\n",
      "Question: us take wake island\n",
      "Prediction: The United States formally took possession of Wake Island on January 17, 1899, under orders from President William McKinley.\n",
      "True Answers: ['July 4 , 1898']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: mitochondria located sperm\n",
      "Prediction: The mitochondria in an animal sperm is located in the midpiece, at the base of the sperm head.\n",
      "True Answers: ['midpiece ( at the base of the sperm head )']\n",
      "EM: 0 \t F1: 0.631578947368421 \t Recall: 1.0\n",
      "Question: oldest team nba\n",
      "Prediction: The oldest team in the NBA is the Boston Celtics, founded in 1946.\n",
      "True Answers: ['New York Knickerbockers']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: latest season keeping kardashians\n",
      "Prediction: The latest season of Keeping Up with the Kardashians aired 14 seasons and is scheduled to be released on March 8, 2018.\n",
      "True Answers: ['14']\n",
      "EM: 0 \t F1: 0.09523809523809523 \t Recall: 1.0\n",
      "Question: last time america hosted summer olympics\n",
      "Prediction: The last time America hosted the Summer Olympics was in 1996.\n",
      "True Answers: ['1996']\n",
      "EM: 0 \t F1: 0.19999999999999998 \t Recall: 1.0\n",
      "Question: dose poet present death voyage crossing bar\n",
      "Prediction: Yes, Alfred, Lord Tennyson's poem \"Crossing the Bar\" presents death as a voyage across a sandbar between the river of life and the ocean of death.\n",
      "True Answers: [\"uses an extended metaphor to compare death with crossing the `` sandbar '' between river of life , with its outgoing `` flood '' , and the ocean that lies beyond ( death ) , the `` boundless deep '' , to which we return\"]\n",
      "EM: 0 \t F1: 0.36000000000000004 \t Recall: 0.3103448275862069\n",
      "Question: pumped kicks song\n",
      "Prediction: I'm sorry, I couldn't identify the question. Please provide me a valid question.\n",
      "True Answers: ['the homicidal thoughts of a troubled youth']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: farmers kept small portion crops & gave rest landowners\n",
      "Prediction: Farmers kept a small portion of their crops and gave the rest to landowners in the form of sharecropping.\n",
      "True Answers: ['Sharecropping']\n",
      "EM: 0 \t F1: 0.11764705882352941 \t Recall: 1.0\n",
      "Question: meaning name comanche\n",
      "Prediction: Comanche is the name of a Native American nation from the Great Plains, whose historic territory consisted of parts of several US states and northern Chihuahua, Mexico. The name \"Comanche\" comes from the Ute word \"kɨmantsi\", which means \"enemy\".\n",
      "True Answers: [\"`` Comanche '' is from the Ute name for them , kɨmantsi ( enemy )\"]\n",
      "EM: 0 \t F1: 0.3255813953488372 \t Recall: 0.7777777777777778\n",
      "Question: us go olympics\n",
      "Prediction: The United States has won gold and silver medals at the Winter Olympics, with their most recent gold medal in 2018.\n",
      "True Answers: ['The 1980 Summer Olympics']\n",
      "EM: 0 \t F1: 0.09090909090909091 \t Recall: 0.3333333333333333\n",
      "Question: us go olympics\n",
      "Prediction: The United States has won gold and silver medals at the Winter Olympics, with their most recent gold medal in 2018.\n",
      "True Answers: ['1980']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: meaning name comanche\n",
      "Prediction: Comanche is the name of a Native American nation from the Great Plains, whose historic territory consisted of parts of several US states and northern Chihuahua, Mexico. The name \"Comanche\" comes from the Ute word \"kɨmantsi\", which means \"enemy\".\n",
      "True Answers: ['enemy']\n",
      "EM: 0 \t F1: 0.05714285714285715 \t Recall: 1.0\n",
      "Question: building new raiders stadium\n",
      "Prediction: The new stadium being built in Paradise, Nevada is the future home of the Las Vegas Raiders and UNLV Rebels football team. It is expected to be completed in time for the 2020 NFL season.\n",
      "True Answers: ['in Paradise , Nevada', 'west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive , just west of Interstate 15']\n",
      "EM: 0 \t F1: 0.17647058823529413 \t Recall: 1.0\n",
      "Question: event provoked congress propose eleventh amendment states ratify\n",
      "Prediction: The Eleventh Amendment was proposed by Congress on March 4, 1794, and was ratified by states on February 7, 1795, to deal with each state's sovereign immunity and overrule the Supreme Court's decision in Chisholm v. Georgia.\n",
      "True Answers: [\"U.S. Supreme Court 's decision in Chisholm v. Georgia\"]\n",
      "EM: 0 \t F1: 0.27272727272727276 \t Recall: 0.6666666666666666\n",
      "Question: event provoked congress propose eleventh amendment states ratify\n",
      "Prediction: The Eleventh Amendment was proposed by Congress on March 4, 1794, and was ratified by states on February 7, 1795, to deal with each state's sovereign immunity and overrule the Supreme Court's decision in Chisholm v. Georgia.\n",
      "True Answers: [\"the U.S. Supreme Court 's decision in Chisholm v. Georgia , 2 U.S. 419 ( 1793 )\"]\n",
      "EM: 0 \t F1: 0.25000000000000006 \t Recall: 0.46153846153846156\n",
      "Question: building new raiders stadium\n",
      "Prediction: The new stadium being built in Paradise, Nevada is the future home of the Las Vegas Raiders and UNLV Rebels football team. It is expected to be completed in time for the 2020 NFL season.\n",
      "True Answers: ['Paradise , Nevada']\n",
      "EM: 0 \t F1: 0.12121212121212122 \t Recall: 1.0\n",
      "Question: 2 . reasons states impose protectionists policies countries\n",
      "Prediction: Two reasons states impose protectionist policies are to shield domestic producers, businesses, and workers from foreign competitors and to generate revenue through tariffs and other regulations.\n",
      "True Answers: ['shield the producers , businesses , and workers of the import - competing sector in the country from foreign competitors']\n",
      "EM: 0 \t F1: 0.4 \t Recall: 0.5714285714285714\n",
      "Question: event provoked congress propose eleventh amendment states ratify\n",
      "Prediction: The Eleventh Amendment was proposed by Congress on March 4, 1794, and was ratified by states on February 7, 1795, to deal with each state's sovereign immunity and overrule the Supreme Court's decision in Chisholm v. Georgia.\n",
      "True Answers: [\"the Supreme Court 's ruling in Chisholm v. Georgia\"]\n",
      "EM: 0 \t F1: 0.23255813953488372 \t Recall: 0.625\n",
      "Question: event provoked congress propose eleventh amendment states ratify\n",
      "Prediction: The Eleventh Amendment was proposed by Congress on March 4, 1794, and was ratified by states on February 7, 1795, to deal with each state's sovereign immunity and overrule the Supreme Court's decision in Chisholm v. Georgia.\n",
      "True Answers: [\"the Supreme Court 's ruling in Chisholm v. Georgia , 2 U.S. 419 ( 1793 )\"]\n",
      "EM: 0 \t F1: 0.21276595744680848 \t Recall: 0.4166666666666667\n",
      "Question: meet gates heaven\n",
      "Prediction: The gates of heaven are often depicted as large gates made of gold, white, or wrought iron located in the clouds and guarded by Saint Peter, who uses a book to determine who is allowed to enter. Those not fit for heaven are denied entrance and sent to Hell.\n",
      "True Answers: ['Saint Peter']\n",
      "EM: 0 \t F1: 0.08333333333333333 \t Recall: 1.0\n",
      "Question: plays timon lion king broadway\n",
      "Prediction: Tim Rice wrote the lyrics for The Lion King Broadway musical.\n",
      "True Answers: ['Max Casella']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: meaning dragon boat festival\n",
      "Prediction: The Dragon Boat Festival is a traditional holiday originating in China, occurring near the summer solstice. It is also known as Zhongxiao Festival.\n",
      "True Answers: ['commemorating fealty and filial piety']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: edmund days lives\n",
      "Prediction: Sir Edmund Barton lived from 1849 to 1920.\n",
      "True Answers: ['Adam Caine']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: bulls used farm\n",
      "Prediction: Yes, bulls are used on farms for breeding purposes and for producing bull beef.\n",
      "True Answers: ['breeding', 'as work oxen', 'slaughtered for meat']\n",
      "EM: 0 \t F1: 0.13333333333333333 \t Recall: 1.0\n",
      "Question: baby elephants name jungle book\n",
      "Prediction: The baby elephant's name in The Jungle Book is Hathi Jr.\n",
      "True Answers: ['Hathi Jr .']\n",
      "EM: 0 \t F1: 0.3636363636363636 \t Recall: 1.0\n",
      "Question: bulls used farm\n",
      "Prediction: Yes, bulls are used on farms for breeding purposes and for producing bull beef.\n",
      "True Answers: ['herd maintenance']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: joyful mysteries holy rosary\n",
      "Prediction: Joyful Mysteries are part of the Rosary that consists of meditations on the events in the lives of Jesus and Mary. The Joyful Mysteries refer to the Annunciation, Visitation, Nativity, Presentation of Jesus at the Temple, and the Finding of Jesus in the Temple.\n",
      "True Answers: ['The Annunciation', 'The Visitation', 'The Nativity', 'The Presentation of Jesus at the Temple', 'The Finding of Jesus in the Temple']\n",
      "EM: 0 \t F1: 0.24390243902439027 \t Recall: 1.0\n",
      "Question: bulls used farm\n",
      "Prediction: Yes, bulls are used on farms for breeding purposes and for producing bull beef.\n",
      "True Answers: ['breeding', 'slaughtered for meat', 'work oxen for haulage']\n",
      "EM: 0 \t F1: 0.13333333333333333 \t Recall: 1.0\n",
      "Question: royal society protection birds number members\n",
      "Prediction: The Royal Society for the Protection of Birds has more than a million members.\n",
      "True Answers: ['more than a million members ( including 195,000 youth members )']\n",
      "EM: 0 \t F1: 0.4210526315789474 \t Recall: 0.5\n",
      "Question: royal society protection birds number members\n",
      "Prediction: The Royal Society for the Protection of Birds has more than a million members.\n",
      "True Answers: ['more than a million']\n",
      "EM: 0 \t F1: 0.42857142857142855 \t Recall: 1.0\n",
      "Question: want everywhere song\n",
      "Prediction: The song \"Everywhere\" is by Michelle Branch.\n",
      "True Answers: ['Everywhere']\n",
      "EM: 0 \t F1: 0.2857142857142857 \t Recall: 1.0\n",
      "Question: theme summer place released\n",
      "Prediction: \"Theme from A Summer Place\" was released as a single by Percy Faith and his Orchestra in September 1959.\n",
      "True Answers: ['1959']\n",
      "EM: 0 \t F1: 0.1111111111111111 \t Recall: 1.0\n",
      "Question: athens emerges wealthiest greek city state\n",
      "Prediction: Athens emerged as the wealthiest Greek city state due to their efficient trading system with other city states, the finding of several veins of silver in the neighboring mountains, and the trading of luxury goods such as marble and bronze artwork.\n",
      "True Answers: ['the late 6th century BCE']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: athens emerges wealthiest greek city state\n",
      "Prediction: Athens emerged as the wealthiest Greek city state due to their efficient trading system with other city states, the finding of several veins of silver in the neighboring mountains, and the trading of luxury goods such as marble and bronze artwork.\n",
      "True Answers: ['around the late 6th century BCE']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: theme summer place released\n",
      "Prediction: \"Theme from A Summer Place\" was released as a single by Percy Faith and his Orchestra in September 1959.\n",
      "True Answers: ['September 1959']\n",
      "EM: 0 \t F1: 0.21052631578947367 \t Recall: 1.0\n",
      "Question: movie jeremiah johnson filmed\n",
      "Prediction: The movie Jeremiah Johnson was filmed at Old Tucson Studios in Arizona and in the nearby Saguaro National Park.\n",
      "True Answers: ['Utah']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: color cue ball pool\n",
      "Prediction: The cue ball in pool is usually white.\n",
      "True Answers: ['white']\n",
      "EM: 0 \t F1: 0.25 \t Recall: 1.0\n",
      "Question: sings war n 't let\n",
      "Prediction: UPDATE CONTEXT\n",
      "True Answers: ['American production duo The Chainsmokers', 'vocals of American singer Daya']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: movie jeremiah johnson filmed\n",
      "Prediction: The movie Jeremiah Johnson was filmed at Old Tucson Studios in Arizona and in the nearby Saguaro National Park.\n",
      "True Answers: ['in nearly 100 locations across Utah , including : Mount Timpanogos , Ashley National Forest , Leeds , Snow Canyon State Park , St. George , Sundance Resort , Uinta National Forest , Wasatch - Cache National Forest , and Zion National Park']\n",
      "EM: 0 \t F1: 0.16326530612244897 \t Recall: 0.125\n",
      "Question: meaning name comanche\n",
      "Prediction: Comanche is the name of a Native American nation from the Great Plains, whose historic territory consisted of parts of several US states and northern Chihuahua, Mexico. The name \"Comanche\" comes from the Ute word \"kɨmantsi\", which means \"enemy\".\n",
      "True Answers: ['a Native American nation from the Great Plains']\n",
      "EM: 0 \t F1: 0.3 \t Recall: 1.0\n",
      "Question: jenny humphrey come back gossip girl\n",
      "Prediction: Yes, Jenny Humphrey comes back in the final episode of Gossip Girl.\n",
      "True Answers: ['the final episode of the series']\n",
      "EM: 0 \t F1: 0.39999999999999997 \t Recall: 0.75\n",
      "Question: theme summer place released\n",
      "Prediction: \"Theme from A Summer Place\" was released as a single by Percy Faith and his Orchestra in September 1959.\n",
      "True Answers: ['1960']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: degree crock pot low\n",
      "Prediction: The low setting on a crockpot typically heats the contents to around 200 degrees Fahrenheit.\n",
      "True Answers: ['71 -- 74 ° C ( 160 -- 165 ° F )']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: performed first c section 1794\n",
      "Prediction: The Eleventh Amendment was passed by Congress on March 4, 1794.\n",
      "True Answers: ['Dr. Jesse Bennett']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: long prime minister stay office canada\n",
      "Prediction: The Canadian Prime Minister does not have a fixed term in office, and remains in office until he or she resigns, is dismissed, or dies. There is no limit on how long a Prime Minister can stay in office.\n",
      "True Answers: ['until he or she resigns , is dismissed , or dies']\n",
      "EM: 0 \t F1: 0.3555555555555555 \t Recall: 0.8888888888888888\n",
      "Question: long prime minister stay office canada\n",
      "Prediction: The Canadian Prime Minister does not have a fixed term in office, and remains in office until he or she resigns, is dismissed, or dies. There is no limit on how long a Prime Minister can stay in office.\n",
      "True Answers: ['the post does not have a fixed term']\n",
      "EM: 0 \t F1: 0.2380952380952381 \t Recall: 0.8333333333333334\n",
      "Question: scored tries rugby union\n",
      "Prediction: Daisuke Ohata from Japan has scored the most tries in Rugby Union with 69.\n",
      "True Answers: ['Daisuke Ohata']\n",
      "EM: 0 \t F1: 0.2666666666666667 \t Recall: 1.0\n",
      "Question: willow tara get back together\n",
      "Prediction: There is no indication that Willow and Tara will get back together in this context.\n",
      "True Answers: ['Seeing Red']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: long prime minister stay office canada\n",
      "Prediction: The Canadian Prime Minister does not have a fixed term in office, and remains in office until he or she resigns, is dismissed, or dies. There is no limit on how long a Prime Minister can stay in office.\n",
      "True Answers: ['four years']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: new season lost space come\n",
      "Prediction: Lost in Space has been renewed for a third season.\n",
      "True Answers: ['April 13 , 2018']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: last time military drafted\n",
      "Prediction: The last time men were drafted to the United States Armed Forces was during the Vietnam War, with the last men born in 1952 being conscripted and reporting for duty in June 1973. After that, the draft was never reinstated and the Armed Forces moved to an all-volunteer military force.\n",
      "True Answers: ['1973']\n",
      "EM: 0 \t F1: 0.04545454545454545 \t Recall: 1.0\n",
      "Question: plays characters yo gabba gabba\n",
      "Prediction: There is no character from Yo Gabba Gabba listed in the provided context.\n",
      "True Answers: ['DJ Lance Rock -- ( Portrayed by Lance Robertson U.S. , voice dubbed by Ortis Deley in the UK )', 'Muno -- ( Voiced by Adam Deibert U.S. , Bob Golding UK )', 'Foofa -- ( Voiced by Emma Jacobs - Briggs U.S. , Teresa Gallagher UK )', 'Brobee -- ( Voiced by Amos Watene U.S. , Simon Feilder UK )']\n",
      "EM: 0 \t F1: 0.07407407407407407 \t Recall: 0.06666666666666667\n",
      "Question: plays characters yo gabba gabba\n",
      "Prediction: There is no character from Yo Gabba Gabba listed in the provided context.\n",
      "True Answers: ['DJ Lance Rock', 'Muno', 'Foofa', 'Brobee', 'Toodee', 'Plex', 'Gooble']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: name three large lakes great lakes united states\n",
      "Prediction: Lake Superior, Lake Huron, and Lake Michigan.\n",
      "True Answers: ['Great Salt Lake', 'Lake of the Woods', 'Iliamna Lake']\n",
      "EM: 0 \t F1: 0.22222222222222224 \t Recall: 0.5\n",
      "Question: aeneas go left carthage\n",
      "Prediction: I'm sorry, I don't understand the question. Please provide more details or a clearer question.\n",
      "True Answers: ['Sicily']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: wrote theme song mission impossible\n",
      "Prediction: The theme song for Mission: Impossible was written and composed by Lalo Schifrin.\n",
      "True Answers: ['Lalo Schifrin']\n",
      "EM: 0 \t F1: 0.2857142857142857 \t Recall: 1.0\n",
      "Question: driver 's license\n",
      "Prediction: A driver's license is an official document that allows a person to operate one or more types of motorized vehicles on a public road and is issued by the state's DMV.\n",
      "True Answers: ['to operate one or more types of motorized vehicles , such as a motorcycle , car , truck , or bus on a public road']\n",
      "EM: 0 \t F1: 0.5333333333333333 \t Recall: 0.631578947368421\n",
      "Question: capital habsburg empire located\n",
      "Prediction: The capital of the Habsburg Empire was Vienna, except from 1583 to 1611 when it was moved to Prague.\n",
      "True Answers: ['Vienna', 'Vienna']\n",
      "EM: 0 \t F1: 0.1111111111111111 \t Recall: 1.0\n",
      "Question: plays general hux last jedi\n",
      "Prediction: Domhnall Gleeson portrays General Hux in Star Wars: The Last Jedi.\n",
      "True Answers: ['Domhnall Gleeson']\n",
      "EM: 0 \t F1: 0.33333333333333337 \t Recall: 1.0\n",
      "Question: participated super bowls\n",
      "Prediction: The Pittsburgh Steelers have won the most Super Bowls with six championships.\n",
      "True Answers: ['New England Patriots']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: sang movie walk line\n",
      "Prediction: The movie being referred to is \"Walk the Line\".\n",
      "True Answers: ['Joaquin Phoenix', 'Reese Witherspoon', 'Waylon Payne', 'Johnathan Rice', 'Tyler Hilton', 'Shooter Jennings']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: starred remake true grit\n",
      "Prediction: The 2010 version of True Grit is a remake of the 1969 film, starring Jeff Bridges and Hailee Steinfeld.\n",
      "True Answers: [\"Hailee Steinfeld as Mattie Ross and Jeff Bridges as Deputy U.S. Marshal Reuben J. `` Rooster '' Cogburn , along with Matt Damon , Josh Brolin , and Barry Pepper\"]\n",
      "EM: 0 \t F1: 0.24390243902439027 \t Recall: 0.2\n",
      "Question: seat moved capitol\n",
      "Prediction: The House of Representatives moved to their House wing at the Capitol in 1807.\n",
      "True Answers: ['December 1800']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: many students attend university texas austin\n",
      "Prediction: Over 50,000 undergraduate and graduate students attend the University of Texas at Austin.\n",
      "True Answers: ['over 50,000 undergraduate and graduate students']\n",
      "EM: 0 \t F1: 0.6666666666666666 \t Recall: 1.0\n",
      "Question: winnie pooh books written\n",
      "Prediction: The Winnie Pooh books were written by A.A. Milne, and the first collection of stories was published in 1926.\n",
      "True Answers: ['1926', '1928', '1924', '1927']\n",
      "EM: 0 \t F1: 0.1111111111111111 \t Recall: 1.0\n",
      "Question: played tom world turns\n",
      "Prediction: Tom Hughes was played by Scott Holmes on As The World Turns from 1987 to 2010.\n",
      "True Answers: ['Peter Galman', 'C. David Colson', 'Tom Tammi', 'Justin Deas', 'Jason Kincaid', 'Gregg Marx', 'Scott Holmes', 'others']\n",
      "EM: 0 \t F1: 0.23529411764705882 \t Recall: 1.0\n",
      "Question: last time military drafted\n",
      "Prediction: The last time men were drafted to the United States Armed Forces was during the Vietnam War, with the last men born in 1952 being conscripted and reporting for duty in June 1973. After that, the draft was never reinstated and the Armed Forces moved to an all-volunteer military force.\n",
      "True Answers: ['1972']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "Question: many students attend university texas austin\n",
      "Prediction: Over 50,000 undergraduate and graduate students attend the University of Texas at Austin.\n",
      "True Answers: ['51,331']\n",
      "EM: 0 \t F1: 0 \t Recall: 0\n",
      "=======================================\n",
      "Average EM: 0.0\n",
      "Average F1: 0.16547723189085997\n",
      "Average Recall: 0.5305852545284663\n"
     ]
    }
   ],
   "source": [
    "all_em_scores = []\n",
    "all_f1_scores = []\n",
    "all_recall_scores = []\n",
    "for i in range(len(retrieve_answers)):\n",
    "    prediction = retrieve_answers[i]\n",
    "    gold_answers = answers_sample[i]\n",
    "\n",
    "    em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
    "    f1_score = max((compute_f1_recall(prediction, answer)[0]) for answer in gold_answers)\n",
    "    recall_score = max((compute_f1_recall(prediction, answer)[1]) for answer in gold_answers)\n",
    "\n",
    "    all_em_scores.append(em_score)\n",
    "    all_f1_scores.append(f1_score)\n",
    "    all_recall_scores.append(recall_score)\n",
    "\n",
    "    # if i % 10 == 0 or recall_score < 0.3:\n",
    "    print(f\"Question: {questions_sample[i]}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"True Answers: {gold_answers}\")\n",
    "    print(f\"EM: {em_score} \\t F1: {f1_score} \\t Recall: {recall_score}\")\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(f\"Average EM: {sum(all_em_scores) / len(all_em_scores)}\")\n",
    "print(f\"Average F1: {sum(all_f1_scores) / len(all_f1_scores)}\")\n",
    "print(f\"Average Recall: {sum(all_recall_scores) / len(all_recall_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Save the current sys.stdout for later restoration\n",
    "original_stdout = sys.stdout\n",
    "\n",
    "# Redirect sys.stdout to a file\n",
    "with open('logs-clean-100.txt', 'w') as f:\n",
    "    sys.stdout = f\n",
    "    \n",
    "    for qa_problem in questions[:100]:\n",
    "        print(f\"\\n\\n>>>>>>>>>>>>>> case: {qa_problem} <<<<<<<<<<<<<<\\n\\n\")\n",
    "        assistant.reset()\n",
    "        try:\n",
    "            ragproxyagent.initiate_chat(assistant, problem=qa_problem, n_results=10)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "\n",
    "# Restore sys.stdout to its original value\n",
    "sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis log file: logs-clean-100.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of questions: 6775\n",
      "len_lines=5921\n",
      "question='non controlling interest balance sheet'\n",
      "question='many episodes chicago fire season 4'\n",
      "question='sings love keep us alive eagles'\n",
      "question='leader ontario pc party'\n",
      "question='last name keith come'\n",
      "question='last name keith come'\n",
      "question='leader ontario pc party'\n",
      "question='converting stereo signal mono signal called'\n",
      "question='plays max voice goofy movie'\n",
      "question='sings love keep us alive eagles'\n",
      "question='three elves got rings'\n",
      "question='new tappan zee bridge going finished'\n",
      "question=\"chosen brand ambassador campaign ' beti bachao-beti padhao\"\n",
      "question='new tappan zee bridge going finished'\n",
      "question=\"chosen brand ambassador campaign ' beti bachao-beti padhao\"\n",
      "question='makes decisions produce market economy'\n",
      "question='order prove disparate impact first must establish'\n",
      "question='plays doc back future'\n",
      "question='nitty gritty dirt band fishin dark album'\n",
      "question='characters live us'\n",
      "question='leader ontario pc party'\n",
      "question='many seasons prison break netflix'\n",
      "question=\"recorded ca n 't help falling love\"\n",
      "question='name atom bomb dropped usa hiroshima'\n",
      "question='us take wake island'\n",
      "question='stop cigarette advertising television'\n",
      "question='stop cigarette advertising television'\n",
      "question='many lines symmetry equilateral triangle'\n",
      "question='many lines symmetry equilateral triangle'\n",
      "question='number one ranked golfer world right'\n",
      "question='latest season keeping kardashians'\n",
      "question='mitochondria located sperm'\n",
      "question='us take wake island'\n",
      "question='mitochondria located sperm'\n",
      "question='oldest team nba'\n",
      "question='latest season keeping kardashians'\n",
      "question='last time america hosted summer olympics'\n",
      "question='dose poet present death voyage crossing bar'\n",
      "question='pumped kicks song'\n",
      "question='farmers kept small portion crops & gave rest landowners'\n",
      "question='meaning name comanche'\n",
      "question='us go olympics'\n",
      "question='us go olympics'\n",
      "question='meaning name comanche'\n",
      "question='building new raiders stadium'\n",
      "question='event provoked congress propose eleventh amendment states ratify'\n",
      "question='event provoked congress propose eleventh amendment states ratify'\n",
      "question='building new raiders stadium'\n",
      "question='2 . reasons states impose protectionists policies countries'\n",
      "question='event provoked congress propose eleventh amendment states ratify'\n",
      "question='event provoked congress propose eleventh amendment states ratify'\n",
      "question='meet gates heaven'\n",
      "question='plays timon lion king broadway'\n",
      "question='meaning dragon boat festival'\n",
      "question='edmund days lives'\n",
      "question='bulls used farm'\n",
      "question='baby elephants name jungle book'\n",
      "question='bulls used farm'\n",
      "question='joyful mysteries holy rosary'\n",
      "question='bulls used farm'\n",
      "question='royal society protection birds number members'\n",
      "question='royal society protection birds number members'\n",
      "question='want everywhere song'\n",
      "question='theme summer place released'\n",
      "question='athens emerges wealthiest greek city state'\n",
      "question='athens emerges wealthiest greek city state'\n",
      "question='theme summer place released'\n",
      "question='movie jeremiah johnson filmed'\n",
      "question='color cue ball pool'\n",
      "question=\"sings war n 't let\"\n",
      "question='movie jeremiah johnson filmed'\n",
      "question='meaning name comanche'\n",
      "question='jenny humphrey come back gossip girl'\n",
      "question='theme summer place released'\n",
      "question='degree crock pot low'\n",
      "question='performed first c section 1794'\n",
      "question='long prime minister stay office canada'\n",
      "question='long prime minister stay office canada'\n",
      "question='scored tries rugby union'\n",
      "question='willow tara get back together'\n",
      "question='long prime minister stay office canada'\n",
      "question='new season lost space come'\n",
      "question='last time military drafted'\n",
      "question='plays characters yo gabba gabba'\n",
      "question='plays characters yo gabba gabba'\n",
      "question='name three large lakes great lakes united states'\n",
      "question='aeneas go left carthage'\n",
      "question='wrote theme song mission impossible'\n",
      "question=\"driver 's license\"\n",
      "question='capital habsburg empire located'\n",
      "question='plays general hux last jedi'\n",
      "question='participated super bowls'\n",
      "question='sang movie walk line'\n",
      "question='starred remake true grit'\n",
      "question='seat moved capitol'\n",
      "question='many students attend university texas austin'\n",
      "question='winnie pooh books written'\n",
      "question='played tom world turns'\n",
      "question='last time military drafted'\n",
      "question='many students attend university texas austin'\n",
      "_cnt_update_context=48\n",
      "\n",
      "\n",
      "====================== mode='all' ======================\n",
      "Number of questions: 71\n",
      "Average EM: 0.0\n",
      "Average F1: 0.16468189261493435\n",
      "Average Recall: 0.5439548804946155\n",
      "\n",
      "\n",
      "====================== mode='update_context' ======================\n",
      "Number of questions: 10\n",
      "Average EM: 0.0\n",
      "Average F1: 0.07812169312169312\n",
      "Average Recall: 0.3066666666666667\n",
      "\n",
      "\n",
      "====================== mode='no_update_context' ======================\n",
      "Number of questions: 61\n",
      "Average EM: 0.0\n",
      "Average F1: 0.1788720892531706\n",
      "Average Recall: 0.5828545876795251\n"
     ]
    }
   ],
   "source": [
    "from analysis_log import main\n",
    "main(\"logs-clean-100.txt\", question_process=remove_common_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
