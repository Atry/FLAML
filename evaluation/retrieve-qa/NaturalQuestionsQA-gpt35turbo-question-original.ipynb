{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"pyautogen[retrievechat]~=0.2.0b5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/FLAML/docs/reference/autogen/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijiang1/anaconda3/envs/autogen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models to use:  ['gpt-35-turbo']\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\".config.local\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4\",\n",
    "            \"gpt4\",\n",
    "            \"gpt-4-32k\",\n",
    "            \"gpt-4-32k-0314\",\n",
    "            \"gpt-35-turbo\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "assert len(config_list) > 0\n",
    "config_list[0]['model'] = 'gpt-35-turbo'\n",
    "print(\"models to use: \", [config_list[i][\"model\"] for i in range(len(config_list))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct agents for RetrieveChat\n",
    "\n",
    "We start by initialzing the `RetrieveAssistantAgent` and `RetrieveUserProxyAgent`. The system message needs to be set to \"You are a helpful assistant.\" for RetrieveAssistantAgent. The detailed instructions are given in the user message. Later we will use the `RetrieveUserProxyAgent.generate_init_prompt` to combine the instructions and a math problem for an initial prompt to be sent to the LLM assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijiang1/anaconda3/envs/autogen/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "import chromadb\n",
    "\n",
    "# 1. create an RetrieveAssistantAgent instance named \"assistant\"\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\", \n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"timeout\": 60,\n",
    "        \"cache_seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. create the RetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
    "corpus_file = \"https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/corpus.txt\"\n",
    "\n",
    "# Create a new collection for NaturalQuestions dataset\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": corpus_file,\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"natural-questions\",\n",
    "        \"chunk_mode\": \"one_line\",\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        \"get_or_create\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Questions QA\n",
    "\n",
    "Use RetrieveChat to answer questions for [NaturalQuestion](https://ai.google.com/research/NaturalQuestions) dataset.\n",
    "\n",
    "We'll first create a new document collection based on all the context corpus, then we select some questions and answer them with RetrieveChat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-22 00:12:47--  https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/queries.jsonl\n",
      "Resolving huggingface.co (huggingface.co)... 99.84.108.55, 99.84.108.129, 99.84.108.87, ...\n",
      "Connecting to huggingface.co (huggingface.co)|99.84.108.55|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1380571 (1.3M) [text/plain]\n",
      "Saving to: ‘/tmp/chromadb/queries.jsonl’\n",
      "\n",
      "/tmp/chromadb/queri 100%[===================>]   1.32M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2023-11-22 00:12:47 (40.6 MB/s) - ‘/tmp/chromadb/queries.jsonl’ saved [1380571/1380571]\n",
      "\n",
      "['what is non controlling interest on balance sheet', 'how many episodes are in chicago fire season 4', 'who sings love will keep us alive by the eagles', 'who is the leader of the ontario pc party', 'where did the last name keith come from']\n",
      "[[\"the portion of a subsidiary corporation 's stock that is not owned by the parent corporation\"], ['23'], ['Timothy B. Schmit'], ['Patrick Walter Brown'], ['from Keith in East Lothian , Scotland', \"from a nickname , derived from the Middle High German kīt , a word meaning `` sprout '' , `` offspring ''\"]]\n",
      "Number of questions: 6775\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "queries_file = \"https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/queries.jsonl\"\n",
    "!wget -O /tmp/chromadb/queries.jsonl $queries_file\n",
    "queries = [json.loads(line) for line in open(\"/tmp/chromadb/queries.jsonl\").readlines() if line]\n",
    "questions = [q[\"text\"] for q in queries]\n",
    "answers = [q[\"metadata\"][\"answer\"] for q in queries]\n",
    "print(questions[:5])\n",
    "print(answers[:5])\n",
    "print(\"Number of questions:\", len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO \n",
    "import sys\n",
    "\n",
    "class Capturing(list):\n",
    "    def __enter__(self):\n",
    "        self._stdout = sys.stdout\n",
    "        sys.stdout = self._stringio = StringIO()\n",
    "        return self\n",
    "    def __exit__(self, *args):\n",
    "        self.extend(self._stringio.getvalue().splitlines())\n",
    "        del self._stringio    # free up some memory\n",
    "        sys.stdout = self._stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 0.00%, Time Used 0.00 hours\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Film </Th> <Th> Year </Th> <Th> Fuck count </Th> <Th> Minutes </Th> <Th> Uses / mi ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Character </Th> <Th> Ultimate Avengers </Th> <Th> Ultimate Avengers 2 </Th> <Th> I ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Position </Th> <Th> Country </Th> <Th> Town / City </Th> <Th> PM2. 5 </Th> <Th> PM ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Country ( or dependent territory ) </Th> <Th> Population </Th> <Th ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> State </Th> <Th> Gross collections ( in thousands ) </Th> <Th> Rev ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Date </Th> <Th> Province </Th> <Th> Mag . </Th> <Th> MMI </Th> <Th> Deaths </Th> < ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> City </Th> <Th> River </Th> <Th> State </Th> </Tr> <Tr> <Td> Gangakhed </Td> <Td>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Player </Th> <Th> Pos . </Th> <Th> Team </Th> <Th> Career start </Th> <Th> Career  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> ABO and Rh blood type distribution by country ( population averages ) <Tr> <Th> Country </Th ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> </Th> <Th colspan=\"3\"> Total area </Th> <Th colspan=\"4\"> Land area </Th> <Th colsp ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> Performance in the European Cup and UEFA Champions League by club <Tr> <Th> <Ul> <Li> </Li>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> City </Th> <Th> State </Th> <Th> Land area ( sq mi ) </Th> <Th> La ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> # </Th> <Th> Country </Th> <Th> Name </Th> <Th> International goals </Th> <Th> Cap ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> City </Th> <Th> Image </Th> <Th> Population </Th> <Th> Definition  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Team </Th> <Th> Won </Th> <Th> Lost </Th> <Th> Tied </Th> <Th> Pct ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Territory </Th> <Th> Rights holder </Th> <Th> Ref </Th> </Tr> <Tr> <Td> Asia </Td> ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> ( hide ) Rank </Th> <Th> Nat </Th> <Th> Name </Th> <Th> Years </Th> <Th> Goals </T ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> </Th> <Th colspan=\"3\"> Total area </Th> <Th colspan=\"4\"> Land area </Th> <Th colsp ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th colspan=\"2\"> Bids by school </Th> <Th colspan=\"2\"> Most recent </Th> <Th colspan=\"2 ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Name </Th> <Th> Nation </Th> <Th> TP </Th> <Th colspan=\"2\"> SP </T ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> 2014 Rank </Th> <Th> City </Th> <Th> 2014 Estimate </Th> <Th> 2010 Census </Th> <T ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> S.No . </Th> <Th> Year </Th> <Th> Name </Th> </Tr> <Tr> <Td> </Td> <Td> 1961 </Td> ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> Densities of various materials covering a range of values <Tr> <Th> Material </Th> <Th> ρ (  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Club </Th> <Th> Season </Th> <Th colspan=\"3\"> League </Th> <Th colspan=\"2\"> Nation ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank ( 2016 ) </Th> <Th> Airports ( large hubs ) </Th> <Th> IATA Code </Th> <Th> M ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> City </Th> <Th> Region / State </Th> <Th> Country </Th> <Th> Park name </Th> <Th>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Year </Th> <Th> Winner ( nationally ) </Th> <Th> Votes </Th> <Th> Percent </Th> <T ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Compound </Th> <Th> SERT </Th> <Th> NET </Th> <Th> DAT </Th> <Th> 5 - HT </Th> <Th ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Name </Th> <Th> Industry </Th> <Th> Revenue ( USD millions ) </Th> ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Name </Th> <Th> Name in Georgian </Th> <Th> Population 1989 </Th>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Country </Th> <Th colspan=\"2\"> The World Factbook </Th> <Th colspan=\"2\"> World Res ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Country </Th> <Th> Area ( km2 ) </Th> <Th> Notes </Th> </Tr> <Tr>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Country </Th> <Th> Area ( km2 ) </Th> <Th> Notes </Th> </Tr> <Tr>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Date </Th> <Th> State ( s ) </Th> <Th> Magnitude </Th> <Th> Fatalities </Th> <Th>  ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Artist </Th> <Th> # Gold </Th> <Th> # Platinum </Th> <Th> # Multi-Platinum </Th> < ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> </Th> <Th colspan=\"2\"> Name </Th> <Th> Number of locations </Th> <Th> Revenue </Th ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> </Th> <Th> Name </Th> <Th> Country </Th> <Th> Region </Th> <Th> Depth ( meters ) < ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> Rank </Th> <Th> Player ( 2017 HRs ) </Th> <Th> HR </Th> </Tr> <Tr> <Td> </Td> <Td> ...\n",
      "WARNING:autogen.retrieve_utils:max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t<Table> <Tr> <Th> No . </Th> <Th> Athlete </Th> <Th> Nation </Th> <Th> Sport </Th> <Th> Years </Th>  ...\n",
      "INFO:autogen.retrieve_utils:Found 5369 chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error in problem:  who played tom on as the world turns\n",
      "\n",
      "Progress 1.43%, Time Used 0.24 hours\n",
      "\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error in problem:  who played tom on as the world turns\n",
      "\n",
      "Progress 2.86%, Time Used 0.27 hours\n",
      "\n",
      "\n",
      "Progress 4.29%, Time Used 0.31 hours\n",
      "\n",
      "\n",
      "Progress 5.71%, Time Used 0.34 hours\n",
      "\n",
      "\n",
      "Progress 7.14%, Time Used 0.36 hours\n",
      "\n",
      "\n",
      "Progress 8.57%, Time Used 0.40 hours\n",
      "\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in problem:  what episode of pll does jenna get her sight back\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in problem:  what episode of pll does jenna get her sight back\n",
      "\n",
      "Progress 10.00%, Time Used 0.47 hours\n",
      "\n",
      "\n",
      "Progress 11.43%, Time Used 0.53 hours\n",
      "\n",
      "\n",
      "Progress 12.86%, Time Used 0.54 hours\n",
      "\n",
      "\n",
      "Progress 14.29%, Time Used 0.55 hours\n",
      "\n",
      "\n",
      "Progress 15.71%, Time Used 0.57 hours\n",
      "\n",
      "\n",
      "Progress 17.14%, Time Used 0.62 hours\n",
      "\n",
      "\n",
      "Progress 18.57%, Time Used 0.62 hours\n",
      "\n",
      "\n",
      "Progress 20.00%, Time Used 0.63 hours\n",
      "\n",
      "\n",
      "Progress 21.43%, Time Used 0.64 hours\n",
      "\n",
      "\n",
      "Progress 22.86%, Time Used 0.65 hours\n",
      "\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error in problem:  who is the guy who walked across the twin towers\n",
      "\n",
      "Progress 24.29%, Time Used 0.66 hours\n",
      "\n",
      "\n",
      "Progress 25.71%, Time Used 0.67 hours\n",
      "\n",
      "\n",
      "Progress 27.14%, Time Used 0.67 hours\n",
      "\n",
      "\n",
      "Progress 28.57%, Time Used 0.68 hours\n",
      "\n",
      "\n",
      "Progress 30.00%, Time Used 0.69 hours\n",
      "\n",
      "\n",
      "Progress 31.43%, Time Used 0.72 hours\n",
      "\n",
      "\n",
      "Progress 32.86%, Time Used 0.75 hours\n",
      "\n",
      "\n",
      "Progress 34.29%, Time Used 0.76 hours\n",
      "\n",
      "\n",
      "Progress 35.71%, Time Used 0.77 hours\n",
      "\n",
      "\n",
      "Progress 37.14%, Time Used 0.78 hours\n",
      "\n",
      "\n",
      "Progress 38.57%, Time Used 0.79 hours\n",
      "\n",
      "\n",
      "Progress 40.00%, Time Used 0.80 hours\n",
      "\n",
      "\n",
      "Progress 41.43%, Time Used 0.81 hours\n",
      "\n",
      "\n",
      "Progress 42.86%, Time Used 0.82 hours\n",
      "\n",
      "\n",
      "Progress 44.29%, Time Used 0.83 hours\n",
      "\n",
      "\n",
      "Progress 45.71%, Time Used 0.84 hours\n",
      "\n",
      "\n",
      "Progress 47.14%, Time Used 0.84 hours\n",
      "\n",
      "\n",
      "Progress 48.57%, Time Used 0.85 hours\n",
      "\n",
      "\n",
      "Progress 50.00%, Time Used 0.86 hours\n",
      "\n",
      "\n",
      "Progress 51.43%, Time Used 0.87 hours\n",
      "\n",
      "\n",
      "Progress 52.86%, Time Used 0.88 hours\n",
      "\n",
      "\n",
      "Progress 54.29%, Time Used 0.88 hours\n",
      "\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error in problem:  who are the judges on the fisa court\n",
      "\n",
      "Progress 55.71%, Time Used 0.89 hours\n",
      "\n",
      "\n",
      "Progress 57.14%, Time Used 0.90 hours\n",
      "\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error in problem:  the stonewall riot in new york city in 1969 involved a protest by\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error in problem:  the stonewall riot in new york city in 1969 involved a protest by\n",
      "\n",
      "Progress 58.57%, Time Used 0.91 hours\n",
      "\n",
      "\n",
      "Progress 60.00%, Time Used 0.92 hours\n",
      "\n",
      "\n",
      "Progress 61.43%, Time Used 0.93 hours\n",
      "\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error in problem:  when was the death penalty reinstated in oregon\n",
      "\n",
      "Progress 62.86%, Time Used 0.94 hours\n",
      "\n",
      "\n",
      "Progress 64.29%, Time Used 0.94 hours\n",
      "\n",
      "\n",
      "Progress 65.71%, Time Used 0.96 hours\n",
      "\n",
      "\n",
      "Progress 67.14%, Time Used 0.96 hours\n",
      "\n",
      "\n",
      "Progress 68.57%, Time Used 0.97 hours\n",
      "\n",
      "\n",
      "Progress 70.00%, Time Used 0.98 hours\n",
      "\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in problem:  who plays zoey in i love you man\n",
      "\n",
      "Progress 71.43%, Time Used 0.99 hours\n",
      "\n",
      "\n",
      "Progress 72.86%, Time Used 0.99 hours\n",
      "\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error in problem:  how long did the menendez brothers get in prison for killing their parents\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error in problem:  how long did the menendez brothers get in prison for killing their parents\n",
      "\n",
      "Progress 74.29%, Time Used 1.00 hours\n",
      "\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error in problem:  how long did the menendez brothers get in prison for killing their parents\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error in problem:  how long did the menendez brothers get in prison for killing their parents\n",
      "\n",
      "Progress 75.71%, Time Used 1.01 hours\n",
      "\n",
      "\n",
      "Progress 77.14%, Time Used 1.02 hours\n",
      "\n",
      "\n",
      "Progress 78.57%, Time Used 1.03 hours\n",
      "\n",
      "\n",
      "Progress 80.00%, Time Used 1.04 hours\n",
      "\n",
      "\n",
      "Progress 81.43%, Time Used 1.05 hours\n",
      "\n",
      "\n",
      "Progress 82.86%, Time Used 1.06 hours\n",
      "\n",
      "\n",
      "Progress 84.29%, Time Used 1.07 hours\n",
      "\n",
      "\n",
      "Progress 85.71%, Time Used 1.08 hours\n",
      "\n",
      "\n",
      "Progress 87.14%, Time Used 1.09 hours\n",
      "\n",
      "\n",
      "Progress 88.57%, Time Used 1.10 hours\n",
      "\n",
      "\n",
      "Progress 90.00%, Time Used 1.11 hours\n",
      "\n",
      "\n",
      "Progress 91.43%, Time Used 1.11 hours\n",
      "\n",
      "\n",
      "Progress 92.86%, Time Used 1.12 hours\n",
      "\n",
      "\n",
      "Progress 94.29%, Time Used 1.13 hours\n",
      "\n",
      "\n",
      "Progress 95.71%, Time Used 1.14 hours\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "retrieve_answers = []\n",
    "questions_sample = []\n",
    "answers_sample = []\n",
    "num_questions = 7000\n",
    "st = time.time()\n",
    "for idx, qa_problem in enumerate(questions[:num_questions]):\n",
    "    if idx % 100 == 0:\n",
    "        ct = time.time()\n",
    "        print(f\"\\nProgress {idx/num_questions*100:.2f}%, Time Used {(ct-st)/3600:.2f} hours\\n\")\n",
    "    assistant.reset()\n",
    "    try:\n",
    "        with Capturing() as print_output:\n",
    "            ragproxyagent.initiate_chat(assistant, problem=qa_problem, n_results=10)\n",
    "        retrieve_answers.append(print_output[-3])\n",
    "        questions_sample.append(qa_problem)\n",
    "        answers_sample.append(answers[:num_questions][idx])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error in problem: \", qa_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Non controlling interest or minority interest is the portion of a subsidiary corporation's stock that is not owned by the parent corporation. It is shown as a separate line item on the balance sheet under equity.\", 'The fourth season of Chicago Fire has 23 episodes.', 'The Eagles sing Love Will Keep Us Alive.', 'Patrick Walter Brown is the current leader of the Ontario PC Party.', 'The surname Keith has several origins. In some cases it is derived from Keith in East Lothian, Scotland. In other cases, the surname is originated from a nickname, derived from the Middle High German kīt, a word meaning \"sprout\", \"offspring\".']\n",
      "len(retrieve_answers): 6761\n",
      "len(answers_sample): 6761\n",
      "len(questions_sample): 6761\n"
     ]
    }
   ],
   "source": [
    "print(retrieve_answers[:5])\n",
    "print(\"len(retrieve_answers):\", len(retrieve_answers))\n",
    "print(\"len(answers_sample):\", len(answers_sample))\n",
    "print(\"len(questions_sample):\", len(questions_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#F1\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1_recall(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens), int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec), rec\n",
    "\n",
    "def get_gold_answers(example):\n",
    "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\n",
    "    \n",
    "    gold_answers = [answer[\"text\"] for answer in example.answers if answer[\"text\"]]\n",
    "\n",
    "    # if gold_answers doesn't exist it's because this is a negative example - \n",
    "    # the only correct answer is an empty string\n",
    "    if not gold_answers:\n",
    "        gold_answers = [\"\"]\n",
    "        \n",
    "    return gold_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_em_scores = []\n",
    "all_f1_scores = []\n",
    "all_recall_scores = []\n",
    "for i in range(len(retrieve_answers)):\n",
    "    prediction = retrieve_answers[i]\n",
    "    gold_answers = answers_sample[i]\n",
    "\n",
    "    em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
    "    f1_score = max((compute_f1_recall(prediction, answer)[0]) for answer in gold_answers)\n",
    "    recall_score = max((compute_f1_recall(prediction, answer)[1]) for answer in gold_answers)\n",
    "\n",
    "    all_em_scores.append(em_score)\n",
    "    all_f1_scores.append(f1_score)\n",
    "    all_recall_scores.append(recall_score)\n",
    "\n",
    "    # if i % 10 == 0 or recall_score < 0.3:\n",
    "    print(f\"Question: {questions_sample[i]}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"True Answers: {gold_answers}\")\n",
    "    print(f\"EM: {em_score} \\t F1: {f1_score} \\t Recall: {recall_score}\")\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(f\"Average EM: {sum(all_em_scores) / len(all_em_scores)}\")\n",
    "print(f\"Average F1: {sum(all_f1_scores) / len(all_f1_scores)}\")\n",
    "print(f\"Average Recall: {sum(all_recall_scores) / len(all_recall_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Save the current sys.stdout for later restoration\n",
    "original_stdout = sys.stdout\n",
    "\n",
    "# Redirect sys.stdout to a file\n",
    "with open('logs-original-all.txt', 'w') as f:\n",
    "    sys.stdout = f\n",
    "    \n",
    "    for qa_problem in questions[:7000]:\n",
    "        print(f\"\\n\\n>>>>>>>>>>>>>> case: {qa_problem} <<<<<<<<<<<<<<\\n\\n\")\n",
    "        assistant.reset()\n",
    "        try:\n",
    "            ragproxyagent.initiate_chat(assistant, problem=qa_problem, n_results=10)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "\n",
    "# Restore sys.stdout to its original value\n",
    "sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lijiang1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lijiang1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis log file: logs-original-all.txt\n",
      "Total Number of questions: 6775\n",
      "len_lines=344802\n",
      "_cnt_update_context=1597\n",
      "\n",
      "\n",
      "====================== mode='all' ======================\n",
      "Number of questions: 6761\n",
      "Average EM: 0.007691169945274368\n",
      "Average F1: 0.2725437837938005\n",
      "Average Recall: 0.6939445335402984\n",
      "\n",
      "\n",
      "====================== mode='update_context' ======================\n",
      "Number of questions: 422\n",
      "Average EM: 0.004739336492890996\n",
      "Average F1: 0.10725425464574016\n",
      "Average Recall: 0.26880411462104403\n",
      "\n",
      "\n",
      "====================== mode='no_update_context' ======================\n",
      "Number of questions: 6339\n",
      "Average EM: 0.007887679444707366\n",
      "Average F1: 0.28354744072714694\n",
      "Average Recall: 0.7222469876787945\n"
     ]
    }
   ],
   "source": [
    "from analysis_log import main\n",
    "main(\"logs-original-all.txt\", question_process=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
