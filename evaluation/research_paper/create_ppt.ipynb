{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet flaml\"[autogen]\"~=2.0.0 docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import autogen\n",
    "\n",
    "llm_config={\n",
    "    \"request_timeout\": 600,\n",
    "    \"seed\": 44,  # change the seed for different trials\n",
    "    \"config_list\": autogen.config_list_from_json(\"OAI_CONFIG_LIST\",  filter_dict={\n",
    "        \"model\": [\"gpt-4-32k\"],\n",
    "    },),\n",
    "    \"temperature\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent instance named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    is_termination_msg= lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg= lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"work_dir\",\n",
    "        \"use_docker\": [\"python:3.9.6\"],\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Download and save the pdf from the given URL to file called paper.pdf.\n",
      "URL: https://arxiv.org/pdf/2006.14779.pdf to\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To download and save the PDF from the given URL, you can use the following shell script:\n",
      "\n",
      "```sh\n",
      "# filename: download_pdf.sh\n",
      "#!/bin/sh\n",
      "url=\"https://arxiv.org/pdf/2006.14779.pdf\"\n",
      "output_file=\"paper.pdf\"\n",
      "curl -L -o \"$output_file\" \"$url\"\n",
      "```\n",
      "\n",
      "Please save the above code in a file named `download_pdf.sh` and execute it by running the following command in your terminal:\n",
      "\n",
      "```sh\n",
      "sh download_pdf.sh\n",
      "```\n",
      "\n",
      "This will download the PDF from the given URL and save it as `paper.pdf` in the current directory.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 2 (execution failed)\n",
      "Code output: \n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 3340k  100 3340k    0     0   915k      0  0:00:03  0:00:03 --:--:--  914k\n",
      "\n",
      "sh: 0: cannot open download_pdf.sh: No such file\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "I apologize for the confusion. It seems that the file `download_pdf.sh` was not created or not found in the current directory. Let's try another approach using a Python script to download and save the PDF.\n",
      "\n",
      "Please execute the following Python code:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "url = \"https://arxiv.org/pdf/2006.14779.pdf\"\n",
      "output_file = \"paper.pdf\"\n",
      "\n",
      "response = requests.get(url)\n",
      "\n",
      "with open(output_file, \"wb\") as f:\n",
      "    f.write(response.content)\n",
      "\n",
      "print(\"PDF downloaded and saved as\", output_file)\n",
      "```\n",
      "\n",
      "This Python script will download the PDF from the given URL and save it as `paper.pdf` in the current directory.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 1, in <module>\n",
      "    import requests\n",
      "ModuleNotFoundError: No module named 'requests'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "I apologize for the oversight. It seems that the `requests` library is not installed on your system. Let's use the built-in `urllib` library instead. Please execute the following Python code:\n",
      "\n",
      "```python\n",
      "import urllib.request\n",
      "\n",
      "url = \"https://arxiv.org/pdf/2006.14779.pdf\"\n",
      "output_file = \"paper.pdf\"\n",
      "\n",
      "urllib.request.urlretrieve(url, output_file)\n",
      "\n",
      "print(\"PDF downloaded and saved as\", output_file)\n",
      "```\n",
      "\n",
      "This Python script will download the PDF from the given URL and save it as `paper.pdf` in the current directory.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "PDF downloaded and saved as paper.pdf\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Great! The PDF has been successfully downloaded and saved as `paper.pdf` in the current directory. If you have any further questions or need assistance with another task, feel free to ask.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task1 = \"\"\"Download and save the pdf from the given URL to file called paper.pdf.\n",
    "URL: https://arxiv.org/pdf/2006.14779.pdf to\n",
    "\"\"\"\n",
    "\n",
    "user_proxy.initiate_chat(assistant, message=task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Convert the pdf to text and save the text to a file named paper.txt\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To convert the PDF to text, we can use the `PyPDF2` library. First, you need to install the library if you haven't already. You can install it using the following command:\n",
      "\n",
      "```sh\n",
      "pip install PyPDF2\n",
      "```\n",
      "\n",
      "After installing the library, please execute the following Python code to convert the PDF to text and save it to a file named `paper.txt`:\n",
      "\n",
      "```python\n",
      "import PyPDF2\n",
      "\n",
      "input_file = \"paper.pdf\"\n",
      "output_file = \"paper.txt\"\n",
      "\n",
      "with open(input_file, \"rb\") as f:\n",
      "    pdf_reader = PyPDF2.PdfFileReader(f)\n",
      "    num_pages = pdf_reader.numPages\n",
      "    text = \"\"\n",
      "\n",
      "    for page_num in range(num_pages):\n",
      "        page = pdf_reader.getPage(page_num)\n",
      "        text += page.extractText()\n",
      "\n",
      "with open(output_file, \"w\") as f:\n",
      "    f.write(text)\n",
      "\n",
      "print(\"Text extracted and saved as\", output_file)\n",
      "```\n",
      "\n",
      "This Python script will read the `paper.pdf` file, extract the text from it, and save the text to a file named `paper.txt` in the current directory.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Collecting typing_extensions>=3.10.0.0\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing-extensions, PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1 typing-extensions-4.7.1\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 7, in <module>\n",
      "    pdf_reader = PyPDF2.PdfFileReader(f)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/PyPDF2/_reader.py\", line 1974, in __init__\n",
      "    deprecation_with_replacement(\"PdfFileReader\", \"PdfReader\", \"3.0.0\")\n",
      "  File \"/usr/local/lib/python3.9/site-packages/PyPDF2/_utils.py\", line 369, in deprecation_with_replacement\n",
      "    deprecation(DEPR_MSG_HAPPENED.format(old_name, removed_in, new_name))\n",
      "  File \"/usr/local/lib/python3.9/site-packages/PyPDF2/_utils.py\", line 351, in deprecation\n",
      "    raise DeprecationError(msg)\n",
      "PyPDF2.errors.DeprecationError: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "I apologize for the oversight. It seems that the `PdfFileReader` class has been deprecated in the latest version of PyPDF2. Let's use the updated `PdfReader` class instead. Please execute the following Python code:\n",
      "\n",
      "```python\n",
      "import PyPDF2\n",
      "\n",
      "input_file = \"paper.pdf\"\n",
      "output_file = \"paper.txt\"\n",
      "\n",
      "with open(input_file, \"rb\") as f:\n",
      "    pdf_reader = PyPDF2.PdfReader(f)\n",
      "    num_pages = len(pdf_reader.pages)\n",
      "    text = \"\"\n",
      "\n",
      "    for page in pdf_reader.pages:\n",
      "        text += page.extract_text()\n",
      "\n",
      "with open(output_file, \"w\") as f:\n",
      "    f.write(text)\n",
      "\n",
      "print(\"Text extracted and saved as\", output_file)\n",
      "```\n",
      "\n",
      "This Python script will read the `paper.pdf` file, extract the text from it, and save the text to a file named `paper.txt` in the current directory.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Text extracted and saved as paper.txt\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Great! The text has been successfully extracted from the `paper.pdf` file and saved as `paper.txt` in the current directory. If you have any further questions or need assistance with another task, feel free to ask.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task2 = \"Convert the pdf to text and save the text to a file named paper.txt\"\n",
    "user_proxy.initiate_chat(assistant, message=task2, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Based on the text in paper.txt, answer the following question.\n",
      "\n",
      "Question: What it the title of the paper?\n",
      "\n",
      "Hint: Print the contents of the pdf, and you your language ability to answer.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "First, let's print the contents of the `paper.txt` file. Please execute the following Python code:\n",
      "\n",
      "```python\n",
      "file_name = \"paper.txt\"\n",
      "\n",
      "with open(file_name, \"r\") as f:\n",
      "    content = f.read()\n",
      "\n",
      "print(content)\n",
      "```\n",
      "\n",
      "After the content is printed, I will use my language ability to find the title of the paper.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Does the Whole Exceed its Parts? The Effect of AI Explanations\n",
      "on Complementary Team Performance\n",
      "Gagan Bansal∗\n",
      "Tongshuang Wu∗\n",
      "bansalg@cs.washington.edu\n",
      "wtshuang@cs.washington.edu\n",
      "University of WashingtonJoyce Zhou†\n",
      "Raymond Fok†\n",
      "jyzhou15@cs.washington.edu\n",
      "rayfok@cs.washington.edu\n",
      "University of WashingtonBesmira Nushi\n",
      "besmira.nushi@microsoft.com\n",
      "Microsoft Research\n",
      "Ece Kamar\n",
      "eckamar@microsoft.com\n",
      "Microsoft ResearchMarco Tulio Ribeiro\n",
      "marcotcr@microsoft.com\n",
      "Microsoft ResearchDaniel S. Weld\n",
      "weld@cs.washington.edu\n",
      "University of Washington &\n",
      "Allen Institute for Artificial\n",
      "Intelligence\n",
      "ABSTRACT\n",
      "Many researchers motivate explainable AI with studies showing\n",
      "that human-AI team performance on decision-making tasks im-\n",
      "proves when the AI explains its recommendations. However, prior\n",
      "studies observed improvements from explanations only when the\n",
      "AI, alone, outperformed both the human and the best team. Can\n",
      "explanations help lead to complementary performance , where team\n",
      "accuracy is higher than either the human or the AI working solo?\n",
      "We conduct mixed-method user studies on three datasets, where an\n",
      "AI with accuracy comparable to humans helps participants solve a\n",
      "task (explaining itself in some conditions). While we observed com-\n",
      "plementary improvements from AI augmentation, they were notin-\n",
      "creased by explanations. Rather, explanations increased the chance\n",
      "that humans will accept the AI’s recommendation, regardless of its\n",
      "correctness. Our result poses new challenges for human-centered\n",
      "AI: Can we develop explanatory approaches that encourage ap-\n",
      "propriate trust in AI, and therefore help generate (or improve)\n",
      "complementary performance?\n",
      "CCS CONCEPTS\n",
      "•Human-centered computing →Empirical studies in HCI ;\n",
      "Interactive systems and tools ;•Computing methodologies →\n",
      "Machine learning.\n",
      "KEYWORDS\n",
      "Explainable AI, Human-AI teams, Augmented intelligence\n",
      "∗Equal contribution.\n",
      "†Made especially large contributions.\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than the\n",
      "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\n",
      "republish, to post on servers or to redistribute to lists, requires prior specific permission\n",
      "and/or a fee. Request permissions from permissions@acm.org.\n",
      "CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "©2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n",
      "ACM ISBN 978-1-4503-8096-6/21/05. . . $15.00\n",
      "https://doi.org/10.1145/3411764.3445717ACM Reference Format:\n",
      "Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi,\n",
      "Ece Kamar, Marco Tulio Ribeiro, and Daniel S. Weld. 2021. Does the Whole\n",
      "Exceed its Parts? The Effect of AI Explanations on Complementary Team\n",
      "Performance. In CHI Conference on Human Factors in Computing Systems\n",
      "(CHI ’21), May 8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA,\n",
      "16 pages. https://doi.org/10.1145/3411764.3445717\n",
      "1 INTRODUCTION\n",
      "Although the accuracy of Artificial Intelligence (AI) systems is\n",
      "rapidly improving, in many cases, it remains risky for an AI to op-\n",
      "erate autonomously, e.g., in high-stakes domains or when legal and\n",
      "ethical matters prohibit full autonomy. A viable strategy for these\n",
      "scenarios is to form Human-AI teams , in which the AI system aug-\n",
      "ments one or more humans by recommending its predictions, but\n",
      "people retain agency and have accountability on the final decisions.\n",
      "Examples include AI systems that predict likely hospital readmis-\n",
      "sion to assist doctors with correlated care decisions [ 8,13,15,78]\n",
      "and AIs that estimate recidivism to help judges decide whether\n",
      "to grant bail to defendants [ 2,30]. In such scenarios, it is impor-\n",
      "tant that the human-AI team achieves complementary performance\n",
      "(i.e., performs better than either alone): From a decision-theoretic\n",
      "perspective, a rational developer would only deploy a team if it\n",
      "adds utility to the decision-making process [ 73]. For example, sig-\n",
      "nificantly improving decision accuracy by closing deficiencies in\n",
      "automated reasoning with human effort, and vice versa [35, 70].\n",
      "Many researchers have argued that such human-AI teams would\n",
      "be improved if the AI systems could explain their reasoning . In addi-\n",
      "tion to increasing trust between humans and machines or improving\n",
      "the speed of decision making, one hopes that an explanation should\n",
      "help the responsible human know when to trust the AI’s sugges-\n",
      "tion and when to be skeptical, e.g., when the explanation doesn’t\n",
      "“make sense.” Such appropriate reliance [46] is crucial for users to\n",
      "leverage AI assistance and improve task performance [ 10]. Indeed,\n",
      "at first glance, it appears that researchers have already confirmed\n",
      "the utility of explanations on tasks ranging from medical diagno-\n",
      "sis [14,53], data annotation [ 67] to deception detection [ 43]. In\n",
      "each case, the papers show that, when the AI provides explanations,\n",
      "team accuracy reaches a level higher than human-alone.arXiv:2006.14779v3  [cs.AI]  12 Jan 2021CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "Recommendation (R)\n",
      "+ Explanation\n",
      "0.01.0\n",
      "A\n",
      "Input Decision\n",
      "Human AI\n",
      "Complementary zone \n",
      "( max (Human, AI) , 1 ]\n",
      "Teammates\n",
      "Prior work Ours> 0\n",
      "Change of performance\n",
      "0.0Accuracy of decisions1.0\n",
      "BAccuracy of decisions\n",
      "R+Explanation\n",
      "R only> 0R+Explanation\n",
      "R only?Team setting\n",
      "Figure 1: (Best viewed in color) Do AI explanations lead to complementary team performance? In a team setting, when given an\n",
      "input, the human uses (usually imperfect) recommendations from an AI model to make the final decision. We seek to under-\n",
      "stand if automatically generated explanations of the AI’s recommendation improve team performance compared to baselines,\n",
      "such as simply providing the AI’s recommendation, 𝑅, and confidence. (A) Most previous work concludes that explanations\n",
      "improve team performance ( i.e.,Δ𝐴>0); however, it usually considers settings where AI systems are much more accurate\n",
      "than people and even the human-AI team. (B) Our study considers settings where human and AI performance is comparable\n",
      "to allow room for complementary improvement. We ask, “Do explanations help in this context, and how do they compare to\n",
      "simple confidence-based strategies?” (Is Δ𝐵>0?).\n",
      "However, a careful reading of these papers shows another com-\n",
      "monality: in every situation, while explanations are shown to help\n",
      "raise team performance closer to that of the AI, one would achieve\n",
      "an even better result by stripping humans from the loop and let-\n",
      "ting the AI operate autonomously (Figure 1A & Table 1). Thus,\n",
      "the existing work suggests several important open questions for\n",
      "the AI and HCI community: Do explanations help achieve comple-\n",
      "mentary performance by enabling humans to anticipate when the\n",
      "AI is potentially incorrect? Furthermore, do explanations provide\n",
      "significant value over simpler strategies such as displaying the AI’s\n",
      "uncertainty? In the quest to build the best human-machine teams,\n",
      "such questions deserve critical attention.\n",
      "To explore these questions, we conduct new experiments where\n",
      "we control the study design, ensuring that the AI’s accuracy is\n",
      "comparable to the human’s (Figure 1B). Specifically, we measure\n",
      "the human skill on our experiment tasks and then control AI ac-\n",
      "curacy by purposely selecting study samples where AI has compa-\n",
      "rable accuracy. This setting simulates situations where there is a\n",
      "strong incentive to deploy human-AI teams, e.g., because there ex-\n",
      "ists more potential for complementary performance (by correcting\n",
      "each other’s mistakes), and where simple heuristics such as blindly\n",
      "following the AI are unlikely to achieve the highest performance.\n",
      "We selected three common-sense tasks that can be tackled by\n",
      "crowd workers with little training: sentiment analysis of book and\n",
      "beer reviews and a set of LSAT questions that require logical reason-\n",
      "ing. We conducted large-scale studies using a variety of explanation\n",
      "sources (AI versus expert-generated) and strategies (explaining just\n",
      "the predicted class, or explaining other classes as well). We observed\n",
      "complementary performance on every task, but — surprisingly —\n",
      "explanations did not appear to offer benefit compared to simply\n",
      "displaying the AI’s confidence. Notably, explanations increased\n",
      "reliance on recommendations even when the AI was incorrect. Our\n",
      "result echoes prior work on inappropriate trust on systems [ 38,58],\n",
      "i.e., explanations can lead humans to either follow incorrect AIsuggestions or ignore the correct ones [ 13,69]. However, using\n",
      "end-to-end studies, we go one step further to quantify the impact\n",
      "of such over-reliance on objective metrics of team performance.\n",
      "As a first attempt to tackle the problem of blind reliance on AI,\n",
      "we introduce Adaptive Explanation . Our mechanism tries to reduce\n",
      "human trust when the AI has low confidence: it only explains\n",
      "the predicted class when the AI is confident, but also explains\n",
      "the alternative otherwise. While it failed to produce significant\n",
      "improvement in final team performance over other explanation\n",
      "types, there is suggestive evidence that the adaptive approach can\n",
      "push the agreement between AI predictions and human decisions\n",
      "towards the desired direction.\n",
      "Through extensive qualitative analysis, we also summarize po-\n",
      "tential factors that should be considered in experimental settings for\n",
      "studying human-AI complementary performance. For example, the\n",
      "difference in expertise between human and AI affects whether (or\n",
      "how much) AI assistance will help achieve complementary perfor-\n",
      "mance, and the display of the explanation may affect the human’s\n",
      "collaboration strategy. In summary:\n",
      "•We highlight an important limitation of previous work on\n",
      "explainable AI: While many studies show that explaining\n",
      "predictions of AI increases team performance (Table 1), they\n",
      "all consider cases where the AI system is significantly more\n",
      "accurate than both the human partner and the human-AI\n",
      "team. In response, we argue that AI explanations for decision-\n",
      "making should aim for complementary performance, where\n",
      "the human-AI team outperforms both solo human and AI.\n",
      "•To study complementary performance, we develop a new\n",
      "experimental setup and use it in studies with 1626 users on\n",
      "three tasks1to evaluate a variety of explanation sources and\n",
      "strategies. We observe complementary performance in every\n",
      "human-AI teaming condition.\n",
      "1All the task examples and the collected experiment data are available at https://github.\n",
      "com/uw-hai/Complementary-Performance.Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "•However, surprisingly, we do not observe any significant\n",
      "increase in team performance by communicating explana-\n",
      "tions, compared to simply showing the AI’s confidence. Ex-\n",
      "planations often increased accuracy when the AI system\n",
      "was correct but, worryingly, decreased it when the AI erred,\n",
      "resulting in a minimal net change — even for our adaptive\n",
      "explanations. Through qualitative analysis, we discuss poten-\n",
      "tial causes for failure of explanations, behavioral differences\n",
      "among tasks, and suggest directions for developing more\n",
      "effective AI explanations.\n",
      "2 BACKGROUND AND RELATED WORK\n",
      "Explanations can be useful in many scenarios where a human\n",
      "and AI interact: transparently communicating model predictions\n",
      "[10,21,38,40,66], teaching humans tasks like translation [ 3,26]\n",
      "or content moderation [ 36], augmenting human analysis proce-\n",
      "dure [ 36] or creativity [ 17], legal imperatives [ 57,77], etc. Various\n",
      "studies have evaluated the effect of explanations from different di-\n",
      "mensions, including whether the explanation improves users’ trust\n",
      "in the AI [ 41,81] or enables users to simulate the model predic-\n",
      "tions [16, 65], or assists developers to debug models [9, 38].\n",
      "In this paper, we focus explicitly on AI-assisted decision making\n",
      "scenarios [ 6,74], where an AI assistant ( e.g., a classification model)\n",
      "makes recommendations to a human ( e.g., a judge), who is respon-\n",
      "sible for making final decisions ( e.g., whether or not to grant bail).\n",
      "In particular, we assess performance in terms of the accuracy of the\n",
      "human-AI team. While other metrics can be used for evaluation\n",
      "(more discussed in Section 6.1), we directly evaluate end-to-end\n",
      "team accuracy for three reasons. First, deploying such a human-AI\n",
      "team is ideal if it achieves complementary performance ,i.e., if it\n",
      "outperforms both the AI and the human acting alone. Second, eval-\n",
      "uating explanations using proxy tasks (such as whether humans\n",
      "can use it to guess the model’s prediction) can lead to different,\n",
      "misleading conclusions for achieving best team performance than\n",
      "an end-to-end evaluation [ 12]. Third, AI-assisted decision making\n",
      "is often listed as a major motivation for AI explanations. In recent\n",
      "years numerous papers have employed user studies to show that\n",
      "human accuracy increases if the AI system explains its reasoning\n",
      "for tasks as diverse as medical diagnosis, predicting loan defaults,\n",
      "and answering trivia questions. However, as summarized in Table\n",
      "1, complementary performance was not observed in any of these\n",
      "studies – in each case, adding the human to the loop decreased\n",
      "performance compared to if AI had acted alone.\n",
      "For example, in Lai et al. [42,43], MTurk workers classified de-\n",
      "ceptive hotel reviews with predictions from SVM and BERT-based\n",
      "models, as well as explanations in the form of inline-highlights.\n",
      "However, models outperformed every team (see Table 1 and Fig-\n",
      "ure 6 in [ 42]). Zhang et al. [83] noticed the superior behavior of\n",
      "the models in Lai et al. ’s work, and evaluated the accuracy and\n",
      "trust calibration where the gap between human and the AI per-\n",
      "formances was less severe. Still, on their task of income category\n",
      "prediction, their Gradient Boosted Trees model had 10% higher ac-\n",
      "curacy compared to their MTurk workers, which seemed borderline\n",
      "“comparable” at best. Furthermore, when run autonomously, their\n",
      "AI model performed just slightly better than the best team (see\n",
      "Section 4.2.2 and Figure 10 in [ 83]). A similar performance trend isobserved on tasks other than classification. In Sangdeh et al. [65],\n",
      "MTurk workers predicted house price using various regression mod-\n",
      "els that generated explanations in terms of most salient features.\n",
      "Their models’ predictions resulted in lowest error (See Figure 6\n",
      "in [65]). In Feng et al. [21], experts and novices played Quiz Bowl\n",
      "with recommendation from Elastic Search system. The system ex-\n",
      "plained its predictions by presenting training examples that were\n",
      "influential, and using inline-highlights to explain the connection\n",
      "between question and evidence. However, Feng et al. do not report\n",
      "the exact performance of the AI on their study sample, but mention\n",
      "its superiority in Section 3.1 in [ 21] pointing out that it outper-\n",
      "forms top trivia players. One possible exception is Bligic & Mooney\n",
      "(2005) [ 10], who probably achieved complementary performance on\n",
      "their task of recommending books to users. However, they did not\n",
      "compare explanations against simple baselines, such as showing\n",
      "the book title or the system confidence (rating).\n",
      "At least two potential causes account for the absence of comple-\n",
      "mentary performance in these cases. First, task design may have\n",
      "hindered collaboration: previous researchers considered AI systems\n",
      "whose accuracy was substantially higher than the human’s, leading\n",
      "to a small zone with potential for complementary performance\n",
      "(see Figure 1). For example, this may have made it more likely that\n",
      "human errors were a superset of the AI’s, reducing the possibility of\n",
      "a human overseer spotting a machine mistake. Second, even when\n",
      "the task has the potential for complementary performance, it is\n",
      "unclear if the collaboration mechanisms under study supported it.\n",
      "Collaboration factors like incentives, the format of explanations,\n",
      "and whether AI’s uncertainty was displayed may drive the human\n",
      "towards simple, less collaborative heuristics, such as “always trust\n",
      "the AI” or “never trust the AI.”\n",
      "3 SETUP AND PILOT STUDIES\n",
      "To better understand the role of explanations in producing com-\n",
      "plementary performance, we enlarge the zone of potential comple-\n",
      "mentarity by matching AI accuracy to that of an average human,2\n",
      "and investigate multiple explanation styles on several domains (Sec-\n",
      "tion 3.1). As Table 2 summarizes, we first designed and conducted\n",
      "pilots studies (Sections 3.2) and used them to inform our final study\n",
      "and hypotheses (Section 4).\n",
      "3.1 Choice of Tasks and Explanations\n",
      "Since our interest is in AI-assisted decision making , we studied\n",
      "the effect of local explanations on team performance – that is,\n",
      "explaining each individual recommendation made by a model [ 66].\n",
      "This contrasts with providing a global understanding of the full\n",
      "model all at once ( e.g., [45]).\n",
      "We conducted experiments on two types of tasks: text classifi-\n",
      "cation (sentiment analysis) and question answering. Text classifi-\n",
      "cation because it is a popular task in natural language processing\n",
      "(NLP) that has been used in several previous studies on human-AI\n",
      "2Of course, complementary performance may be possible even in situations when one\n",
      "of the team partners is significantly more accurate than the other. For example, a low-\n",
      "accuracy team member may be valuable if their errors are independent, because they\n",
      "may be able to spot mistakes made by the team majority. However, it is more difficult\n",
      "to observe complementary performance in such settings, so we first consider the case\n",
      "where humans and AI have similar accuracy. If explanations cannot provide value in\n",
      "such settings, it will be even more difficult to show complementary performance when\n",
      "teammates have disparate skills.CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "Domain TaskPerformance\n",
      "Metric Human alone AI alone Team Complementary?\n",
      "ClassificationDeceptive review [43] Accuracy ↑ 51.1% 87.0% 74.6% ✗\n",
      "Deceptive review [42] Accuracy ↑ 54.6% 86.3% 74.0% ✗\n",
      "Income category [83] Accuracy ↑ 65% 75% 73% ✗\n",
      "Loan defaults [27] Norm. Brier ↑ 0 1 0.682 ✗\n",
      "Hypoxemia risk [53] AUC ↑ 0.66 0.81 0.78 ✗\n",
      "Nutrition prediction [12] Accuracy ↑ 0.46 0.75 0.74 ✗\n",
      "QA Quiz bowl [21] “AI outperforms top trivia players. ” ✗\n",
      "Regression House price [65] Avg. Absolute Error ↓ $331k $200k $232k ✗\n",
      "Table 1: Recent studies that evaluate the effect of automatically generated explanations on human-AI team performance.\n",
      "While explanations did improve team accuracy, the performance was not complementary — acting autonomously, the AI\n",
      "would have performed even better. For papers with multiple domains or experiments, we took one sample with the most\n",
      "comparable human and AI performance. ↑(or↓) indicates whether the metric should be maximized (or minimized).\n",
      "Explain. Strategies Explain. Sources Tasks\n",
      "Explain-Top-1 AI Beer\n",
      "Explain-Top-2 Expert Amzbook\n",
      "Adaptive LSAT\n",
      "Table 2: An overview of our tasks, explanation strategies and\n",
      "sources. We ran our pilot studies (Section 3.2) with condi-\n",
      "tions marked with . Based on the pilot results, we added\n",
      "adaptive explanations and expert explanations (Section 3.3).\n",
      "Along with two additional domains, these form the condi-\n",
      "tions for our final study conditions (Section 4.1).\n",
      "teaming [ 21,29,42,50,62,83] and because it requires little domain\n",
      "expertise, and is thus amenable to crowdsourcing. Specifically, we\n",
      "selected two sentiment analysis datasets to improve the generaliza-\n",
      "tion of our results: beer reviews [ 56] and book reviews [ 31]. More\n",
      "details about these datasets are in Section 4.2. While there exist\n",
      "various local explanation approaches for text classification, we rely\n",
      "onlocal saliency explanations , which explain a single prediction in\n",
      "terms of the importance of input features ( e.g., each word) towards\n",
      "the model’s prediction ( e.g., positive or negative sentiment).\n",
      "As commonly practiced in previous work [ 21,42,43], we dis-\n",
      "play explanations with inline-highlights ,i.e., directly highlighting\n",
      "the explanation in the input text, so the user need not go back\n",
      "and forth between input and the explanation. While there exist\n",
      "other explanatory approaches, such as feature-importance visual-\n",
      "ization [ 11,27,53,61,76] (more suitable for tabular data) or com-\n",
      "municating influential training examples [ 3,40,80] (more suitable\n",
      "for images), these techniques are not ideal for text because they\n",
      "add an additional cognitive cost to mapping the explanation to the\n",
      "respective text. Figure 2 shows one example beer review.\n",
      "We also experimented with Law School Admission Test (LSAT)\n",
      "questions3because it is more challenging. In this task, every ques-\n",
      "tion contains four options with a unique correct answer (Figure 3).\n",
      "Again, answering LSAT questions requires no specialized knowl-\n",
      "edge except common-sense reasoning skills, such as recognizing\n",
      "logical connections and conflicts between arguments [ 82]. Because\n",
      "3https://en.wikipedia.org/wiki/LawSchoolAdmissionTestin this case it is unclear how inline-highlights could be used to com-\n",
      "municate logical constructs ( e.g., contradiction may not be visible\n",
      "by highlighting the input alone), we turned to narrative explana-\n",
      "tions which justify a candidate answer in natural language. We\n",
      "explain these in more detail in Section 4.2.\n",
      "3.2 Pilot Study on Sentiment Classification\n",
      "To iterate on the hypotheses and the associated explanation condi-\n",
      "tions for our main study (detailed later in Section 4), we conducted\n",
      "a pilot study on one of our datasets ( Beer). The between-subject\n",
      "pilot study asked crowdworkers to judge the sentiment of 50 beer\n",
      "reviews with assistance from a logistic regression classifier in three\n",
      "conditions, each condition with 50 workers. One condition only\n",
      "showed the model prediction and confidence; the other two also\n",
      "included the following common explanation strategies4:\n",
      "(1)Explain-Top-1 explains just the predicted class by highlight-\n",
      "ing the most influential words for that class.\n",
      "(2)Explain-Top-2 explains the top two predicted classes, and un-\n",
      "like Explain-Top-1, it also color codes and highlights words\n",
      "for the other sentiment class.\n",
      "The two strategies closely align with the design in prior work [ 24,\n",
      "43,49,75], and have been shown to be beneficial (Table 1). Explain-\n",
      "Top-2 also corresponds to Wang et al. ’s suggestion to mitigate\n",
      "heuristic biases by explaining “multiple outcomes” [74].\n",
      "Observations We summarize our findings from the pilot study:\n",
      "(1)Contrary to many prior works, we observed no significant\n",
      "changes or improvements in aggregated team accuracy by\n",
      "displaying either type of explanations.\n",
      "(2)That said, explaining just the predicted class (Explain-Top-1)\n",
      "performed better than explaining both (Explain-Top-2) .\n",
      "(3)We also observed that explanations increased reliance on rec-\n",
      "ommendations even when they were incorrect : explaining the\n",
      "predicted class slightly improved performance (compared to\n",
      "confidence only) when the recommendation was correct but\n",
      "decreased performance when it was incorrect.\n",
      "(4)This effect was less pronounced in Explain-Top-2, presum-\n",
      "ably because it encouraged users to consider alternatives and\n",
      "4the saliency scores were based on feature weights learned by the linear model [ 27,42]Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "hence deterred over-reliance. In Figure 2, for example, if\n",
      "counter-argument (d) was not highlighted, participants could\n",
      "easily stop reading at the highlighted first sentence and over-\n",
      "look the negative ending.\n",
      "(5)Finally, participants indicated that they wanted higher quality\n",
      "explanations. Crowd-workers were confused when explana-\n",
      "tions did not seem to correlate with model behavior.\n",
      "Because we made similar observations in our main study, we\n",
      "defer detailed discussions and implications of these observations to\n",
      "Section 5.1 and Figure 4.\n",
      "3.3 Additional Explanation Strategies/Sources\n",
      "Added Strategy: Adaptive Explanations. The pilot study indi-\n",
      "cated that Explain-Top-2 was more beneficial than Explain-Top-1\n",
      "when the classifier made mistakes, but not otherwise. Relying on the\n",
      "commonly seen correlations between mistakes and low-confidence\n",
      "[32], we developed a new dynamic strategy, adaptive explanation ,\n",
      "that switches between Explain-Top-1 and Explain-Top-2 depending\n",
      "on the AI’s confidence. This method explains the top-two classes\n",
      "only when the classifier confidence is below a task- and model-\n",
      "specific threshold (described later in Section 4.2), explaining only\n",
      "the top prediction otherwise. Intuitively, it was inspired by an effi-\n",
      "cient assistant that divulges more information (confessing doubts\n",
      "and arguing for alternatives) only when it is unsure about its recom-\n",
      "mendation. Adaptive explanations can also be viewed as changing\n",
      "explanation according to context [1]. While we limit the our context\n",
      "to the AI’s confidence, in general, one could rely on more features\n",
      "of the human-AI team, such as the user, location, or time [ 34,48].\n",
      "Added Source: Expert-Generated Explanations. Users in our\n",
      "pilot study were confused when the explanations did not make in-\n",
      "tuitive sense, perhaps due to either the quality of the underlying\n",
      "linear model-based AI. While we test state-of-the-art models in\n",
      "the final study, we also added expert-generated explanations to\n",
      "serve as an upper bound on explanation quality. We describe their\n",
      "annotation process in Section 4.2.\n",
      "4 FINAL STUDY\n",
      "Based on our pilot studies, we formulated our final hypotheses and\n",
      "used them to inform our final conditions and their interface (Sec-\n",
      "tion 4.1). We then tested these hypotheses for several tasks and AI\n",
      "systems (Section 4.2) through crowdsourcing studies (Section 4.3).\n",
      "4.1 Hypotheses, Conditions, and Interface\n",
      "We formulated the following hypotheses for sentiment analysis:\n",
      "H1Among current explanation strategies, explaining the predicted\n",
      "class will perform better than explaining both classes.\n",
      "H2The better strategies, Explain-Top-1, will still perform similarly\n",
      "to simply showing confidence.\n",
      "H3Our proposed Adaptive explanations, which combines benefits\n",
      "of existing strategies, will improve performance.\n",
      "H4Adaptive explanations would perform even better if AI could\n",
      "generate higher quality explanations.\n",
      "Since generating AI explanations for LSAT was not feasible (Sec-\n",
      "tion 3.3), we slightly modified the hypothesis for LSAT : we omitted\n",
      "the hypothesis on explanation quality ( H4) and tested the first three\n",
      "hypotheses using expert- rather than AI-generated explanations.Conditions .For both domains, we ran two baseline conditions:\n",
      "unassisted users ( Human ), as well as a simple AI assistance that\n",
      "shows the AI’s recommendation and confidence but no explanation\n",
      "(Team (Conf) ). We use this simple assistance because it serves as\n",
      "a stronger and broadly acknowledged baseline than the alterna-\n",
      "tive, i.e., displaying AI’s recommendation without confidence. First,\n",
      "most ML models can generate confidence scores that, in practice,\n",
      "correlate with the model’s true likelihood to err [ 32]. Second, dis-\n",
      "playing uncertainty in predictions can help users can make more\n",
      "optimal decisions [ 19,22,25,37,60]. Hence, we focus on evaluating\n",
      "whether the explanations provide additional value when shown\n",
      "alongside confidence scores. In the rest of the paper, we indicate the\n",
      "explanation conditions using the following template: Team (Strat-\n",
      "egy, Source) . For example, Team (Explain-Top-1, AI) indicates the\n",
      "condition that shows the AI’s explanations for the top prediction.\n",
      "Interface .Figure 2 shows an example UI for sentiment clas-\n",
      "sification for Team (Adaptive, AI). In all explanation conditions,\n",
      "explanations are displayed as inline highlights, with the background\n",
      "color aligned with the positive/negative label buttons. The highlight\n",
      "varies by condition, e.g., Team (Adaptive, AI) has a similar display to\n",
      "Figure 2, except that the AI picks multiple short phrases, instead of\n",
      "a full sentence. In Team (Explain-Top-1, AI) the counter-argument\n",
      "(d) is always missing, and in Team (Conf) no explanations are high-\n",
      "lighted. Figure 3 shows a screenshot of the user interface for LSAT\n",
      "in the Team (Adaptive, Expert) condition.\n",
      "4.2 AI Model, Study Samples and Explanations\n",
      "4.2.1 Sentiment Classification.\n",
      "Training data. To prepare each dataset ( Beer and Amzbook ) for\n",
      "training classification models, we binarized the target labels, split\n",
      "the dataset into training and test sets (80/20 split), removed class\n",
      "imbalance from the train split by oversampling the minority class,\n",
      "and further split the training set to create a validation set.\n",
      "AI Model. For each dataset, we fine-tuned a RoBERTa-based [ 52]\n",
      "text classifier from AllenNLP5on the training dataset and per-\n",
      "formed hyper-parameter selection on the validation set.\n",
      "Task examples. For each domain, we selected 50 examples from\n",
      "the test set to create our study sample. We first conducted additional\n",
      "pilot studies to establish the accuracy of unassisted users, which we\n",
      "observed were 87% for Beer and 85% for Amzbook6We then selected\n",
      "50 unambiguous examples so that the AI’s accuracy was 84% ( i.e.,\n",
      "comparable to human accuracy), with equal false positive and false\n",
      "negative rates. The filtering was for keeping the task objective:\n",
      "If the ground-truth answer was unclear, one cannot compute or\n",
      "compare the accuracy of decisions.\n",
      "Explanations. To generate saliency explanations, we used LIME,\n",
      "which is a popular post hoc method [ 66]. We chose this setup be-\n",
      "cause the combination of RoBERTa and LIME was consistently\n",
      "ranked the highest among the various systems we tried in an ex-\n",
      "plainer comparison study with human judges (details in Appendix).\n",
      "Despite offering accurate predictions, RoBERTa generated poorly\n",
      "calibrated confidence scores, a common issue with neural networks\n",
      "5https://demo.allennlp.org/sentiment-analysis\n",
      "6Again, each condition containing 50 crowd-workers. We estimated the human accu-\n",
      "racy on all the three datasets with another 150 crowd-workers.CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "a\n",
      "bc\n",
      "d\n",
      "Figure 2: A screenshot of the Team (Adaptive, Expert) condition for the Amzbook reviews dataset. Participants read the review\n",
      "(left pane) and used the buttons (right pane) to decide if the review was mostly positive ornegative . The right pane also shows\n",
      "progress and accuracy (a). To make a recommendation, the AI (called “Marvin”) hovers above a button (b) and displays the\n",
      "confidence score under the button. In this case, the AI incorrectly recommended that this review was positive, with confidence\n",
      "62.7%. As part of the explanation, the AI highlighted the most positive sentence (c) in the same color as the positive button.\n",
      "Because confidence was low, the AI also highlights the most negative sentence (d) to provide a counter-argument.\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "0 / 20\n",
      "Figure 3: A screenshot of Team (Adaptive, Expert) for LSAT . Similar to Figure 2, the interface contained a progress indicator (a),\n",
      "AI recommendation (b), and explanations for the top-2 predictions (c and d). To discourage participants from blindly following\n",
      "the AI, all AI information is displayed on the right. In (b), the confidence score is scaled so those for top-2 classes sum to 100%.\n",
      "[28], which we mitigated with post hoc calibration (isotonic regres-\n",
      "sion [7]) on the validation set.\n",
      "In particular, for Adaptive explanation, we used the classifier’s\n",
      "median confidence as the threshold to have an equal number of\n",
      "25 examples displayed as Explain-Top-1 and Explain-Top-2, respec-\n",
      "tively. The thresholds were 89.2% for Beer and 88.9% for Amzbook .\n",
      "We happened to explain 18 correctly predicted and 7 incorrectly\n",
      "predicted examples with Explain-Top-2 for both datasets (leaving\n",
      "1 incorrect and 24 correct cases with Explain-Top-1). While one\n",
      "might learn a better threshold from the data, we leave that to future\n",
      "work. As for expert-generated explanations, one author created\n",
      "expert explanations by selecting one short, convincing text phrase\n",
      "span for each class (positive or negative).4.2.2 LSAT.\n",
      "AI Model. We finetuned a RoBERTa model7on ReClor [ 82], a logic-\n",
      "reasoning dataset that contains questions from standardized exams\n",
      "like the LSAT and GMAT.8\n",
      "Task examples. We selected 20 examples from an LSAT prep\n",
      "book [ 71]. We verified that our questions were not easily searchable\n",
      "online and were not included in the training dataset. We selected\n",
      "fewer LSAT questions than for sentiment analysis, because they\n",
      "are more time consuming to answer and could fatigue participants:\n",
      "LSAT questions took around a minute to answer, compared to\n",
      "around 17 seconds for Beer and Amzbook . The RoBERTa model\n",
      "achieved 65% accuracy on these examples, comparable to the 67%\n",
      "human accuracy that we observed in our pilot study.\n",
      "7Based on the opensource implementation: https://github.com/yuweihao/reclor.\n",
      "8https://en.wikipedia.org/wiki/GraduateManagementAdmissionTestDoes the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "Explanations. We found no automated method that could gen-\n",
      "erate reasonable explanations (unsurprising, given that explana-\n",
      "tions rely on prior knowledge and complex reasoning); instead, we\n",
      "used expert explanations exclusively, which is again based on the\n",
      "prep book. The book contains explanations for the correct answer,\n",
      "which one author condensed to a maximum of two sentences. Since\n",
      "the book did not provide explanations for alternative choices, we\n",
      "created these by manually crafting a logical supporting argument\n",
      "for each choice that adhered to the tone and level of conciseness of\n",
      "the other explanations. Experts only generated explanations and\n",
      "did not determine the model predictions or its uncertainties.\n",
      "4.3 Study Procedure\n",
      "Sentiment Classification .For the final study, participants went\n",
      "through the following steps: 1) A landing page first explained the\n",
      "payment scheme; the classification task was presented (here, pre-\n",
      "dicting the sentiment of reviews); and they were shown dataset-\n",
      "specific examples. 2) To familiarize them with the task and verify\n",
      "their understanding, a screening phase required the participant\n",
      "to correctly label four of six reviews [ 51]. Only participants who\n",
      "passed the gating test were allowed to proceed to the main task. 3)\n",
      "The main task randomly assigned participants to one of our study\n",
      "conditions (Section 3.3) and presented condition-specific instruc-\n",
      "tions, including the meaning and positioning of AI’s prediction,\n",
      "confidence, and explanations. Participants then labeled all 50 study\n",
      "samples one-by-one. For a given dataset, all conditions used the\n",
      "same ordering of examples. The participants received immediate\n",
      "feedback on their correctness after each round of the task. 4) A\n",
      "post-task survey was administered, asking whether they found the\n",
      "model assistance to be helpful, their rating of the usefulness of\n",
      "explanations in particular (if they were present), and their strategy\n",
      "for using model assistance.\n",
      "We recruited participants from Amazon’s Mechanical Turk, lim-\n",
      "iting the pool to subjects from within the United States with a\n",
      "prior task approval rating of at least 97% and a minimum of 1,000\n",
      "approved tasks. To ensure data quality, we removed data from par-\n",
      "ticipants whose median labeling time was less than 2 seconds or\n",
      "those who assigned the same label to all examples. In total, we\n",
      "recruited 566 ( Beer) and 552 ( Amzbook ) crowd workers, and in both\n",
      "datasets, 84% of participants passed the screening and post-filtering.\n",
      "Eventually, we collected data from around 100 participants (ranging\n",
      "from 93 to 101 due to filtering) per condition.\n",
      "Study participants received a base pay of $0.50 for participating,\n",
      "a performance-based bonus for the main task, and a fixed bonus of\n",
      "$0.25 for completing the survey. Our performance-based bonus was\n",
      "a combination of linear and step functions on accuracy: we gave\n",
      "$0.05 for every correct decision in addition to an extra $0.50 if the to-\n",
      "tal accuracy exceeded 90% or $1.00 if it exceeded 95%. The assigned\n",
      "additional bonuses were intended to motivate workers to strive for\n",
      "performance in the complementary zone and improve over the AI-\n",
      "only performance [ 33]. Since we fixed the AI performance at 84%,\n",
      "humans could not obtain the bonus by blindly following the AI’s\n",
      "recommendations. Participants spent 13 minutes on average on the\n",
      "experiment and received an average payment of $3.35 (equivalent\n",
      "to an hourly wage of $15.77).Modifications for LSAT .For the LSAT dataset, we used a very\n",
      "similar procedure but used two screening questions and required\n",
      "workers to answer both correctly. We used a stricter passing re-\n",
      "quirement to avoid low-quality workers who might cheat, which\n",
      "we observed more for this task in our pilots. We again used MTurk\n",
      "with the same filters as sentiment classification, and we post hoc\n",
      "removed data from participants whose median response time was\n",
      "less than three seconds. 508 crowd workers participated in our\n",
      "study, 35% of whom passed the screening and completed the main\n",
      "task, resulting in a total of 100 participants per condition.\n",
      "Participants received a base pay of $0.50 for participating, a\n",
      "performance-based bonus of $0.30 for each correct answer in the\n",
      "main task, and a fixed bonus of $0.25 for completing an exit survey.\n",
      "They received an additional bonus of $1.00, $2.00, and $3.00 for\n",
      "reaching an overall accuracy of 30%, 50%, and 85% to motivate\n",
      "workers to answer more questions correctly and perform their best.\n",
      "The average completion time for the LSAT task was 16 minutes,\n",
      "with an average payment of $6.30 (equals an hourly wage of $23.34).\n",
      "5 RESULTS\n",
      "5.1 Effect of Explanation on Team performance\n",
      "Figure 4A shows the team performance ( i.e., accuracy of final deci-\n",
      "sion) for each domain and condition. We tested the significance of\n",
      "our results using Student’s T-tests with Bonferroni correction.\n",
      "The baseline team condition, Team (Conf), achieved com-\n",
      "plementary performance across tasks. ForBeer, providing AI\n",
      "recommendations and confidence to users increased their perfor-\n",
      "mance to ( 𝜇=0.89±𝜎=0.05), surpassing both AI ( 0.84) and\n",
      "unassisted human accuracy ( 0.82±0.09). Similarly, Team (Conf)\n",
      "achieved complementary performance for Amzbook andLSAT , with\n",
      "relative gains of 2.2% and 20.1% over unassisted workers (Figure 4A).\n",
      "We did not observe a significant difference between Explain-\n",
      "Top-1 and Explain-Top-2 , or that H1was not supported. For ex-\n",
      "ample, in Figure 4A of Beer, explaining the top prediction performed\n",
      "marginally better than explaining the top-two predictions, but the\n",
      "difference was not significant ( 𝑧=0.85, 𝑝=.40). The same was true\n",
      "forAmzbook (𝑧=0.81, 𝑝=.42) and LSAT (𝑧=0.42, 𝑝=.68).\n",
      "We did not observe significant improvements over the con-\n",
      "fidence baseline by displaying explanations. For example, for\n",
      "Beer, Team (Conf) and Team (Explain-Top-1, AI) achieved similar\n",
      "performance, with the accuracy being 0.89±0.05vs.0.88±0.06\n",
      "respectively; the difference was insignificant ( 𝑧=-1.18, 𝑝=.24). We\n",
      "observed the same pattern for Amzbook (𝑧=1.23, 𝑝=.22) and LSAT\n",
      "(𝑧=0.427, 𝑝=.64). As a result, we could not reject our hypothesis H2\n",
      "that Explain-Top-1 performs similar to simply showing confidence\n",
      ". This result motivates the need to develop new AI systems and ex-\n",
      "planation methods that provide true value to team performance by\n",
      "supplementing the model’s confidence, perhaps working in tandem\n",
      "with confidence scores.\n",
      "Though designed to alleviate the limitations of Explain-Top-1\n",
      "and Explain-Top-2 in our experiments, we did not observe im-\n",
      "provements from using Adaptive explanations. For example,\n",
      "we did not observe any significant differences between Team (Adap-\n",
      "tive, AI) and Team (Conf) for Beer (𝑧=-1.02, 𝑝=.31) or Amzbook\n",
      "(𝑧=1.08, 𝑝=.28). We did not observe significant differences betweenCHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "0.75 0.80 0.85 0.90 0.951.00\n",
      "DecisionAccuracyHuman\n",
      "Team (Conf)\n",
      "Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)conditionBeerDecisionAccuracy\n",
      "0.75 0.80 0.85 0.90 0.951.00\n",
      "DecisionAccuracyHuman\n",
      "Team (Conf)\n",
      "Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)conditionAmzbookDecisionAccuracy\n",
      "0.500.55 0.60 0.65 0.70 0.75\n",
      "DecisionAccuracyHuman\n",
      "Team (Conf)\n",
      "Team (ExplainTop-1,Exper t)\n",
      "Team (ExplainTop-2,Exper t)\n",
      "Team (Adapt ive,Expert)conditionLSATDecisionAccuracy0.40.5 0.6 0.7 0.8 0.91.0\n",
      "DecisionAccuracyHuman\n",
      "Team (Conf)\n",
      "Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)conditionBeerDecisionAccuracy(split)\n",
      "0.40.5 0.6 0.7 0.8 0.91.0\n",
      "DecisionAccuracyHuman\n",
      "Team (Conf)\n",
      "Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)conditionAmzbookDecisionAccuracy(split)\n",
      "0.30.4 0.5 0.6 0.7 0.80.9\n",
      "DecisionAccuracyHuman\n",
      "Team (Conf)\n",
      "Team (ExplainTop-1,Exper t)\n",
      "Team (ExplainTop-2,Exper t)\n",
      "Team (Adapt ive,Expert)conditionLSATDecisionAccuracy(split)correct incorrect AIrecomme ndation AI accuracy Complementary zoneA B\n",
      "Figure 4: Team performance (with average accuracy and 95% confidence interval) achieved by different explanation conditions\n",
      "and baselines for three datasets, with around 100 participants per condition. (A) Across every dataset, all team conditions\n",
      "achieved complementary performance. However, we did not observe significant improvements from using explanations over\n",
      "simply showing confidence scores. (B) Splitting the analysis based on the correctness of AI accuracy, we saw that for Beer and\n",
      "LSAT , Explain-Top-1 explanations worsened performance when the AI was incorrect, the impact of Explain-Top-1 and Explain-\n",
      "Top-2 explanations were correlated with the correctness of the AI’s recommendation, and Adaptive explanations seemed to\n",
      "have the potential to improve Explain-Top-1 when the AI was incorrect, and to retain the higher performance of Explain-Top-1\n",
      "when the AI was correct.\n",
      "Team (Adaptive, Expert) and Team (Conf) for LSAT (𝑧=0.16, 𝑝=.87).\n",
      "More surprisingly, switching the source of Adaptive explanation to\n",
      "expert-generated did not significantly improve sentiment analysis\n",
      "results. For example, in Figure 4A, the differences in performance\n",
      "between Team (Adaptive, Expert) and Team (Adaptive, AI) were\n",
      "insignificant: Beer (𝑧=1.31, 𝑝=.19) and Amzbook (𝑧=-0.78, 𝑝=.43). As\n",
      "such, we could not reject the null hypotheses for either H3orH4.\n",
      "While Adaptive explanation did not significantly improve\n",
      "team performance across domains, further analysis may point\n",
      "a way forward by combining the strengths of Explain-Top-1\n",
      "and Explain-Top-2. Split the team performance by whether the\n",
      "AI made a mistake (Figure 4B), we observe that explaining the top\n",
      "prediction lead to better accuracy when the AI recommendation\n",
      "was correct but worse when the AI was incorrect, as in our pilot\n",
      "study. This is consistent with Psychology literature [ 39], which has\n",
      "shown that human explanations cause listeners to agree even when\n",
      "the explanation is wrong, and recent studies that showed explana-\n",
      "tions can mislead data scientists into overtrusting ML models for\n",
      "deployment [ 38]. While these results were obtained by measuringuser’s subjective ratings of trust, to the best of our knowledge, our\n",
      "studies are the first to show this phenomenon for explanation and\n",
      "end-to-end decision making with large-scale studies. As expected,\n",
      "inBeer, Adaptive explanations improved performance over Explain-\n",
      "Top-1 when the AI was incorrect and improved performance over\n",
      "Explain-Top-2 when the AI was correct, although the effect was\n",
      "smaller on other datasets.\n",
      "While Figure 4B shows team performance, the promising effects\n",
      "of Adaptive explanations are clearer if we study the agreement\n",
      "between AI predictions and human decisions (Figure 5). Adaptive\n",
      "explanations seem to encourage participants to consider the AI\n",
      "more when it is confident and solve the task themselves otherwise.\n",
      "Unfortunately, as our experiments show, the effect of using Adap-\n",
      "tive did not seem sufficient to increase the final team accuracy,\n",
      "possibly for two reasons: (1) in high confidence regions (circles in\n",
      "Figure 5), not only did workers have to agree more, but they also had\n",
      "to identify cases where the model failed with very high confidence\n",
      "(unknown unknowns [ 44]). Identifying unknown unknowns could\n",
      "have been a difficult and time-consuming task for workers, and theyDoes the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      ">= threshold<   thresholdPass adaptive threshold?Explain-Top-2Explain-Top-1Display\n",
      "0.6 0.7 0.8 0.9 1.0Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)condition\n",
      "0.6 0.7 0.8 0.9 1.0Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)condition\n",
      "0.5 0.6 0.7 0.8 0.9\n",
      "Human-AIAgreementTeam (ExplainTop-1,Expert)\n",
      "Team (ExplainTop-2,Expert)\n",
      "Team (Adapt ive,Expert)conditionBeer\n",
      "Amzbook\n",
      "LSAT\n",
      "--\n",
      "Figure 5: Relative agreement rates between humans and AI ( i.e., does the final human decision match the AI’s suggestion?) for\n",
      "various conditions, with examples split by whether AI’s confidence exceeded the threshold used for Adaptive explanations.\n",
      "Across the three datasets, Adaptive explanations successfully reduced the human’s tendency to blindly trust the AI ( i.e., de-\n",
      "creased agreement) when it was uncertain and more likely to be incorrect. For example, comparing Team (Explain-Top-1, AI)\n",
      "and Team (Adaptive, AI) on low confidence examples that did not pass the threshold (rectangles), participants in Explain-Top-2\n",
      "(pink rectangles) were less likely to agree with the AI compared to those who saw Explain-Top-1 (blue rectangles).\n",
      "0.000.20 0.40 0.60 0.801.00\n",
      "Humanaccuracy(Binned)0%20%40%%ExamplesBeer\n",
      "0.000.20 0.40 0.60 0.801.00\n",
      "Humanaccuracy(Binned)0%20%40%60%%ExamplesAmzbook\n",
      "0.000.20 0.40 0.60 0.801.00\n",
      "Humanaccuracy(Binned)0%5%10%15%20%%ExamplesLSATcorrect incorrect AIRecomme ndation\n",
      "Figure 6: The distribution of study examples as a function of average human accuracy. For each domain, examples on the right\n",
      "were easy for most humans working alone. Both Beer andLSAT show a distribution that shows potential for complementary\n",
      "team performance: humans can correct easy questions mistaken by the AI (red bars towards the right), and, conversely, the\n",
      "AI may add value on examples where humans frequently err (green bars towards the left). In contrast, Amzbook showed less\n",
      "potential for this kind of human-AI synergy, with less “easy for human” questions (bars towards the left).\n",
      "may have needed other types of support that we did not provide.\n",
      "(2) In low confidence regions (rectangles), not only did workers\n",
      "have to disagree more, but they also had to be able to solve the task\n",
      "correctly when they disagreed. Explain-Top-2 explanations might\n",
      "have enabled them to suspect the model more, but it is unclear if\n",
      "they helped participants make the right decisions. This indicates\n",
      "that more sophisticated strategies are needed to support humans in\n",
      "both situations. We discuss some potential strategies in Section 6.3.\n",
      "Differences in expertise between human and AI affects\n",
      "whether (or how much) AI assistance will help achieve com-\n",
      "plementary performance. To understand how differences in ex-\n",
      "pertise between the human and AI impact team performance, wecomputed the average accuracy of unassisted users on study exam-\n",
      "ples and overlaid the AI’s expertise (whether the recommendation\n",
      "was correct) in Figure 6. The figure helps explain why users ben-\n",
      "efited more from AI recommendations for both Beer and LSAT\n",
      "datasets. There was a significant fraction of examples that the AI\n",
      "predicted correctly but humans struggled with (green bars to the\n",
      "left), while the same was not true for Amzbook (where AI rec-\n",
      "ommendations did not help as much). Further, when the AI was\n",
      "incorrect, explaining predictions on Amzbook via Explain-Top-1\n",
      "improved the performance by 5% over showing confidence (Fig-\n",
      "ure 4B), but it decreased the performance for Beer andLSAT . OneCHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "Team (Conf)\n",
      "Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)ConditionBeer:AIAssistantisUseful Beer:Explanat ionisUseful\n",
      "-50% 0% 50%\n",
      "%Participants-50% 0% 50%\n",
      "%Participants\n",
      "Team (Conf)\n",
      "Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)ConditionAmzbook: AIAssistantisUseful Amzbook:Explanat ionisUseful\n",
      "-50% 0% 50%\n",
      "%Participants-50% 0% 50%\n",
      "%Participants\n",
      "Team (Conf)\n",
      "Team (ExplainTop-1,Exper t)\n",
      "Team (ExplainTop-2,Exper t)\n",
      "Team (Adapt ive,Expert)ConditionLSAT:AIAssistantisUseful LSAT:Explanat ionisUseful\n",
      "-50% 0% 50%\n",
      "%Participants-50% 0% 50%\n",
      "%ParticipantsStrongl ydisagr ee Disa gree Neitheragreenordisagree Agree Strongl yagree Response\n",
      "B A\n",
      "Figure 7: Analysis of participant responses to two statements: (A) “AI’s assistance ( e.g., the information it displayed) helped me\n",
      "solve the task”, and (B) “AI’s explanations in particular helped me solve the task.” Across datasets, a majority of participants\n",
      "found AI assistant to be useful, and they rated all the conditions similarly, with a slight preference towards Team (Adaptive,\n",
      "Expert). In contrast to AI’s overall usefulness, fewer participants rated explanations as useful, particularly Explain-Top-2\n",
      "explanations. Participants also had a clearer preference for higher-quality (expert) Adaptive explanations.\n",
      "possible explanation is that most AI mistakes were predicted cor-\n",
      "rectly by most humans on Amzbook (red bars were mostly towards\n",
      "the right). After observing clear model mistakes, participants may\n",
      "have learned to rely on them less, despite the convincing-effect of\n",
      "explanation. Participants’ self-reported collaboration approaches\n",
      "supported our guess – Amzbook participants reportedly ignored the\n",
      "AI’s assistance the most (Section 5.3). That said, other confounding\n",
      "effects such as the nature of the task ( e.g., binary classification vs.\n",
      "choosing between multiple options) should also be studied.\n",
      "5.2 Survey Responses on Likert Scale Questions\n",
      "Two of the questions in our post-task survey requested categorical\n",
      "ratings of AI and explanation usefulness.9\n",
      "AI usefulness : While participants generally rated AI assistance\n",
      "useful (Figure 7A), the improvements in ratings between most ex-\n",
      "planations and simply showing confidence were marginal. The\n",
      "difference was more clear for high-quality adaptive explanations;\n",
      "forBeer, 70% of the participants rated AI assistance useful with\n",
      "Team (Adaptive, Expert) in contrast to 57% with Team (Conf). We\n",
      "observed a similar pattern on Amzbook (65% vs. 49%) and LSAT (63%\n",
      "vs. 45%), though on LSAT , Team (Explain-Top-2, Expert) received\n",
      "slightly higher ratings than Team (Adaptive, Expert) (66% vs. 63%).\n",
      "9Since we did not pre-register hypotheses for the subjective ratings and only analyzed\n",
      "them post-hoc, we do not perform/claim statistical significant analysis on these metrics.Explanation usefulness : Figure 7B shows that participants’\n",
      "ratings for the usefulness of explanations were lower than the\n",
      "overall usefulness of AI’s assistance (in A). Again, expert-generated\n",
      "Adaptive explanations received higher ratings than AI-generated\n",
      "ones for Beer (53% vs. 38%) vs. Amzbook (50% vs. 40%). This could\n",
      "indicate that showing higher quality explanations improves users’\n",
      "perceived helpfulness of the system. However, it is worth noting\n",
      "that this increased preference did not translate to an improvement in\n",
      "team performance, which is consistent with observations made by\n",
      "Buçinca et al. [12] that show that people may prefer one explanation\n",
      "but make better decisions with another.\n",
      "5.3 Qualitative Analysis on Collaboration\n",
      "To better understand how users collaborated with the AI in different\n",
      "tasks, we coded their response to the prompt: “Describe how you\n",
      "used the information Marvin (the AI) provided.” Two annotators\n",
      "independently read a subset of the responses to identify emergent\n",
      "codes and, using a discussion period, created a codebook (Table 3).\n",
      "Using this codebook, for each team condition and dataset, they\n",
      "coded a sample of 30 random worker responses: 28 were unique\n",
      "and 2 overlapped between annotators, allowing us to compute inter-\n",
      "annotator agreement. Our final analysis used 409 unique responses\n",
      "after removing 11 responses deemed to be of poor quality (Table 3).\n",
      "We scored the inter-annotator agreement with both the Cohen’s 𝜅Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "Codes Definitions and Examples #Participants\n",
      "Overall Collaboration Approach (codes are mutually exclusive)\n",
      "Mostly Follow AI The participant mostly followed the AI.\n",
      "“I went with Marvin most times. ”23 (6%)\n",
      "AI as Prior Guide Used AI as a starting reference point.\n",
      "“I looked at his prediction and then I read the passage. ”190 (47%)\n",
      "AI as Post Check Double-checked after they made their own decisions.\n",
      "“I ignored it until I made my decision and then verified what it said. ”102 (25%)\n",
      "Mostly Ignore AI Mostly made their own decisions without the AI.\n",
      "“I didn’t. I figured out the paragraph for myself. ”90 (22%)\n",
      "The Usage of Explanation (codes can overlap)\n",
      "Used Expl. Explicitly acknowledged they used the explanation.\n",
      "“I skimmed his highlighted words. ”138 (42%)\n",
      "Speed Read Used explanations to quickly skim through the example.\n",
      "“I looked at Marvin’s review initially then speed read the review. ”29 (9%)\n",
      "Validate AI Used the explanation to validate AI’s reasoning.\n",
      "“Marvin focuses on the wrong points at times. This made me\n",
      "cautious when taking Marvin’s advice. ”17 (5%)\n",
      "The Usage of Confidence (codes can overlap)\n",
      "Used Conf. Explicitly acknowledged they used the confidence.\n",
      "“I mostly relied on Marvin’s confident levels to guide me. ”90 (22%)\n",
      "Conf. Threshold Was more likely to accept AI above the threshold.\n",
      "“If Marvin was above 85% confidence, I took his word for it. ”24 (6%)\n",
      "Others (codes can overlap)\n",
      "Fall Back to AI Followed the AI’s label if they failed to decide.\n",
      "“I used it if I was unsure of my own decision. ”54 (13%)\n",
      "Updated Strategy Changed their strategy as they interacted more.\n",
      "“I decided myself after seeing that sometimes Marvin failed me. ”12 (2%)\n",
      "Table 3: The codebook for participants’ descriptions of how they used the AI, with the number of self-reports.\n",
      "and the raw overlap between the coding. We achieved reasonably\n",
      "high agreements, with an average 𝜇(𝜅)=0.71, 𝜎(𝜅)=0.18(the\n",
      "average agreement was 93%±6.5%). We noticed the following,\n",
      "which echo the performance differences observed across datasets:\n",
      "Most participants used the AI’s recommendation as a prior\n",
      "or to double-check their answers. For all datasets, more than\n",
      "70% of the participants mentioned they would partially take AI’s\n",
      "recommendation into consideration rather than blindly following\n",
      "AI or fully ignoring it (Figure 8). Participants used the AI as a prior\n",
      "guide more than as a post-check for sentiment analysis, but not\n",
      "forLSAT , which aligns with our interface design: for LSAT , AI\n",
      "recommendations were on a separate pane, encouraging users to\n",
      "solve the task on their own before consulting the AI.\n",
      "Participants ignored the AI more on domains where AI\n",
      "expertise did not supplement their expertise. Figure 8 shows\n",
      "that while only 11% of LSAT participants claimed that they mostly\n",
      "ignored the AI, the ratio doubled ( Beer, 23%) or even tripled ( Amz-\n",
      "book , 30%) for sentiment analysis. As discussed in Figure 6, this\n",
      "may be due to correlation differences between human and AI errors\n",
      "for different datasets: Amzbook participants were less likely to see\n",
      "cases where AI was more correct than they were, and therefore they\n",
      "may have learned to rely less on it. For example, one participant in\n",
      "Amzbook mentioned, “I had initially tried to take Marvin’s adviceinto account for a few rounds, and stopped after I got 2 incorrect\n",
      "answers. After that I read all of the reviews carefully and followed\n",
      "my own discretion. ”\n",
      "In contrast, a Beer participant relied more on the AI once realizing\n",
      "it could be correct: “At first I tried reading the passages and making\n",
      "my own judgments, but then I got several items wrong. After that, I\n",
      "just switched to going with Marvin’s recommendation every time. ”\n",
      "In addition to the user’s collaboration behavior, these differ-\n",
      "ences between domains may have affected our quantitative ob-\n",
      "servations of team performance. For example, a small difference\n",
      "between human and AI expertise (distribution of errors) means\n",
      "that the improvement in performance when the AI is correct would\n",
      "be less substantial. In fact, in Figure 4B, if we compare the team\n",
      "performance when the AI is correct, the difference between team\n",
      "conditions and the human baseline is least substantial for Amzbook .\n",
      "Some participants developed mental models of the AI’s\n",
      "confidence score to determine when to trust the AI. Among\n",
      "participants who mentioned they used confidence scores (90 in\n",
      "total), 27% reported using an explicit confidence threshold, below\n",
      "which they were likely to distrust the AI. The threshold mostly\n",
      "varied between 80 to 100 ( 83±8forBeer,89±7forAmzbook , and\n",
      "90±0forLSAT ) but could go as low as 65, indicating that users\n",
      "built different mental models about when they considered AI toCHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "MostlyFollowAI\n",
      "AIasPriorGuide\n",
      "AIasPostCheck\n",
      "MostlyIgnoreAICollaborationBeer Amzbook LSAT\n",
      "0%10% 20% 30% 40% 50%\n",
      "%Participants0%10% 20% 30% 40% 50%\n",
      "%Participants0%10% 20% 30% 40% 50%\n",
      "%Participants\n",
      "Figure 8: Instead of ignoring or strictly following the AI, participants reported taking the AI information into consideration\n",
      "most of the time. They most frequently used AI as a prior guide in sentiment analysis, but used it as post-check in LSAT . They\n",
      "were also more likely to ignore the AI in sentiment analysis than in LSAT .\n",
      "Conf.\n",
      "Conf.+ Expl.conditionAmzbook Beer LSAT\n",
      "0% 5% 10%\n",
      "%Participants0% 5% 10% 15%\n",
      "%Participants0% 2% 4% 6%\n",
      "%ParticipantsConf.\n",
      "Conf.+ Expl.condition\n",
      "Figure 9: Comparing the occurrence of Used Conf. in just the confidence condition and in those with explanations, we saw a\n",
      "similar proportion of users that explicitly acknowledged using confidence, regardless of whether they saw an explanation.\n",
      "be “trustworthy.” While this observation empirically shows that\n",
      "end-users develop mental model of trust in AI-assisted decision\n",
      "making [ 5], it more importantly shows how the AI’s confidence is\n",
      "a simple, yet salient feature via which users create a mental model\n",
      "of the AI’s global behavior [ 23]. Note that across all three domains,\n",
      "the same proportion of participants self-reported using AI’s confi-\n",
      "dence scores regardless of whether they saw explanations (Figure 9).\n",
      "Furthermore, some participants consigned the task to AI when\n",
      "they were themselves uncertain. For example, 13% participants\n",
      "mentioned that they would go with the AI’s decision if they were on\n",
      "the fence by themselves: “There were some that I could go either way\n",
      "on, and I went with what Marvin suggested. ” These user behaviors are\n",
      "similar to observations in psychology literature on Truth-Default\n",
      "Theory [47], which shows that people exhibit truth-default behavior:\n",
      "by default, people are biased to assume that the speaker is being\n",
      "truthful, especially when triggers that raise suspicion are absent.\n",
      "Furthermore, our participants’ distrust in low-confidence recom-\n",
      "mendations is also consistent with examples of triggers that cause\n",
      "people to abandon the truth-default behavior.\n",
      "Explanations can help participants validate the AI’s deci-\n",
      "sions, and the inline-highlight format helped participants\n",
      "speed up their decision making. Among the participants who\n",
      "explicitly mentioned using explanations, 27% in Beer and 32% in\n",
      "Amzbook reported that they used them to read the review text faster.\n",
      "Since LSAT explanations required reading additional text, we did\n",
      "not expect LSAT users to find this benefit. Interestingly, for Beer\n",
      "and Amzbook , while a small percentage of users (17%) reported\n",
      "using the explanations to validate the AI’s decisions (see Figure 3),\n",
      "only 2% did so in LSAT . This could be because LSAT is a harder task\n",
      "than sentiment analysis, and verifying AI’s explanations is costlier.\n",
      "Other participants mostly mentioned that they would supplement\n",
      "their own reasoning with the AI’s: “I read the Marvin rationale and\n",
      "weighed it against my intuition and understanding. ”6 DISCUSSION & FUTURE DIRECTIONS\n",
      "Though conducted in a limited scope, our findings should help guide\n",
      "future work on explanations and other mechanisms for improving\n",
      "decision making with human-AI teams.\n",
      "6.1 Limitations\n",
      "As mentioned in Section 2, AI explanations have other motivations\n",
      "not addressed by this paper. Our work, as well as the papers listed\n",
      "in Table 1, evaluated team performance along one dimension: accu-\n",
      "racy of decisions. We did not explore the benefits on other metrics\n",
      "(e.g.increasing speed as reported by some users in Section 5.2), but\n",
      "in general, one may wish to achieve complementary performance\n",
      "on a multi-dimensional metric. In fact, research shows that large\n",
      "collaborative communities like Wikipedia require AI systems that\n",
      "balance multiple aspects, e.g., reducing human effort, improving\n",
      "trust and positive engagement [ 68]. We encourage future research\n",
      "to extend the definition of complementarity, and to evaluate the\n",
      "impact of explanations on those dimensions accordingly.\n",
      "Further, we restricted ourselves to tasks amenable to crowdsourc-\n",
      "ing (text classification and question answering), so our results may\n",
      "not generalize to high-stakes domains with expert users such as\n",
      "medical diagnosis. We also note that the effectiveness of expla-\n",
      "nations may depend on user expertise, a factor that we did not\n",
      "explore. Investigating this in our framework would either require\n",
      "recruiting lay and expert users for the same task [ 21] or utilizing a\n",
      "within-subject experimental design to measure user expertise.\n",
      "Finally, we only explored two possible ways to present expla-\n",
      "nations (highlighting keywords and natural language arguments).\n",
      "While these methods are widely adopted [ 24,43,49,75], alternative\n",
      "approaches may provide more benefit to team performance.\n",
      "6.2 Explaining AI for Appropriate Reliance\n",
      "One concerning observation was that explanations increased blind\n",
      "trust rather than appropriate reliance on AI. This is problematic\n",
      "especially in domains where humans are required in the loop forDoes the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "moral or legal reasons ( e.g., medical diagnosis) and suppose the\n",
      "presence of explanations simply soothes the experts ( e.g., doctors),\n",
      "making them more compliant so they blindly (or become more\n",
      "likely to) agree with the computer. Encouraging human-AI interac-\n",
      "tions like these seems deeply unsatisfactory and ethically fraught.\n",
      "Importantly, while prior works also observed instances of inap-\n",
      "propriate reliance on AI [ 18,38,58,74], our studies quantified its\n",
      "effect on team performance. Since the nature of the proxy tasks can\n",
      "significantly change the human behavior, they can lead to potential\n",
      "misleading conclusions [ 12]. The emphasis of the complementary\n",
      "team performance in end-to-end tasks can objectively evaluate the\n",
      "extent of such issues or about the effectiveness of a solution.\n",
      "Our Adaptive Explanation aims to encourage the human to think\n",
      "more carefully when the system had a low confidence. While the\n",
      "relative agreement rates showed that the Explain-Top-2 explanation\n",
      "might cue the humans to suspect the model’s veracity (Figure 5),\n",
      "the method was not sufficient to significantly increase the final\n",
      "team accuracy (Figure 4). This is perhaps because users still have\n",
      "to identify high-confidence mistakes (unknown-unknowns) and\n",
      "solve the task when the AI is uncertain (Section 5.1). A followup\n",
      "question is, then, what kind of interactions would help humans\n",
      "perform correctly when the AI is incorrect?\n",
      "Explanations should be informative, instead of just convincing.\n",
      "Our current expert explanations did not help any more than the AI\n",
      "explanations, which may indicate that having the ML produce the\n",
      "maximally convincing explanation — a common objective shared in\n",
      "the design of many AI explanation algorithms — might be a poor\n",
      "choice for complementary performance [ 12]. A more ideal goal is\n",
      "explanations that accurately inform the user – such that the user\n",
      "can quickly gauge through the explanation when the AI’s reasoning\n",
      "is correct and when it should raise suspicion. A successful example\n",
      "of this was seen with Generalized Additive Models (GAMs) for\n",
      "healthcare, where its global explanations helped medical experts\n",
      "suspect that the model had learned incorrect, spurious correlations\n",
      "(e.g.a history of asthma reduces the risk of dying from pneumo-\n",
      "nia [ 15]). We hope future research can produce explanations that\n",
      "better enable the human to effectively catch AI’s mistakes, rather\n",
      "than finding plausible justifications when it erred.\n",
      "High complementary performance may require adapting beyond\n",
      "confidence. Since approaches based on confidence scores make it\n",
      "difficult to spot unknown-unknowns, instead it may be worthwhile\n",
      "to design explanation strategies that adapt based on the frequency\n",
      "of agreement between the human and AI. For example, instead of\n",
      "explaining why it believes an answer to be true, the AI might play\n",
      "a devil’s advocate role, explaining its doubts — even when it agrees\n",
      "with the human. The doubts can even be expressed in an interac-\n",
      "tive fashion (as a back and forth conversation) than a set of static\n",
      "justifications, so to avoid cognitive overload. For example, even if\n",
      "the system agrees with the user, the system can present a high-level\n",
      "summary of evidence for top-K alternatives and let the user drill\n",
      "down, i.e., ask the system for more detailed evidence for the subset\n",
      "of alternatives that they now think are worth investigating.6.3 Rethinking AI’s Role in Human-AI Teams\n",
      "Comparable accuracy does not guarantee complementary part-\n",
      "ners. Rather, in an ideal team, the human and AI would have mini-\n",
      "mally overlapping mistakes so that there is a greater opportunity\n",
      "to correct each other’s mistakes. In one of our experiment domains\n",
      "(Amzbook ), AI errors correlated much more strongly with humans’\n",
      "than in others, and thus we saw relatively smaller gains in per-\n",
      "formance from AI assistance (Figure 6). As recent work has sug-\n",
      "gested [ 4,55,59,79], it may be useful to directly optimize for com-\n",
      "plementary behavior by accounting for the human behavior during\n",
      "training, who may have access to a different set of features [72].\n",
      "Furthermore, the human and AI could maximize their talents in\n",
      "different dimensions. For example, for grading exams, AI could use\n",
      "its computation power to quickly gather statistics and highlight\n",
      "commonly missed corner cases, whereas the human teacher could\n",
      "focus on ranking the intelligence of the student proposed algo-\n",
      "rithms [ 26]. Similarly, to maximize human performance at Quiz\n",
      "Bowl, Feng and Graber [ 21] designed interaction so that the AI\n",
      "memorized and quickly retrieved documents relevant to a ques-\n",
      "tion, a talent which humans lacked because of cognitive limitations;\n",
      "however, they left the task of combining found evidence and logical\n",
      "reasoning to human partners. Future research should explore other\n",
      "ways to increase synergy.\n",
      "The timing of AI recommendations is important. Besides the types\n",
      "of explanations, it is also important to carefully design when the\n",
      "AI provides its viewpoint. All of our methods used a workflow\n",
      "that showed the AI’s prediction (and its explanation) to the human,\n",
      "before they attempted to solve the problem on their own. However,\n",
      "by presenting an answer and accompanying justification upfront,\n",
      "and perhaps overlaid right onto the instance, our design makes it\n",
      "almost impossible for the human to reason independently, ignoring\n",
      "the AI’s opinion while considering the task. This approach risks\n",
      "invoking the anchor effect, studied in Psychology [ 20] and intro-\n",
      "duced to the AI explanation field by Wang et al. [ 74] — people rely\n",
      "heavily on the first information that is presented by others when\n",
      "making decisions. This effect was reflected in an increase in the\n",
      "use of the “ AI as Prior Guide ” collaboration approach in the\n",
      "sentiment analysis domain, compared to LSAT (Figure 8).\n",
      "Alternate approaches that present AI recommendations in an\n",
      "asynchronous fashion might increase independence and improve\n",
      "accuracy. For example, pairing humans with slower AIs (that wait\n",
      "or take more time to make recommendation) may provide humans\n",
      "with a better chance to reflect on their own decisions [ 63]. Methods\n",
      "that embody recommendations from management science for avoid-\n",
      "ing group-think [ 54] might also be effective, e.g., showing the AI’s\n",
      "prediction after the human’s initial answer or only having the AI\n",
      "present an explanation if it disagreed with the human’s choice. We\n",
      "note that these approaches correspond to the Update and Feedback\n",
      "methods of Green & Chen [ 27], which were effective, albeit not in\n",
      "the complementary zone. Another approach is to limit the AI’s capa-\n",
      "bilities. For example, one might design the AI to summarize the best\n",
      "evidence for all possible options, without giving hard predictions,\n",
      "by training evidence agents [64]. However, by delaying display of\n",
      "the AI’s recommendation until after the human has solved the task\n",
      "independently or restricting to only per class evidences, one mayCHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "preclude improvement to the speed of problem solving, which often\n",
      "correlates to the cost of performing the task.\n",
      "As a result, there is a strong tension between the competing\n",
      "objectives of speed, accuracy, and independence; We encourage\n",
      "the field to design and conduct experiments and explore different\n",
      "architectures for balancing these factors.\n",
      "7 CONCLUSIONS\n",
      "Previous work has shown that the accuracy of a human-AI team\n",
      "can be improved when the AI explains its suggestions, but these\n",
      "results are only obtained in situations where the AI, operating in-\n",
      "dependently, is better than either the human or the best human-AI\n",
      "team. We ask if AI explanations help achieve complementary team\n",
      "performance, i.e.whether the team is more accurate than either the\n",
      "AI or human acting independently. We conducted large-scale exper-\n",
      "iments with more than 1,500 participants. Importantly, we selected\n",
      "our study questions to ensure that our AI systems had accuracy\n",
      "comparable to humans and increased the opportunity for seeing\n",
      "complementary performance. While all human-AI teams showed\n",
      "complementarity, none of the explanation conditions produced an\n",
      "accuracy significantly higher than the simple baseline of showing\n",
      "the AI’s confidence — in contrast to prior work. Explanations in-\n",
      "creased team performance when the system was correct, but they\n",
      "decreased the accuracy on examples when the system was wrong,\n",
      "making the net improvement minimal.\n",
      "By highlighting critical challenges, we hope this paper will serve\n",
      "as a “Call to action” for the HCI and AI communities: and AI commu-\n",
      "nities. In future work, characterize when human-AI collaboration\n",
      "can be beneficial ( i.e., when both parties complement each other),\n",
      "developing explanation approaches and coordination strategies that\n",
      "result in a complementary team performance that exceeds what can\n",
      "be produced by simply showing AI’s confidence, and communicate\n",
      "explanations to increase understanding rather than just to persuade.\n",
      "At the highest level, we hope researchers can develop new interac-\n",
      "tion methods that increase complementary performance beyond\n",
      "having an AI telegraph its confidence.\n",
      "ACKNOWLEDGMENTS\n",
      "This material is based upon work supported by ONR grant N00014-\n",
      "18-1-2193, NSF RAPID grant 2040196, the University of Washington\n",
      "WRF/Cable Professorship, and the Allen Institute for Artificial In-\n",
      "telligence (AI2), and Microsoft Research. The authors thank Umang\n",
      "Bhatt, Jim Chen, Elena Glassman, Walter Lasecki, Qisheng Li, Eu-\n",
      "nice Jun, Sandy Kaplan, Younghoon Kim, Galen Weld, Amy Zhang,\n",
      "and anonymous reviewers for helpful discussions and comments.\n",
      "REFERENCES\n",
      "[1]Gregory D. Abowd, Anind K. Dey, Peter J. Brown, Nigel Davies, Mark Smith, and\n",
      "Pete Steggles. 1999. Towards a Better Understanding of Context and Context-\n",
      "Awareness. In Proceedings of the 1st International Symposium on Handheld and\n",
      "Ubiquitous Computing (Karlsruhe, Germany) (HUC ’99) . Springer-Verlag, Berlin,\n",
      "Heidelberg, 304–307.\n",
      "[2]Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine bias:\n",
      "There’s software across the country to predict future criminals and it’s biased\n",
      "against blacks.\n",
      "[3]Oisin Mac Aodha, Shihan Su, Yuxin Chen, Pietro Perona, and Yisong Yue. 2018.\n",
      "Teaching Categories to Human Learners With Visual Explanations. In 2018 IEEE\n",
      "Conference on Computer Vision and Pattern Recognition, CVPR 2018 . IEEE Com-\n",
      "puter Society, Salt Lake City, UT, USA, 3820–3828. https://doi.org/10.1109/CVPR.\n",
      "2018.00402[4]Gagan Bansal, Besmira Nushi, Ece Kamar, Eric Horvitz, and Daniel S. Weld. 2020.\n",
      "Optimizing AI for Teamwork. arXiv:2004.13102 [cs.AI]\n",
      "[5]Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S. Lasecki, Daniel S. Weld, and\n",
      "Eric Horvitz. 2019. Beyond Accuracy: The Role of Mental Models in Human-AI\n",
      "Team Performance. Proceedings of the AAAI Conference on Human Computation\n",
      "and Crowdsourcing 7, 1 (Oct. 2019), 2–11. https://ojs.aaai.org/index.php/HCOMP/\n",
      "article/view/5285\n",
      "[6]Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel S. Weld, Walter S. Lasecki, and\n",
      "Eric Horvitz. 2019. Updates in Human-AI Teams: Understanding and Addressing\n",
      "the Performance/Compatibility Tradeoff. Proceedings of the AAAI Conference on\n",
      "Artificial Intelligence 33, 01 (Jul. 2019), 2429–2437. https://doi.org/10.1609/aaai.\n",
      "v33i01.33012429\n",
      "[7]Richard E Barlow and Hugh D Brunk. 1972. The isotonic regression problem and\n",
      "its dual. J. Amer. Statist. Assoc. 67, 337 (1972), 140–147.\n",
      "[8]Mohsen Bayati, Mark Braverman, Michael Gillam, Karen M. Mack, George Ruiz,\n",
      "Mark S. Smith, and Eric Horvitz. 2014. Data-Driven Decisions for Reducing\n",
      "Readmissions for Heart Failure: General Methodology and Case Study. PLOS\n",
      "ONE 9, 10 (10 2014), 1–9. https://doi.org/10.1371/journal.pone.0109264\n",
      "[9]Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yun-\n",
      "han Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura, and Peter Eckersley.\n",
      "2020. Explainable Machine Learning in Deployment. In Proceedings of the\n",
      "2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain)\n",
      "(FAT* ’20) . Association for Computing Machinery, New York, NY, USA, 648–657.\n",
      "https://doi.org/10.1145/3351095.3375624\n",
      "[10] Mustafa Bilgic. 2005. Explaining Recommendations: Satisfaction vs. Promotion. ,\n",
      "13–18 pages.\n",
      "[11] Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and\n",
      "Nigel Shadbolt. 2018. ’It’s Reducing a Human Being to a Percentage’: Per-\n",
      "ceptions of Justice in Algorithmic Decisions. In Proceedings of the 2018 CHI\n",
      "Conference on Human Factors in Computing Systems (Montreal QC, Canada)\n",
      "(CHI ’18) . Association for Computing Machinery, New York, NY, USA, 1–14.\n",
      "https://doi.org/10.1145/3173574.3173951\n",
      "[12] Zana Buçinca, Phoebe Lin, Krzysztof Z. Gajos, and Elena L. Glassman. 2020. Proxy\n",
      "Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI\n",
      "Systems. In Proceedings of the 25th International Conference on Intelligent User\n",
      "Interfaces (Cagliari, Italy) (IUI ’20) . Association for Computing Machinery, New\n",
      "York, NY, USA, 454–464. https://doi.org/10.1145/3377325.3377498\n",
      "[13] Adrian Bussone, Simone Stumpf, and Dympna O’Sullivan. 2015. The role of\n",
      "explanations on trust and reliance in clinical decision support systems. In 2015\n",
      "International Conference on Healthcare Informatics . IEEE, IEEE, Dallas, Texas,\n",
      "160–169.\n",
      "[14] Carrie J Cai, Samantha Winter, David Steiner, Lauren Wilcox, and Michael Terry.\n",
      "2019. \" Hello AI\": Uncovering the Onboarding Needs of Medical Practitioners for\n",
      "Human-AI Collaborative Decision-Making. Proceedings of the ACM on Human-\n",
      "Computer Interaction 3, CSCW (2019), 1–24.\n",
      "[15] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie\n",
      "Elhadad. 2015. Intelligible Models for HealthCare: Predicting Pneumonia Risk\n",
      "and Hospital 30-Day Readmission. In Proceedings of the 21th ACM SIGKDD In-\n",
      "ternational Conference on Knowledge Discovery and Data Mining (Sydney, NSW,\n",
      "Australia) (KDD ’15) . Association for Computing Machinery, New York, NY, USA,\n",
      "1721–1730. https://doi.org/10.1145/2783258.2788613\n",
      "[16] Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay,\n",
      "and Devi Parikh. 2018. Do explanations make VQA models more predictable\n",
      "to a human?. In Proceedings of the 2018 Conference on Empirical Methods in\n",
      "Natural Language Processing . Association for Computational Linguistics, Brussels,\n",
      "Belgium, 1036–1042. https://doi.org/10.18653/v1/D18-1128\n",
      "[17] Elizabeth Clark, Anne Spencer Ross, Chenhao Tan, Yangfeng Ji, and Noah A.\n",
      "Smith. 2018. Creative Writing with a Machine in the Loop: Case Studies on\n",
      "Slogans and Stories. In 23rd International Conference on Intelligent User Interfaces\n",
      "(Tokyo, Japan) (IUI ’18) . Association for Computing Machinery, New York, NY,\n",
      "USA, 329–340. https://doi.org/10.1145/3172944.3172983\n",
      "[18] Pat Croskerry. 2009. Clinical cognition and diagnostic error: applications of a\n",
      "dual process model of reasoning. Advances in health sciences education 14, 1\n",
      "(2009), 27–35.\n",
      "[19] Xiao Dong and Caroline C Hayes. 2012. Uncertainty visualizations: Helping\n",
      "decision makers become more aware of uncertainty and its implications. Journal\n",
      "of Cognitive Engineering and Decision Making 6, 1 (2012), 30–56.\n",
      "[20] Birte Englich, Thomas Mussweiler, and Fritz Strack. 2006. Playing dice with crim-\n",
      "inal sentences: The influence of irrelevant anchors on experts’ judicial decision\n",
      "making. Personality and Social Psychology Bulletin 32, 2 (2006), 188–200.\n",
      "[21] Shi Feng and Jordan Boyd-Graber. 2019. What Can AI Do for Me? Evaluating\n",
      "Machine Learning Interpretations in Cooperative Play. In Proceedings of the 24th\n",
      "International Conference on Intelligent User Interfaces (Marina del Ray, California)\n",
      "(IUI ’19) . Association for Computing Machinery, New York, NY, USA, 229–239.\n",
      "https://doi.org/10.1145/3301275.3302265\n",
      "[22] Michael Fernandes, Logan Walls, Sean Munson, Jessica Hullman, and Matthew\n",
      "Kay. 2018. Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit\n",
      "Decision-Making. In Proceedings of the 2018 CHI Conference on Human Factors inDoes the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "Computing Systems (Montreal QC, Canada) (CHI ’18) . Association for Computing\n",
      "Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/3173574.3173718\n",
      "[23] Katy Ilonka Gero, Zahra Ashktorab, Casey Dugan, Qian Pan, James Johnson,\n",
      "Werner Geyer, Maria Ruiz, Sarah Miller, David R. Millen, Murray Campbell,\n",
      "Sadhana Kumaravel, and Wei Zhang. 2020. Mental Models of AI Agents in a\n",
      "Cooperative Game Setting. In Proceedings of the 2020 CHI Conference on Human\n",
      "Factors in Computing Systems (Honolulu, HI, USA) (CHI ’20) . Association for\n",
      "Computing Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/\n",
      "3313831.3376316\n",
      "[24] Reza Ghaeini, Xiaoli Fern, and Prasad Tadepalli. 2018. Interpreting Recurrent and\n",
      "Attention-Based Neural Models: a Case Study on Natural Language Inference.\n",
      "InProceedings of the 2018 Conference on Empirical Methods in Natural Language\n",
      "Processing . Association for Computational Linguistics, Brussels, Belgium, 4952–\n",
      "4957. https://doi.org/10.18653/v1/D18-1537\n",
      "[25] Dimitra Gkatzia, Oliver Lemon, and Verena Rieser. 2016. Natural Language\n",
      "Generation enhances human decision-making with uncertain information. In\n",
      "Proceedings of the 54th Annual Meeting of the Association for Computational\n",
      "Linguistics (Volume 2: Short Papers) . Association for Computational Linguistics,\n",
      "Berlin, Germany, 264–268. https://doi.org/10.18653/v1/P16-2043\n",
      "[26] Elena L Glassman, Jeremy Scott, Rishabh Singh, Philip J Guo, and Robert C Miller.\n",
      "2015. OverCode: Visualizing variation in student solutions to programming\n",
      "problems at scale. ACM Transactions on Computer-Human Interaction (TOCHI)\n",
      "22, 2 (2015), 1–35.\n",
      "[27] Ben Green and Yiling Chen. 2019. The principles and limits of algorithm-in-the-\n",
      "loop decision making. Proceedings of the ACM on Human-Computer Interaction 3,\n",
      "CSCW (2019), 1–24.\n",
      "[28] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On Calibration\n",
      "of Modern Neural Networks. In Proceedings of the 34th International Conference\n",
      "on Machine Learning - Volume 70 (ICML’17) . JMLR.org, Sydney, NSW, Australia,\n",
      "1321–1330.\n",
      "[29] Peter Hase and Mohit Bansal. 2020. Evaluating Explainable AI: Which Algorith-\n",
      "mic Explanations Help Users Predict Model Behavior?. In Proceedings of the 58th\n",
      "Annual Meeting of the Association for Computational Linguistics . Association for\n",
      "Computational Linguistics, Online, 5540–5552. https://doi.org/10.18653/v1/2020.\n",
      "acl-main.491\n",
      "[30] Yugo Hayashi and Kosuke Wakabayashi. 2017. Can AI Become Reliable Source\n",
      "to Support Human Decision Making in a Court Scene?. In Companion of the 2017\n",
      "ACM Conference on Computer Supported Cooperative Work and Social Computing\n",
      "(Portland, Oregon, USA) (CSCW ’17 Companion) . Association for Computing Ma-\n",
      "chinery, New York, NY, USA, 195–198. https://doi.org/10.1145/3022198.3026338\n",
      "[31] Ruining He and Julian McAuley. 2016. Ups and Downs: Modeling the Vi-\n",
      "sual Evolution of Fashion Trends with One-Class Collaborative Filtering. In\n",
      "Proceedings of the 25th International Conference on World Wide Web (Mon-\n",
      "tréal, Québec, Canada) (WWW ’16) . International World Wide Web Confer-\n",
      "ences Steering Committee, Republic and Canton of Geneva, CHE, 507–517.\n",
      "https://doi.org/10.1145/2872427.2883037\n",
      "[32] Dan Hendrycks and Kevin Gimpel. 2017. A Baseline for Detecting Misclassified\n",
      "and Out-of-Distribution Examples in Neural Networks. In 5th International Con-\n",
      "ference on Learning Representations, ICLR 2017 . OpenReview.net, Toulon, France,\n",
      "1–12. https://openreview.net/forum?id=Hkg4TI9xl\n",
      "[33] Chien-Ju Ho, Aleksandrs Slivkins, Siddharth Suri, and Jennifer Wortman Vaughan.\n",
      "2015. Incentivizing High Quality Crowdwork. In Proceedings of the 24th Interna-\n",
      "tional Conference on World Wide Web (Florence, Italy) (WWW ’15) . International\n",
      "World Wide Web Conferences Steering Committee, Republic and Canton of\n",
      "Geneva, CHE, 419–429. https://doi.org/10.1145/2736277.2741102\n",
      "[34] Eric Horvitz. 1999. Principles of Mixed-Initiative User Interfaces. In Proceedings\n",
      "of the SIGCHI Conference on Human Factors in Computing Systems (Pittsburgh,\n",
      "Pennsylvania, USA) (CHI ’99) . Association for Computing Machinery, New York,\n",
      "NY, USA, 159–166. https://doi.org/10.1145/302979.303030\n",
      "[35] Eric Horvitz and Tim Paek. 2007. Complementary computing: policies for trans-\n",
      "ferring callers from dialog systems to human receptionists. User Modeling and\n",
      "User-Adapted Interaction 17, 1-2 (2007), 159–182.\n",
      "[36] Shagun Jhaver, Amy Bruckman, and Eric Gilbert. 2019. Does transparency in\n",
      "moderation really matter? User behavior after content removal explanations on\n",
      "reddit. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019),\n",
      "1–27.\n",
      "[37] Susan Joslyn and Jared LeClerc. 2013. Decisions with uncertainty: The glass half\n",
      "full. Current Directions in Psychological Science 22, 4 (2013), 308–315.\n",
      "[38] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach,\n",
      "and Jennifer Wortman Vaughan. 2020. Interpreting Interpretability: Under-\n",
      "standing Data Scientists’ Use of Interpretability Tools for Machine Learning. In\n",
      "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems\n",
      "(Honolulu, HI, USA) (CHI ’20) . Association for Computing Machinery, New York,\n",
      "NY, USA, 1–14. https://doi.org/10.1145/3313831.3376219\n",
      "[39] Derek J Koehler. 1991. Explanation, imagination, and confidence in judgment.\n",
      "Psychological bulletin 110, 3 (1991), 499.\n",
      "[40] Pang Wei Koh and Percy Liang. 2017. Understanding Black-Box Predictions\n",
      "via Influence Functions. In Proceedings of the 34th International Conference onMachine Learning - Volume 70 (ICML’17) . JMLR.org, Sydney, NSW, Australia,\n",
      "1885–1894.\n",
      "[41] Johannes Kunkel, Tim Donkers, Lisa Michael, Catalin-Mihai Barbu, and Jür-\n",
      "gen Ziegler. 2019. Let Me Explain: Impact of Personal and Impersonal Ex-\n",
      "planations on Trust in Recommender Systems. In Proceedings of the 2019 CHI\n",
      "Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk)\n",
      "(CHI ’19) . Association for Computing Machinery, New York, NY, USA, 1–12.\n",
      "https://doi.org/10.1145/3290605.3300717\n",
      "[42] Vivian Lai, Han Liu, and Chenhao Tan. 2020. \"Why is ’Chicago’ Deceptive?\"\n",
      "Towards Building Model-Driven Tutorials for Humans. In Proceedings of the 2020\n",
      "CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA)\n",
      "(CHI ’20) . Association for Computing Machinery, New York, NY, USA, 1–13.\n",
      "https://doi.org/10.1145/3313831.3376873\n",
      "[43] Vivian Lai and Chenhao Tan. 2019. On Human Predictions with Explanations and\n",
      "Predictions of Machine Learning Models: A Case Study on Deception Detection.\n",
      "InProceedings of the Conference on Fairness, Accountability, and Transparency\n",
      "(Atlanta, GA, USA) (FAT* ’19) . Association for Computing Machinery, New York,\n",
      "NY, USA, 29–38. https://doi.org/10.1145/3287560.3287590\n",
      "[44] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Eric Horvitz. 2017. Iden-\n",
      "tifying Unknown Unknowns in the Open World: Representations and Policies\n",
      "for Guided Exploration. In Proceedings of the Thirty-First AAAI Conference on\n",
      "Artificial Intelligence (AAAI’17) . AAAI Press, San Francisco, California, USA,\n",
      "2124–2132.\n",
      "[45] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec.\n",
      "2017. Interpretable & Explorable Approximations of Black Box Models.\n",
      "arXiv:1707.01154 [cs.AI]\n",
      "[46] John D. Lee and Katrina A. See. 2004. Trust in Automation: Designing for\n",
      "Appropriate Reliance. Human Factors 46, 1 (2004), 50–80. https://doi.org/10.\n",
      "1518/hfes.46.1.50_30392\n",
      "[47] Timothy R Levine. 2014. Truth-default theory (TDT) a theory of human deception\n",
      "and deception detection. Journal of Language and Social Psychology 33, 4 (2014),\n",
      "378–392.\n",
      "[48] B. Lim, A. Dey, and D. Avrahami. 2009. Why and Why Not Explanations Improve\n",
      "the Intelligibility of Context-aware Intelligent Systems. In Proceedings of the\n",
      "SIGCHI Conference on Human Factors in Computing Systems (Boston, MA, USA)\n",
      "(CHI ’09) . ACM, New York, NY, USA, 2119–2128. https://doi.org/10.1145/1518701.\n",
      "1519023\n",
      "[49] Zhouhan Lin, Minwei Feng, Cícero Nogueira dos Santos, Mo Yu, Bing Xiang,\n",
      "Bowen Zhou, and Yoshua Bengio. 2017. A Structured Self-Attentive Sentence\n",
      "Embedding. In 5th International Conference on Learning Representations, ICLR\n",
      "2017, Conference Track Proceedings . OpenReview.net, Toulon, France, 1–15. https:\n",
      "//openreview.net/forum?id=BJC_jUqxe\n",
      "[50] Zachary C. Lipton. 2018. The Mythos of Model Interpretability: In Machine\n",
      "Learning, the Concept of Interpretability is Both Important and Slippery. Queue\n",
      "16, 3 (June 2018), 31–57. https://doi.org/10.1145/3236386.3241340\n",
      "[51] Angli Liu, Stephen Soderland, Jonathan Bragg, Christopher H. Lin, Xiao Ling,\n",
      "and Daniel S. Weld. 2016. Effective Crowd Annotation for Relation Extraction. In\n",
      "Proceedings of the 2016 Conference of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language Technologies . Association for\n",
      "Computational Linguistics, San Diego, California, 897–906. https://doi.org/10.\n",
      "18653/v1/N16-1104\n",
      "[52] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\n",
      "Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\n",
      "Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\n",
      "[53] Scott M. Lundberg, Bala Nair, Monica S. Vavilala, Mayumi Horibe, Michael J.\n",
      "Eisses, Trevor Adams, David E. Liston, Daniel King-Wai Low, Shu-Fang Newman,\n",
      "Jerry Kim, and Su-In Lee. 2018. Explainable machine-learning predictions for the\n",
      "prevention of hypoxaemia during surgery. Nature Biomedical Engineering 2, 10\n",
      "(01 Oct 2018), 749–760. https://doi.org/10.1038/s41551-018-0304-0\n",
      "[54] Les Macleod. 2011. Avoiding “groupthink” A manager’s challenge. Nursing\n",
      "management 42, 10 (2011), 44–48.\n",
      "[55] David Madras, Toniann Pitassi, and Richard Zemel. 2018. Predict Responsibly:\n",
      "Improving Fairness and Accuracy by Learning to Defer. In Proceedings of the\n",
      "32nd International Conference on Neural Information Processing Systems (Montréal,\n",
      "Canada) (NIPS’18) . Curran Associates Inc., Red Hook, NY, USA, 6150–6160.\n",
      "[56] Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012. Learning Attitudes\n",
      "and Attributes from Multi-Aspect Reviews. In Proceedings of the 2012 IEEE 12th\n",
      "International Conference on Data Mining (ICDM ’12) . IEEE Computer Society,\n",
      "USA, 1020–1025. https://doi.org/10.1109/ICDM.2012.110\n",
      "[57] T. Miller. 2018. Explanation in artificial intelligence: Insights from the social\n",
      "sciences. Artificial Intelligence 267 (February 2018), 1–38.\n",
      "[58] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,\n",
      "Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019.\n",
      "Model Cards for Model Reporting. In Proceedings of the Conference on Fairness,\n",
      "Accountability, and Transparency (Atlanta, GA, USA) (FAT* ’19) . Association for\n",
      "Computing Machinery, New York, NY, USA, 220–229. https://doi.org/10.1145/\n",
      "3287560.3287596CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "[59] Hussein Mozannar and David Sontag. 2020. Consistent Estimators for Learning to\n",
      "Defer to an Expert. In Proceedings of the 37th International Conference on Machine\n",
      "Learning (Proceedings of Machine Learning Research, Vol. 119) , Hal Daumé III and\n",
      "Aarti Singh (Eds.). PMLR, Virtual, 7076–7087. http://proceedings.mlr.press/v119/\n",
      "mozannar20b.html\n",
      "[60] Limor Nadav-Greenberg and Susan L Joslyn. 2009. Uncertainty forecasts improve\n",
      "decision making among nonexperts. Journal of Cognitive Engineering and Decision\n",
      "Making 3, 3 (2009), 209–227.\n",
      "[61] Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, and Finale\n",
      "Doshi-Velez. 2018. How do Humans Understand Explanations from Machine\n",
      "Learning Systems? An Evaluation of the Human-Interpretability of Explanation.\n",
      "arXiv:1802.00682 [cs.AI]\n",
      "[62] Dong Nguyen. 2018. Comparing Automatic and Human Evaluation of Local\n",
      "Explanations for Text Classification. In Proceedings of the 2018 Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics: Human\n",
      "Language Technologies, Volume 1 (Long Papers) . Association for Computational\n",
      "Linguistics, New Orleans, Louisiana, 1069–1078. https://doi.org/10.18653/v1/N18-\n",
      "1097\n",
      "[63] Joon Sung Park, Rick Barber, Alex Kirlik, and Karrie Karahalios. 2019. A Slow\n",
      "Algorithm Improves Users’ Assessments of the Algorithm’s Accuracy. Proceedings\n",
      "of the ACM on Human-Computer Interaction 3, CSCW (2019), 1–15.\n",
      "[64] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and\n",
      "Kyunghyun Cho. 2019. Finding Generalizable Evidence by Learning to Convince\n",
      "Q&A Models. In Proceedings of the 2019 Conference on Empirical Methods in\n",
      "Natural Language Processing and the 9th International Joint Conference on Natural\n",
      "Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics,\n",
      "Hong Kong, China, 2402–2411. https://doi.org/10.18653/v1/D19-1244\n",
      "[65] Forough Poursabzi-Sangdeh, Daniel G. Goldstein, Jake M. Hofman, Jennifer Wort-\n",
      "man Vaughan, and Hanna Wallach. 2019. Manipulating and Measuring Model\n",
      "Interpretability. arXiv:1802.07810 [cs.AI]\n",
      "[66] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I\n",
      "Trust You?\": Explaining the Predictions of Any Classifier. In Proceedings of the\n",
      "22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\n",
      "Mining (San Francisco, California, USA) (KDD ’16) . Association for Computing\n",
      "Machinery, New York, NY, USA, 1135–1144. https://doi.org/10.1145/2939672.\n",
      "2939778\n",
      "[67] Philipp Schmidt and Felix Biessmann. 2019. Quantifying Interpretability and\n",
      "Trust in Machine Learning Systems. arXiv:1901.08558 [cs.LG]\n",
      "[68] C. Estelle Smith, Bowen Yu, Anjali Srivastava, Aaron Halfaker, Loren Terveen,\n",
      "and Haiyi Zhu. 2020. Keeping Community in the Loop: Understanding Wikipedia\n",
      "Stakeholder Values for Machine Learning-Based Systems. In Proceedings of the\n",
      "2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI,\n",
      "USA) (CHI ’20) . Association for Computing Machinery, New York, NY, USA, 1–14.\n",
      "https://doi.org/10.1145/3313831.3376783\n",
      "[69] Simone Stumpf, Vidya Rajaram, Lida Li, Weng-Keen Wong, Margaret Burnett,\n",
      "Thomas Dietterich, Erin Sullivan, and Jonathan Herlocker. 2009. Interacting\n",
      "meaningfully with machine learning systems: Three experiments. International\n",
      "Journal of Human-Computer Studies 67, 8 (2009), 639–662.\n",
      "[70] Sarah Tan, Julius Adebayo, Kori Inkpen, and Ece Kamar. 2018. Inves-\n",
      "tigating Human + Machine Complementarity for Recidivism Predictions.\n",
      "arXiv:1808.09123 [cs.LG][71] LSAT Prep Books Team. 2017. LSAT prep book study guide: quick study & practice\n",
      "test questions for the Law School Admissions council’s (LSAC) Law school admission\n",
      "test. Mometrix Test Preparation, Beaumont, TX.\n",
      "[72] Kush R. Varshney, Prashant Khanduri, Pranay Sharma, Shan Zhang, and\n",
      "Pramod K. Varshney. 2018. Why Interpretability in Machine Learn-\n",
      "ing? An Answer Using Distributed Detection and Data Fusion Theory.\n",
      "arXiv:1806.09710 [stat.ML]\n",
      "[73] J. von Neumann and O. Morgenstern. 1947. Theory of games and economic\n",
      "behavior . Princeton University Press, Princeton, New Jersey.\n",
      "[74] Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y. Lim. 2019. Design-\n",
      "ing Theory-Driven User-Centric Explainable AI. In Proceedings of the 2019 CHI\n",
      "Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk)\n",
      "(CHI ’19) . Association for Computing Machinery, New York, NY, USA, 1–15.\n",
      "https://doi.org/10.1145/3290605.3300831\n",
      "[75] Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. 2016. Attention-based\n",
      "LSTM for Aspect-level Sentiment Classification. In Proceedings of the 2016 Confer-\n",
      "ence on Empirical Methods in Natural Language Processing . Association for Com-\n",
      "putational Linguistics, Austin, Texas, 606–615. https://doi.org/10.18653/v1/D16-\n",
      "1058\n",
      "[76] Hilde J. P. Weerts, Werner van Ipenburg, and Mykola Pechenizkiy. 2019. A Human-\n",
      "Grounded Evaluation of SHAP for Alert Processing. arXiv:1907.03324 [cs.LG]\n",
      "[77] Daniel S. Weld and Gagan Bansal. 2019. The Challenge of Crafting Intelligible\n",
      "Intelligence. Commun. ACM 62, 6 (May 2019), 70–79. https://doi.org/10.1145/\n",
      "3282486\n",
      "[78] Jenna Wiens, John Guttag, and Eric Horvitz. 2016. Patient risk stratification\n",
      "with time-varying parameters: a multitask learning approach. JMLR 17, 1 (2016),\n",
      "2797–2819.\n",
      "[79] Bryan Wilder, Eric Horvitz, and Ece Kamar. 2020. Learning to Complement\n",
      "Humans. In Proceedings of the Twenty-Ninth International Joint Conference on\n",
      "Artificial Intelligence, IJCAI-20 , Christian Bessiere (Ed.). International Joint Con-\n",
      "ferences on Artificial Intelligence Organization, Yokohama, Japan, 1526–1533.\n",
      "https://doi.org/10.24963/ijcai.2020/212 Main track.\n",
      "[80] Fumeng Yang, Zhuanyi Huang, Jean Scholtz, and Dustin L. Arendt. 2020. How Do\n",
      "Visual Explanations Foster End Users’ Appropriate Trust in Machine Learning?.\n",
      "InProceedings of the 25th International Conference on Intelligent User Interfaces\n",
      "(Cagliari, Italy) (IUI ’20) . Association for Computing Machinery, New York, NY,\n",
      "USA, 189–201. https://doi.org/10.1145/3377325.3377480\n",
      "[81] Kun Yu, Shlomo Berkovsky, Ronnie Taib, Jianlong Zhou, and Fang Chen. 2019.\n",
      "Do I Trust My Machine Teammate? An Investigation from Perception to Decision.\n",
      "InProceedings of the 24th International Conference on Intelligent User Interfaces\n",
      "(Marina del Ray, California) (IUI ’19) . Association for Computing Machinery, New\n",
      "York, NY, USA, 460–468. https://doi.org/10.1145/3301275.3302277\n",
      "[82] Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. ReClor: A Reading\n",
      "Comprehension Dataset Requiring Logical Reasoning. In 8th International Con-\n",
      "ference on Learning Representations, ICLR 2020 . OpenReview.net, Addis Ababa,\n",
      "Ethiopia, 1–26. https://openreview.net/forum?id=HJgJtT4tvB\n",
      "[83] Yunfeng Zhang, Q. Vera Liao, and Rachel K. E. Bellamy. 2020. Effect of Confi-\n",
      "dence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision\n",
      "Making. In Proceedings of the 2020 Conference on Fairness, Accountability, and\n",
      "Transparency (Barcelona, Spain) (FAT* ’20) . Association for Computing Machin-\n",
      "ery, New York, NY, USA, 295–305. https://doi.org/10.1145/3351095.3372852\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The title of the paper is \"Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance.\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task3 = \"\"\"Based on the text in paper.txt, answer the following question.\n",
    "\n",
    "Question: What it the title of the paper?\n",
    "\n",
    "Hint: Print the contents of the pdf, and you your language ability to answer.\n",
    "\"\"\"\n",
    "user_proxy.initiate_chat(assistant, message=task3, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Based on the text in paper.txt, answer the following question.\n",
      "The answer must be 3 bullet points that can be inserted in a ppt slide.\n",
      "\n",
      "Question: What is the key problem being discussed in the paper?\n",
      "\n",
      "Hint: Print the contents of the pdf, and you your language ability to answer.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "First, let's print the contents of the `paper.txt` file again:\n",
      "\n",
      "```python\n",
      "file_name = \"paper.txt\"\n",
      "\n",
      "with open(file_name, \"r\") as f:\n",
      "    content = f.read()\n",
      "\n",
      "print(content)\n",
      "```\n",
      "\n",
      "After the content is printed, I will use my language ability to find the key problem being discussed in the paper.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Does the Whole Exceed its Parts? The Effect of AI Explanations\n",
      "on Complementary Team Performance\n",
      "Gagan Bansal∗\n",
      "Tongshuang Wu∗\n",
      "bansalg@cs.washington.edu\n",
      "wtshuang@cs.washington.edu\n",
      "University of WashingtonJoyce Zhou†\n",
      "Raymond Fok†\n",
      "jyzhou15@cs.washington.edu\n",
      "rayfok@cs.washington.edu\n",
      "University of WashingtonBesmira Nushi\n",
      "besmira.nushi@microsoft.com\n",
      "Microsoft Research\n",
      "Ece Kamar\n",
      "eckamar@microsoft.com\n",
      "Microsoft ResearchMarco Tulio Ribeiro\n",
      "marcotcr@microsoft.com\n",
      "Microsoft ResearchDaniel S. Weld\n",
      "weld@cs.washington.edu\n",
      "University of Washington &\n",
      "Allen Institute for Artificial\n",
      "Intelligence\n",
      "ABSTRACT\n",
      "Many researchers motivate explainable AI with studies showing\n",
      "that human-AI team performance on decision-making tasks im-\n",
      "proves when the AI explains its recommendations. However, prior\n",
      "studies observed improvements from explanations only when the\n",
      "AI, alone, outperformed both the human and the best team. Can\n",
      "explanations help lead to complementary performance , where team\n",
      "accuracy is higher than either the human or the AI working solo?\n",
      "We conduct mixed-method user studies on three datasets, where an\n",
      "AI with accuracy comparable to humans helps participants solve a\n",
      "task (explaining itself in some conditions). While we observed com-\n",
      "plementary improvements from AI augmentation, they were notin-\n",
      "creased by explanations. Rather, explanations increased the chance\n",
      "that humans will accept the AI’s recommendation, regardless of its\n",
      "correctness. Our result poses new challenges for human-centered\n",
      "AI: Can we develop explanatory approaches that encourage ap-\n",
      "propriate trust in AI, and therefore help generate (or improve)\n",
      "complementary performance?\n",
      "CCS CONCEPTS\n",
      "•Human-centered computing →Empirical studies in HCI ;\n",
      "Interactive systems and tools ;•Computing methodologies →\n",
      "Machine learning.\n",
      "KEYWORDS\n",
      "Explainable AI, Human-AI teams, Augmented intelligence\n",
      "∗Equal contribution.\n",
      "†Made especially large contributions.\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than the\n",
      "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\n",
      "republish, to post on servers or to redistribute to lists, requires prior specific permission\n",
      "and/or a fee. Request permissions from permissions@acm.org.\n",
      "CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "©2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n",
      "ACM ISBN 978-1-4503-8096-6/21/05. . . $15.00\n",
      "https://doi.org/10.1145/3411764.3445717ACM Reference Format:\n",
      "Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi,\n",
      "Ece Kamar, Marco Tulio Ribeiro, and Daniel S. Weld. 2021. Does the Whole\n",
      "Exceed its Parts? The Effect of AI Explanations on Complementary Team\n",
      "Performance. In CHI Conference on Human Factors in Computing Systems\n",
      "(CHI ’21), May 8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA,\n",
      "16 pages. https://doi.org/10.1145/3411764.3445717\n",
      "1 INTRODUCTION\n",
      "Although the accuracy of Artificial Intelligence (AI) systems is\n",
      "rapidly improving, in many cases, it remains risky for an AI to op-\n",
      "erate autonomously, e.g., in high-stakes domains or when legal and\n",
      "ethical matters prohibit full autonomy. A viable strategy for these\n",
      "scenarios is to form Human-AI teams , in which the AI system aug-\n",
      "ments one or more humans by recommending its predictions, but\n",
      "people retain agency and have accountability on the final decisions.\n",
      "Examples include AI systems that predict likely hospital readmis-\n",
      "sion to assist doctors with correlated care decisions [ 8,13,15,78]\n",
      "and AIs that estimate recidivism to help judges decide whether\n",
      "to grant bail to defendants [ 2,30]. In such scenarios, it is impor-\n",
      "tant that the human-AI team achieves complementary performance\n",
      "(i.e., performs better than either alone): From a decision-theoretic\n",
      "perspective, a rational developer would only deploy a team if it\n",
      "adds utility to the decision-making process [ 73]. For example, sig-\n",
      "nificantly improving decision accuracy by closing deficiencies in\n",
      "automated reasoning with human effort, and vice versa [35, 70].\n",
      "Many researchers have argued that such human-AI teams would\n",
      "be improved if the AI systems could explain their reasoning . In addi-\n",
      "tion to increasing trust between humans and machines or improving\n",
      "the speed of decision making, one hopes that an explanation should\n",
      "help the responsible human know when to trust the AI’s sugges-\n",
      "tion and when to be skeptical, e.g., when the explanation doesn’t\n",
      "“make sense.” Such appropriate reliance [46] is crucial for users to\n",
      "leverage AI assistance and improve task performance [ 10]. Indeed,\n",
      "at first glance, it appears that researchers have already confirmed\n",
      "the utility of explanations on tasks ranging from medical diagno-\n",
      "sis [14,53], data annotation [ 67] to deception detection [ 43]. In\n",
      "each case, the papers show that, when the AI provides explanations,\n",
      "team accuracy reaches a level higher than human-alone.arXiv:2006.14779v3  [cs.AI]  12 Jan 2021CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "Recommendation (R)\n",
      "+ Explanation\n",
      "0.01.0\n",
      "A\n",
      "Input Decision\n",
      "Human AI\n",
      "Complementary zone \n",
      "( max (Human, AI) , 1 ]\n",
      "Teammates\n",
      "Prior work Ours> 0\n",
      "Change of performance\n",
      "0.0Accuracy of decisions1.0\n",
      "BAccuracy of decisions\n",
      "R+Explanation\n",
      "R only> 0R+Explanation\n",
      "R only?Team setting\n",
      "Figure 1: (Best viewed in color) Do AI explanations lead to complementary team performance? In a team setting, when given an\n",
      "input, the human uses (usually imperfect) recommendations from an AI model to make the final decision. We seek to under-\n",
      "stand if automatically generated explanations of the AI’s recommendation improve team performance compared to baselines,\n",
      "such as simply providing the AI’s recommendation, 𝑅, and confidence. (A) Most previous work concludes that explanations\n",
      "improve team performance ( i.e.,Δ𝐴>0); however, it usually considers settings where AI systems are much more accurate\n",
      "than people and even the human-AI team. (B) Our study considers settings where human and AI performance is comparable\n",
      "to allow room for complementary improvement. We ask, “Do explanations help in this context, and how do they compare to\n",
      "simple confidence-based strategies?” (Is Δ𝐵>0?).\n",
      "However, a careful reading of these papers shows another com-\n",
      "monality: in every situation, while explanations are shown to help\n",
      "raise team performance closer to that of the AI, one would achieve\n",
      "an even better result by stripping humans from the loop and let-\n",
      "ting the AI operate autonomously (Figure 1A & Table 1). Thus,\n",
      "the existing work suggests several important open questions for\n",
      "the AI and HCI community: Do explanations help achieve comple-\n",
      "mentary performance by enabling humans to anticipate when the\n",
      "AI is potentially incorrect? Furthermore, do explanations provide\n",
      "significant value over simpler strategies such as displaying the AI’s\n",
      "uncertainty? In the quest to build the best human-machine teams,\n",
      "such questions deserve critical attention.\n",
      "To explore these questions, we conduct new experiments where\n",
      "we control the study design, ensuring that the AI’s accuracy is\n",
      "comparable to the human’s (Figure 1B). Specifically, we measure\n",
      "the human skill on our experiment tasks and then control AI ac-\n",
      "curacy by purposely selecting study samples where AI has compa-\n",
      "rable accuracy. This setting simulates situations where there is a\n",
      "strong incentive to deploy human-AI teams, e.g., because there ex-\n",
      "ists more potential for complementary performance (by correcting\n",
      "each other’s mistakes), and where simple heuristics such as blindly\n",
      "following the AI are unlikely to achieve the highest performance.\n",
      "We selected three common-sense tasks that can be tackled by\n",
      "crowd workers with little training: sentiment analysis of book and\n",
      "beer reviews and a set of LSAT questions that require logical reason-\n",
      "ing. We conducted large-scale studies using a variety of explanation\n",
      "sources (AI versus expert-generated) and strategies (explaining just\n",
      "the predicted class, or explaining other classes as well). We observed\n",
      "complementary performance on every task, but — surprisingly —\n",
      "explanations did not appear to offer benefit compared to simply\n",
      "displaying the AI’s confidence. Notably, explanations increased\n",
      "reliance on recommendations even when the AI was incorrect. Our\n",
      "result echoes prior work on inappropriate trust on systems [ 38,58],\n",
      "i.e., explanations can lead humans to either follow incorrect AIsuggestions or ignore the correct ones [ 13,69]. However, using\n",
      "end-to-end studies, we go one step further to quantify the impact\n",
      "of such over-reliance on objective metrics of team performance.\n",
      "As a first attempt to tackle the problem of blind reliance on AI,\n",
      "we introduce Adaptive Explanation . Our mechanism tries to reduce\n",
      "human trust when the AI has low confidence: it only explains\n",
      "the predicted class when the AI is confident, but also explains\n",
      "the alternative otherwise. While it failed to produce significant\n",
      "improvement in final team performance over other explanation\n",
      "types, there is suggestive evidence that the adaptive approach can\n",
      "push the agreement between AI predictions and human decisions\n",
      "towards the desired direction.\n",
      "Through extensive qualitative analysis, we also summarize po-\n",
      "tential factors that should be considered in experimental settings for\n",
      "studying human-AI complementary performance. For example, the\n",
      "difference in expertise between human and AI affects whether (or\n",
      "how much) AI assistance will help achieve complementary perfor-\n",
      "mance, and the display of the explanation may affect the human’s\n",
      "collaboration strategy. In summary:\n",
      "•We highlight an important limitation of previous work on\n",
      "explainable AI: While many studies show that explaining\n",
      "predictions of AI increases team performance (Table 1), they\n",
      "all consider cases where the AI system is significantly more\n",
      "accurate than both the human partner and the human-AI\n",
      "team. In response, we argue that AI explanations for decision-\n",
      "making should aim for complementary performance, where\n",
      "the human-AI team outperforms both solo human and AI.\n",
      "•To study complementary performance, we develop a new\n",
      "experimental setup and use it in studies with 1626 users on\n",
      "three tasks1to evaluate a variety of explanation sources and\n",
      "strategies. We observe complementary performance in every\n",
      "human-AI teaming condition.\n",
      "1All the task examples and the collected experiment data are available at https://github.\n",
      "com/uw-hai/Complementary-Performance.Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "•However, surprisingly, we do not observe any significant\n",
      "increase in team performance by communicating explana-\n",
      "tions, compared to simply showing the AI’s confidence. Ex-\n",
      "planations often increased accuracy when the AI system\n",
      "was correct but, worryingly, decreased it when the AI erred,\n",
      "resulting in a minimal net change — even for our adaptive\n",
      "explanations. Through qualitative analysis, we discuss poten-\n",
      "tial causes for failure of explanations, behavioral differences\n",
      "among tasks, and suggest directions for developing more\n",
      "effective AI explanations.\n",
      "2 BACKGROUND AND RELATED WORK\n",
      "Explanations can be useful in many scenarios where a human\n",
      "and AI interact: transparently communicating model predictions\n",
      "[10,21,38,40,66], teaching humans tasks like translation [ 3,26]\n",
      "or content moderation [ 36], augmenting human analysis proce-\n",
      "dure [ 36] or creativity [ 17], legal imperatives [ 57,77], etc. Various\n",
      "studies have evaluated the effect of explanations from different di-\n",
      "mensions, including whether the explanation improves users’ trust\n",
      "in the AI [ 41,81] or enables users to simulate the model predic-\n",
      "tions [16, 65], or assists developers to debug models [9, 38].\n",
      "In this paper, we focus explicitly on AI-assisted decision making\n",
      "scenarios [ 6,74], where an AI assistant ( e.g., a classification model)\n",
      "makes recommendations to a human ( e.g., a judge), who is respon-\n",
      "sible for making final decisions ( e.g., whether or not to grant bail).\n",
      "In particular, we assess performance in terms of the accuracy of the\n",
      "human-AI team. While other metrics can be used for evaluation\n",
      "(more discussed in Section 6.1), we directly evaluate end-to-end\n",
      "team accuracy for three reasons. First, deploying such a human-AI\n",
      "team is ideal if it achieves complementary performance ,i.e., if it\n",
      "outperforms both the AI and the human acting alone. Second, eval-\n",
      "uating explanations using proxy tasks (such as whether humans\n",
      "can use it to guess the model’s prediction) can lead to different,\n",
      "misleading conclusions for achieving best team performance than\n",
      "an end-to-end evaluation [ 12]. Third, AI-assisted decision making\n",
      "is often listed as a major motivation for AI explanations. In recent\n",
      "years numerous papers have employed user studies to show that\n",
      "human accuracy increases if the AI system explains its reasoning\n",
      "for tasks as diverse as medical diagnosis, predicting loan defaults,\n",
      "and answering trivia questions. However, as summarized in Table\n",
      "1, complementary performance was not observed in any of these\n",
      "studies – in each case, adding the human to the loop decreased\n",
      "performance compared to if AI had acted alone.\n",
      "For example, in Lai et al. [42,43], MTurk workers classified de-\n",
      "ceptive hotel reviews with predictions from SVM and BERT-based\n",
      "models, as well as explanations in the form of inline-highlights.\n",
      "However, models outperformed every team (see Table 1 and Fig-\n",
      "ure 6 in [ 42]). Zhang et al. [83] noticed the superior behavior of\n",
      "the models in Lai et al. ’s work, and evaluated the accuracy and\n",
      "trust calibration where the gap between human and the AI per-\n",
      "formances was less severe. Still, on their task of income category\n",
      "prediction, their Gradient Boosted Trees model had 10% higher ac-\n",
      "curacy compared to their MTurk workers, which seemed borderline\n",
      "“comparable” at best. Furthermore, when run autonomously, their\n",
      "AI model performed just slightly better than the best team (see\n",
      "Section 4.2.2 and Figure 10 in [ 83]). A similar performance trend isobserved on tasks other than classification. In Sangdeh et al. [65],\n",
      "MTurk workers predicted house price using various regression mod-\n",
      "els that generated explanations in terms of most salient features.\n",
      "Their models’ predictions resulted in lowest error (See Figure 6\n",
      "in [65]). In Feng et al. [21], experts and novices played Quiz Bowl\n",
      "with recommendation from Elastic Search system. The system ex-\n",
      "plained its predictions by presenting training examples that were\n",
      "influential, and using inline-highlights to explain the connection\n",
      "between question and evidence. However, Feng et al. do not report\n",
      "the exact performance of the AI on their study sample, but mention\n",
      "its superiority in Section 3.1 in [ 21] pointing out that it outper-\n",
      "forms top trivia players. One possible exception is Bligic & Mooney\n",
      "(2005) [ 10], who probably achieved complementary performance on\n",
      "their task of recommending books to users. However, they did not\n",
      "compare explanations against simple baselines, such as showing\n",
      "the book title or the system confidence (rating).\n",
      "At least two potential causes account for the absence of comple-\n",
      "mentary performance in these cases. First, task design may have\n",
      "hindered collaboration: previous researchers considered AI systems\n",
      "whose accuracy was substantially higher than the human’s, leading\n",
      "to a small zone with potential for complementary performance\n",
      "(see Figure 1). For example, this may have made it more likely that\n",
      "human errors were a superset of the AI’s, reducing the possibility of\n",
      "a human overseer spotting a machine mistake. Second, even when\n",
      "the task has the potential for complementary performance, it is\n",
      "unclear if the collaboration mechanisms under study supported it.\n",
      "Collaboration factors like incentives, the format of explanations,\n",
      "and whether AI’s uncertainty was displayed may drive the human\n",
      "towards simple, less collaborative heuristics, such as “always trust\n",
      "the AI” or “never trust the AI.”\n",
      "3 SETUP AND PILOT STUDIES\n",
      "To better understand the role of explanations in producing com-\n",
      "plementary performance, we enlarge the zone of potential comple-\n",
      "mentarity by matching AI accuracy to that of an average human,2\n",
      "and investigate multiple explanation styles on several domains (Sec-\n",
      "tion 3.1). As Table 2 summarizes, we first designed and conducted\n",
      "pilots studies (Sections 3.2) and used them to inform our final study\n",
      "and hypotheses (Section 4).\n",
      "3.1 Choice of Tasks and Explanations\n",
      "Since our interest is in AI-assisted decision making , we studied\n",
      "the effect of local explanations on team performance – that is,\n",
      "explaining each individual recommendation made by a model [ 66].\n",
      "This contrasts with providing a global understanding of the full\n",
      "model all at once ( e.g., [45]).\n",
      "We conducted experiments on two types of tasks: text classifi-\n",
      "cation (sentiment analysis) and question answering. Text classifi-\n",
      "cation because it is a popular task in natural language processing\n",
      "(NLP) that has been used in several previous studies on human-AI\n",
      "2Of course, complementary performance may be possible even in situations when one\n",
      "of the team partners is significantly more accurate than the other. For example, a low-\n",
      "accuracy team member may be valuable if their errors are independent, because they\n",
      "may be able to spot mistakes made by the team majority. However, it is more difficult\n",
      "to observe complementary performance in such settings, so we first consider the case\n",
      "where humans and AI have similar accuracy. If explanations cannot provide value in\n",
      "such settings, it will be even more difficult to show complementary performance when\n",
      "teammates have disparate skills.CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "Domain TaskPerformance\n",
      "Metric Human alone AI alone Team Complementary?\n",
      "ClassificationDeceptive review [43] Accuracy ↑ 51.1% 87.0% 74.6% ✗\n",
      "Deceptive review [42] Accuracy ↑ 54.6% 86.3% 74.0% ✗\n",
      "Income category [83] Accuracy ↑ 65% 75% 73% ✗\n",
      "Loan defaults [27] Norm. Brier ↑ 0 1 0.682 ✗\n",
      "Hypoxemia risk [53] AUC ↑ 0.66 0.81 0.78 ✗\n",
      "Nutrition prediction [12] Accuracy ↑ 0.46 0.75 0.74 ✗\n",
      "QA Quiz bowl [21] “AI outperforms top trivia players. ” ✗\n",
      "Regression House price [65] Avg. Absolute Error ↓ $331k $200k $232k ✗\n",
      "Table 1: Recent studies that evaluate the effect of automatically generated explanations on human-AI team performance.\n",
      "While explanations did improve team accuracy, the performance was not complementary — acting autonomously, the AI\n",
      "would have performed even better. For papers with multiple domains or experiments, we took one sample with the most\n",
      "comparable human and AI performance. ↑(or↓) indicates whether the metric should be maximized (or minimized).\n",
      "Explain. Strategies Explain. Sources Tasks\n",
      "Explain-Top-1 AI Beer\n",
      "Explain-Top-2 Expert Amzbook\n",
      "Adaptive LSAT\n",
      "Table 2: An overview of our tasks, explanation strategies and\n",
      "sources. We ran our pilot studies (Section 3.2) with condi-\n",
      "tions marked with . Based on the pilot results, we added\n",
      "adaptive explanations and expert explanations (Section 3.3).\n",
      "Along with two additional domains, these form the condi-\n",
      "tions for our final study conditions (Section 4.1).\n",
      "teaming [ 21,29,42,50,62,83] and because it requires little domain\n",
      "expertise, and is thus amenable to crowdsourcing. Specifically, we\n",
      "selected two sentiment analysis datasets to improve the generaliza-\n",
      "tion of our results: beer reviews [ 56] and book reviews [ 31]. More\n",
      "details about these datasets are in Section 4.2. While there exist\n",
      "various local explanation approaches for text classification, we rely\n",
      "onlocal saliency explanations , which explain a single prediction in\n",
      "terms of the importance of input features ( e.g., each word) towards\n",
      "the model’s prediction ( e.g., positive or negative sentiment).\n",
      "As commonly practiced in previous work [ 21,42,43], we dis-\n",
      "play explanations with inline-highlights ,i.e., directly highlighting\n",
      "the explanation in the input text, so the user need not go back\n",
      "and forth between input and the explanation. While there exist\n",
      "other explanatory approaches, such as feature-importance visual-\n",
      "ization [ 11,27,53,61,76] (more suitable for tabular data) or com-\n",
      "municating influential training examples [ 3,40,80] (more suitable\n",
      "for images), these techniques are not ideal for text because they\n",
      "add an additional cognitive cost to mapping the explanation to the\n",
      "respective text. Figure 2 shows one example beer review.\n",
      "We also experimented with Law School Admission Test (LSAT)\n",
      "questions3because it is more challenging. In this task, every ques-\n",
      "tion contains four options with a unique correct answer (Figure 3).\n",
      "Again, answering LSAT questions requires no specialized knowl-\n",
      "edge except common-sense reasoning skills, such as recognizing\n",
      "logical connections and conflicts between arguments [ 82]. Because\n",
      "3https://en.wikipedia.org/wiki/LawSchoolAdmissionTestin this case it is unclear how inline-highlights could be used to com-\n",
      "municate logical constructs ( e.g., contradiction may not be visible\n",
      "by highlighting the input alone), we turned to narrative explana-\n",
      "tions which justify a candidate answer in natural language. We\n",
      "explain these in more detail in Section 4.2.\n",
      "3.2 Pilot Study on Sentiment Classification\n",
      "To iterate on the hypotheses and the associated explanation condi-\n",
      "tions for our main study (detailed later in Section 4), we conducted\n",
      "a pilot study on one of our datasets ( Beer). The between-subject\n",
      "pilot study asked crowdworkers to judge the sentiment of 50 beer\n",
      "reviews with assistance from a logistic regression classifier in three\n",
      "conditions, each condition with 50 workers. One condition only\n",
      "showed the model prediction and confidence; the other two also\n",
      "included the following common explanation strategies4:\n",
      "(1)Explain-Top-1 explains just the predicted class by highlight-\n",
      "ing the most influential words for that class.\n",
      "(2)Explain-Top-2 explains the top two predicted classes, and un-\n",
      "like Explain-Top-1, it also color codes and highlights words\n",
      "for the other sentiment class.\n",
      "The two strategies closely align with the design in prior work [ 24,\n",
      "43,49,75], and have been shown to be beneficial (Table 1). Explain-\n",
      "Top-2 also corresponds to Wang et al. ’s suggestion to mitigate\n",
      "heuristic biases by explaining “multiple outcomes” [74].\n",
      "Observations We summarize our findings from the pilot study:\n",
      "(1)Contrary to many prior works, we observed no significant\n",
      "changes or improvements in aggregated team accuracy by\n",
      "displaying either type of explanations.\n",
      "(2)That said, explaining just the predicted class (Explain-Top-1)\n",
      "performed better than explaining both (Explain-Top-2) .\n",
      "(3)We also observed that explanations increased reliance on rec-\n",
      "ommendations even when they were incorrect : explaining the\n",
      "predicted class slightly improved performance (compared to\n",
      "confidence only) when the recommendation was correct but\n",
      "decreased performance when it was incorrect.\n",
      "(4)This effect was less pronounced in Explain-Top-2, presum-\n",
      "ably because it encouraged users to consider alternatives and\n",
      "4the saliency scores were based on feature weights learned by the linear model [ 27,42]Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "hence deterred over-reliance. In Figure 2, for example, if\n",
      "counter-argument (d) was not highlighted, participants could\n",
      "easily stop reading at the highlighted first sentence and over-\n",
      "look the negative ending.\n",
      "(5)Finally, participants indicated that they wanted higher quality\n",
      "explanations. Crowd-workers were confused when explana-\n",
      "tions did not seem to correlate with model behavior.\n",
      "Because we made similar observations in our main study, we\n",
      "defer detailed discussions and implications of these observations to\n",
      "Section 5.1 and Figure 4.\n",
      "3.3 Additional Explanation Strategies/Sources\n",
      "Added Strategy: Adaptive Explanations. The pilot study indi-\n",
      "cated that Explain-Top-2 was more beneficial than Explain-Top-1\n",
      "when the classifier made mistakes, but not otherwise. Relying on the\n",
      "commonly seen correlations between mistakes and low-confidence\n",
      "[32], we developed a new dynamic strategy, adaptive explanation ,\n",
      "that switches between Explain-Top-1 and Explain-Top-2 depending\n",
      "on the AI’s confidence. This method explains the top-two classes\n",
      "only when the classifier confidence is below a task- and model-\n",
      "specific threshold (described later in Section 4.2), explaining only\n",
      "the top prediction otherwise. Intuitively, it was inspired by an effi-\n",
      "cient assistant that divulges more information (confessing doubts\n",
      "and arguing for alternatives) only when it is unsure about its recom-\n",
      "mendation. Adaptive explanations can also be viewed as changing\n",
      "explanation according to context [1]. While we limit the our context\n",
      "to the AI’s confidence, in general, one could rely on more features\n",
      "of the human-AI team, such as the user, location, or time [ 34,48].\n",
      "Added Source: Expert-Generated Explanations. Users in our\n",
      "pilot study were confused when the explanations did not make in-\n",
      "tuitive sense, perhaps due to either the quality of the underlying\n",
      "linear model-based AI. While we test state-of-the-art models in\n",
      "the final study, we also added expert-generated explanations to\n",
      "serve as an upper bound on explanation quality. We describe their\n",
      "annotation process in Section 4.2.\n",
      "4 FINAL STUDY\n",
      "Based on our pilot studies, we formulated our final hypotheses and\n",
      "used them to inform our final conditions and their interface (Sec-\n",
      "tion 4.1). We then tested these hypotheses for several tasks and AI\n",
      "systems (Section 4.2) through crowdsourcing studies (Section 4.3).\n",
      "4.1 Hypotheses, Conditions, and Interface\n",
      "We formulated the following hypotheses for sentiment analysis:\n",
      "H1Among current explanation strategies, explaining the predicted\n",
      "class will perform better than explaining both classes.\n",
      "H2The better strategies, Explain-Top-1, will still perform similarly\n",
      "to simply showing confidence.\n",
      "H3Our proposed Adaptive explanations, which combines benefits\n",
      "of existing strategies, will improve performance.\n",
      "H4Adaptive explanations would perform even better if AI could\n",
      "generate higher quality explanations.\n",
      "Since generating AI explanations for LSAT was not feasible (Sec-\n",
      "tion 3.3), we slightly modified the hypothesis for LSAT : we omitted\n",
      "the hypothesis on explanation quality ( H4) and tested the first three\n",
      "hypotheses using expert- rather than AI-generated explanations.Conditions .For both domains, we ran two baseline conditions:\n",
      "unassisted users ( Human ), as well as a simple AI assistance that\n",
      "shows the AI’s recommendation and confidence but no explanation\n",
      "(Team (Conf) ). We use this simple assistance because it serves as\n",
      "a stronger and broadly acknowledged baseline than the alterna-\n",
      "tive, i.e., displaying AI’s recommendation without confidence. First,\n",
      "most ML models can generate confidence scores that, in practice,\n",
      "correlate with the model’s true likelihood to err [ 32]. Second, dis-\n",
      "playing uncertainty in predictions can help users can make more\n",
      "optimal decisions [ 19,22,25,37,60]. Hence, we focus on evaluating\n",
      "whether the explanations provide additional value when shown\n",
      "alongside confidence scores. In the rest of the paper, we indicate the\n",
      "explanation conditions using the following template: Team (Strat-\n",
      "egy, Source) . For example, Team (Explain-Top-1, AI) indicates the\n",
      "condition that shows the AI’s explanations for the top prediction.\n",
      "Interface .Figure 2 shows an example UI for sentiment clas-\n",
      "sification for Team (Adaptive, AI). In all explanation conditions,\n",
      "explanations are displayed as inline highlights, with the background\n",
      "color aligned with the positive/negative label buttons. The highlight\n",
      "varies by condition, e.g., Team (Adaptive, AI) has a similar display to\n",
      "Figure 2, except that the AI picks multiple short phrases, instead of\n",
      "a full sentence. In Team (Explain-Top-1, AI) the counter-argument\n",
      "(d) is always missing, and in Team (Conf) no explanations are high-\n",
      "lighted. Figure 3 shows a screenshot of the user interface for LSAT\n",
      "in the Team (Adaptive, Expert) condition.\n",
      "4.2 AI Model, Study Samples and Explanations\n",
      "4.2.1 Sentiment Classification.\n",
      "Training data. To prepare each dataset ( Beer and Amzbook ) for\n",
      "training classification models, we binarized the target labels, split\n",
      "the dataset into training and test sets (80/20 split), removed class\n",
      "imbalance from the train split by oversampling the minority class,\n",
      "and further split the training set to create a validation set.\n",
      "AI Model. For each dataset, we fine-tuned a RoBERTa-based [ 52]\n",
      "text classifier from AllenNLP5on the training dataset and per-\n",
      "formed hyper-parameter selection on the validation set.\n",
      "Task examples. For each domain, we selected 50 examples from\n",
      "the test set to create our study sample. We first conducted additional\n",
      "pilot studies to establish the accuracy of unassisted users, which we\n",
      "observed were 87% for Beer and 85% for Amzbook6We then selected\n",
      "50 unambiguous examples so that the AI’s accuracy was 84% ( i.e.,\n",
      "comparable to human accuracy), with equal false positive and false\n",
      "negative rates. The filtering was for keeping the task objective:\n",
      "If the ground-truth answer was unclear, one cannot compute or\n",
      "compare the accuracy of decisions.\n",
      "Explanations. To generate saliency explanations, we used LIME,\n",
      "which is a popular post hoc method [ 66]. We chose this setup be-\n",
      "cause the combination of RoBERTa and LIME was consistently\n",
      "ranked the highest among the various systems we tried in an ex-\n",
      "plainer comparison study with human judges (details in Appendix).\n",
      "Despite offering accurate predictions, RoBERTa generated poorly\n",
      "calibrated confidence scores, a common issue with neural networks\n",
      "5https://demo.allennlp.org/sentiment-analysis\n",
      "6Again, each condition containing 50 crowd-workers. We estimated the human accu-\n",
      "racy on all the three datasets with another 150 crowd-workers.CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "a\n",
      "bc\n",
      "d\n",
      "Figure 2: A screenshot of the Team (Adaptive, Expert) condition for the Amzbook reviews dataset. Participants read the review\n",
      "(left pane) and used the buttons (right pane) to decide if the review was mostly positive ornegative . The right pane also shows\n",
      "progress and accuracy (a). To make a recommendation, the AI (called “Marvin”) hovers above a button (b) and displays the\n",
      "confidence score under the button. In this case, the AI incorrectly recommended that this review was positive, with confidence\n",
      "62.7%. As part of the explanation, the AI highlighted the most positive sentence (c) in the same color as the positive button.\n",
      "Because confidence was low, the AI also highlights the most negative sentence (d) to provide a counter-argument.\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "0 / 20\n",
      "Figure 3: A screenshot of Team (Adaptive, Expert) for LSAT . Similar to Figure 2, the interface contained a progress indicator (a),\n",
      "AI recommendation (b), and explanations for the top-2 predictions (c and d). To discourage participants from blindly following\n",
      "the AI, all AI information is displayed on the right. In (b), the confidence score is scaled so those for top-2 classes sum to 100%.\n",
      "[28], which we mitigated with post hoc calibration (isotonic regres-\n",
      "sion [7]) on the validation set.\n",
      "In particular, for Adaptive explanation, we used the classifier’s\n",
      "median confidence as the threshold to have an equal number of\n",
      "25 examples displayed as Explain-Top-1 and Explain-Top-2, respec-\n",
      "tively. The thresholds were 89.2% for Beer and 88.9% for Amzbook .\n",
      "We happened to explain 18 correctly predicted and 7 incorrectly\n",
      "predicted examples with Explain-Top-2 for both datasets (leaving\n",
      "1 incorrect and 24 correct cases with Explain-Top-1). While one\n",
      "might learn a better threshold from the data, we leave that to future\n",
      "work. As for expert-generated explanations, one author created\n",
      "expert explanations by selecting one short, convincing text phrase\n",
      "span for each class (positive or negative).4.2.2 LSAT.\n",
      "AI Model. We finetuned a RoBERTa model7on ReClor [ 82], a logic-\n",
      "reasoning dataset that contains questions from standardized exams\n",
      "like the LSAT and GMAT.8\n",
      "Task examples. We selected 20 examples from an LSAT prep\n",
      "book [ 71]. We verified that our questions were not easily searchable\n",
      "online and were not included in the training dataset. We selected\n",
      "fewer LSAT questions than for sentiment analysis, because they\n",
      "are more time consuming to answer and could fatigue participants:\n",
      "LSAT questions took around a minute to answer, compared to\n",
      "around 17 seconds for Beer and Amzbook . The RoBERTa model\n",
      "achieved 65% accuracy on these examples, comparable to the 67%\n",
      "human accuracy that we observed in our pilot study.\n",
      "7Based on the opensource implementation: https://github.com/yuweihao/reclor.\n",
      "8https://en.wikipedia.org/wiki/GraduateManagementAdmissionTestDoes the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "Explanations. We found no automated method that could gen-\n",
      "erate reasonable explanations (unsurprising, given that explana-\n",
      "tions rely on prior knowledge and complex reasoning); instead, we\n",
      "used expert explanations exclusively, which is again based on the\n",
      "prep book. The book contains explanations for the correct answer,\n",
      "which one author condensed to a maximum of two sentences. Since\n",
      "the book did not provide explanations for alternative choices, we\n",
      "created these by manually crafting a logical supporting argument\n",
      "for each choice that adhered to the tone and level of conciseness of\n",
      "the other explanations. Experts only generated explanations and\n",
      "did not determine the model predictions or its uncertainties.\n",
      "4.3 Study Procedure\n",
      "Sentiment Classification .For the final study, participants went\n",
      "through the following steps: 1) A landing page first explained the\n",
      "payment scheme; the classification task was presented (here, pre-\n",
      "dicting the sentiment of reviews); and they were shown dataset-\n",
      "specific examples. 2) To familiarize them with the task and verify\n",
      "their understanding, a screening phase required the participant\n",
      "to correctly label four of six reviews [ 51]. Only participants who\n",
      "passed the gating test were allowed to proceed to the main task. 3)\n",
      "The main task randomly assigned participants to one of our study\n",
      "conditions (Section 3.3) and presented condition-specific instruc-\n",
      "tions, including the meaning and positioning of AI’s prediction,\n",
      "confidence, and explanations. Participants then labeled all 50 study\n",
      "samples one-by-one. For a given dataset, all conditions used the\n",
      "same ordering of examples. The participants received immediate\n",
      "feedback on their correctness after each round of the task. 4) A\n",
      "post-task survey was administered, asking whether they found the\n",
      "model assistance to be helpful, their rating of the usefulness of\n",
      "explanations in particular (if they were present), and their strategy\n",
      "for using model assistance.\n",
      "We recruited participants from Amazon’s Mechanical Turk, lim-\n",
      "iting the pool to subjects from within the United States with a\n",
      "prior task approval rating of at least 97% and a minimum of 1,000\n",
      "approved tasks. To ensure data quality, we removed data from par-\n",
      "ticipants whose median labeling time was less than 2 seconds or\n",
      "those who assigned the same label to all examples. In total, we\n",
      "recruited 566 ( Beer) and 552 ( Amzbook ) crowd workers, and in both\n",
      "datasets, 84% of participants passed the screening and post-filtering.\n",
      "Eventually, we collected data from around 100 participants (ranging\n",
      "from 93 to 101 due to filtering) per condition.\n",
      "Study participants received a base pay of $0.50 for participating,\n",
      "a performance-based bonus for the main task, and a fixed bonus of\n",
      "$0.25 for completing the survey. Our performance-based bonus was\n",
      "a combination of linear and step functions on accuracy: we gave\n",
      "$0.05 for every correct decision in addition to an extra $0.50 if the to-\n",
      "tal accuracy exceeded 90% or $1.00 if it exceeded 95%. The assigned\n",
      "additional bonuses were intended to motivate workers to strive for\n",
      "performance in the complementary zone and improve over the AI-\n",
      "only performance [ 33]. Since we fixed the AI performance at 84%,\n",
      "humans could not obtain the bonus by blindly following the AI’s\n",
      "recommendations. Participants spent 13 minutes on average on the\n",
      "experiment and received an average payment of $3.35 (equivalent\n",
      "to an hourly wage of $15.77).Modifications for LSAT .For the LSAT dataset, we used a very\n",
      "similar procedure but used two screening questions and required\n",
      "workers to answer both correctly. We used a stricter passing re-\n",
      "quirement to avoid low-quality workers who might cheat, which\n",
      "we observed more for this task in our pilots. We again used MTurk\n",
      "with the same filters as sentiment classification, and we post hoc\n",
      "removed data from participants whose median response time was\n",
      "less than three seconds. 508 crowd workers participated in our\n",
      "study, 35% of whom passed the screening and completed the main\n",
      "task, resulting in a total of 100 participants per condition.\n",
      "Participants received a base pay of $0.50 for participating, a\n",
      "performance-based bonus of $0.30 for each correct answer in the\n",
      "main task, and a fixed bonus of $0.25 for completing an exit survey.\n",
      "They received an additional bonus of $1.00, $2.00, and $3.00 for\n",
      "reaching an overall accuracy of 30%, 50%, and 85% to motivate\n",
      "workers to answer more questions correctly and perform their best.\n",
      "The average completion time for the LSAT task was 16 minutes,\n",
      "with an average payment of $6.30 (equals an hourly wage of $23.34).\n",
      "5 RESULTS\n",
      "5.1 Effect of Explanation on Team performance\n",
      "Figure 4A shows the team performance ( i.e., accuracy of final deci-\n",
      "sion) for each domain and condition. We tested the significance of\n",
      "our results using Student’s T-tests with Bonferroni correction.\n",
      "The baseline team condition, Team (Conf), achieved com-\n",
      "plementary performance across tasks. ForBeer, providing AI\n",
      "recommendations and confidence to users increased their perfor-\n",
      "mance to ( 𝜇=0.89±𝜎=0.05), surpassing both AI ( 0.84) and\n",
      "unassisted human accuracy ( 0.82±0.09). Similarly, Team (Conf)\n",
      "achieved complementary performance for Amzbook andLSAT , with\n",
      "relative gains of 2.2% and 20.1% over unassisted workers (Figure 4A).\n",
      "We did not observe a significant difference between Explain-\n",
      "Top-1 and Explain-Top-2 , or that H1was not supported. For ex-\n",
      "ample, in Figure 4A of Beer, explaining the top prediction performed\n",
      "marginally better than explaining the top-two predictions, but the\n",
      "difference was not significant ( 𝑧=0.85, 𝑝=.40). The same was true\n",
      "forAmzbook (𝑧=0.81, 𝑝=.42) and LSAT (𝑧=0.42, 𝑝=.68).\n",
      "We did not observe significant improvements over the con-\n",
      "fidence baseline by displaying explanations. For example, for\n",
      "Beer, Team (Conf) and Team (Explain-Top-1, AI) achieved similar\n",
      "performance, with the accuracy being 0.89±0.05vs.0.88±0.06\n",
      "respectively; the difference was insignificant ( 𝑧=-1.18, 𝑝=.24). We\n",
      "observed the same pattern for Amzbook (𝑧=1.23, 𝑝=.22) and LSAT\n",
      "(𝑧=0.427, 𝑝=.64). As a result, we could not reject our hypothesis H2\n",
      "that Explain-Top-1 performs similar to simply showing confidence\n",
      ". This result motivates the need to develop new AI systems and ex-\n",
      "planation methods that provide true value to team performance by\n",
      "supplementing the model’s confidence, perhaps working in tandem\n",
      "with confidence scores.\n",
      "Though designed to alleviate the limitations of Explain-Top-1\n",
      "and Explain-Top-2 in our experiments, we did not observe im-\n",
      "provements from using Adaptive explanations. For example,\n",
      "we did not observe any significant differences between Team (Adap-\n",
      "tive, AI) and Team (Conf) for Beer (𝑧=-1.02, 𝑝=.31) or Amzbook\n",
      "(𝑧=1.08, 𝑝=.28). We did not observe significant differences betweenCHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "0.75 0.80 0.85 0.90 0.951.00\n",
      "DecisionAccuracyHuman\n",
      "Team (Conf)\n",
      "Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)conditionBeerDecisionAccuracy\n",
      "0.75 0.80 0.85 0.90 0.951.00\n",
      "DecisionAccuracyHuman\n",
      "Team (Conf)\n",
      "Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)conditionAmzbookDecisionAccuracy\n",
      "0.500.55 0.60 0.65 0.70 0.75\n",
      "DecisionAccuracyHuman\n",
      "Team (Conf)\n",
      "Team (ExplainTop-1,Exper t)\n",
      "Team (ExplainTop-2,Exper t)\n",
      "Team (Adapt ive,Expert)conditionLSATDecisionAccuracy0.40.5 0.6 0.7 0.8 0.91.0\n",
      "DecisionAccuracyHuman\n",
      "Team (Conf)\n",
      "Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)conditionBeerDecisionAccuracy(split)\n",
      "0.40.5 0.6 0.7 0.8 0.91.0\n",
      "DecisionAccuracyHuman\n",
      "Team (Conf)\n",
      "Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)conditionAmzbookDecisionAccuracy(split)\n",
      "0.30.4 0.5 0.6 0.7 0.80.9\n",
      "DecisionAccuracyHuman\n",
      "Team (Conf)\n",
      "Team (ExplainTop-1,Exper t)\n",
      "Team (ExplainTop-2,Exper t)\n",
      "Team (Adapt ive,Expert)conditionLSATDecisionAccuracy(split)correct incorrect AIrecomme ndation AI accuracy Complementary zoneA B\n",
      "Figure 4: Team performance (with average accuracy and 95% confidence interval) achieved by different explanation conditions\n",
      "and baselines for three datasets, with around 100 participants per condition. (A) Across every dataset, all team conditions\n",
      "achieved complementary performance. However, we did not observe significant improvements from using explanations over\n",
      "simply showing confidence scores. (B) Splitting the analysis based on the correctness of AI accuracy, we saw that for Beer and\n",
      "LSAT , Explain-Top-1 explanations worsened performance when the AI was incorrect, the impact of Explain-Top-1 and Explain-\n",
      "Top-2 explanations were correlated with the correctness of the AI’s recommendation, and Adaptive explanations seemed to\n",
      "have the potential to improve Explain-Top-1 when the AI was incorrect, and to retain the higher performance of Explain-Top-1\n",
      "when the AI was correct.\n",
      "Team (Adaptive, Expert) and Team (Conf) for LSAT (𝑧=0.16, 𝑝=.87).\n",
      "More surprisingly, switching the source of Adaptive explanation to\n",
      "expert-generated did not significantly improve sentiment analysis\n",
      "results. For example, in Figure 4A, the differences in performance\n",
      "between Team (Adaptive, Expert) and Team (Adaptive, AI) were\n",
      "insignificant: Beer (𝑧=1.31, 𝑝=.19) and Amzbook (𝑧=-0.78, 𝑝=.43). As\n",
      "such, we could not reject the null hypotheses for either H3orH4.\n",
      "While Adaptive explanation did not significantly improve\n",
      "team performance across domains, further analysis may point\n",
      "a way forward by combining the strengths of Explain-Top-1\n",
      "and Explain-Top-2. Split the team performance by whether the\n",
      "AI made a mistake (Figure 4B), we observe that explaining the top\n",
      "prediction lead to better accuracy when the AI recommendation\n",
      "was correct but worse when the AI was incorrect, as in our pilot\n",
      "study. This is consistent with Psychology literature [ 39], which has\n",
      "shown that human explanations cause listeners to agree even when\n",
      "the explanation is wrong, and recent studies that showed explana-\n",
      "tions can mislead data scientists into overtrusting ML models for\n",
      "deployment [ 38]. While these results were obtained by measuringuser’s subjective ratings of trust, to the best of our knowledge, our\n",
      "studies are the first to show this phenomenon for explanation and\n",
      "end-to-end decision making with large-scale studies. As expected,\n",
      "inBeer, Adaptive explanations improved performance over Explain-\n",
      "Top-1 when the AI was incorrect and improved performance over\n",
      "Explain-Top-2 when the AI was correct, although the effect was\n",
      "smaller on other datasets.\n",
      "While Figure 4B shows team performance, the promising effects\n",
      "of Adaptive explanations are clearer if we study the agreement\n",
      "between AI predictions and human decisions (Figure 5). Adaptive\n",
      "explanations seem to encourage participants to consider the AI\n",
      "more when it is confident and solve the task themselves otherwise.\n",
      "Unfortunately, as our experiments show, the effect of using Adap-\n",
      "tive did not seem sufficient to increase the final team accuracy,\n",
      "possibly for two reasons: (1) in high confidence regions (circles in\n",
      "Figure 5), not only did workers have to agree more, but they also had\n",
      "to identify cases where the model failed with very high confidence\n",
      "(unknown unknowns [ 44]). Identifying unknown unknowns could\n",
      "have been a difficult and time-consuming task for workers, and theyDoes the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      ">= threshold<   thresholdPass adaptive threshold?Explain-Top-2Explain-Top-1Display\n",
      "0.6 0.7 0.8 0.9 1.0Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)condition\n",
      "0.6 0.7 0.8 0.9 1.0Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)condition\n",
      "0.5 0.6 0.7 0.8 0.9\n",
      "Human-AIAgreementTeam (ExplainTop-1,Expert)\n",
      "Team (ExplainTop-2,Expert)\n",
      "Team (Adapt ive,Expert)conditionBeer\n",
      "Amzbook\n",
      "LSAT\n",
      "--\n",
      "Figure 5: Relative agreement rates between humans and AI ( i.e., does the final human decision match the AI’s suggestion?) for\n",
      "various conditions, with examples split by whether AI’s confidence exceeded the threshold used for Adaptive explanations.\n",
      "Across the three datasets, Adaptive explanations successfully reduced the human’s tendency to blindly trust the AI ( i.e., de-\n",
      "creased agreement) when it was uncertain and more likely to be incorrect. For example, comparing Team (Explain-Top-1, AI)\n",
      "and Team (Adaptive, AI) on low confidence examples that did not pass the threshold (rectangles), participants in Explain-Top-2\n",
      "(pink rectangles) were less likely to agree with the AI compared to those who saw Explain-Top-1 (blue rectangles).\n",
      "0.000.20 0.40 0.60 0.801.00\n",
      "Humanaccuracy(Binned)0%20%40%%ExamplesBeer\n",
      "0.000.20 0.40 0.60 0.801.00\n",
      "Humanaccuracy(Binned)0%20%40%60%%ExamplesAmzbook\n",
      "0.000.20 0.40 0.60 0.801.00\n",
      "Humanaccuracy(Binned)0%5%10%15%20%%ExamplesLSATcorrect incorrect AIRecomme ndation\n",
      "Figure 6: The distribution of study examples as a function of average human accuracy. For each domain, examples on the right\n",
      "were easy for most humans working alone. Both Beer andLSAT show a distribution that shows potential for complementary\n",
      "team performance: humans can correct easy questions mistaken by the AI (red bars towards the right), and, conversely, the\n",
      "AI may add value on examples where humans frequently err (green bars towards the left). In contrast, Amzbook showed less\n",
      "potential for this kind of human-AI synergy, with less “easy for human” questions (bars towards the left).\n",
      "may have needed other types of support that we did not provide.\n",
      "(2) In low confidence regions (rectangles), not only did workers\n",
      "have to disagree more, but they also had to be able to solve the task\n",
      "correctly when they disagreed. Explain-Top-2 explanations might\n",
      "have enabled them to suspect the model more, but it is unclear if\n",
      "they helped participants make the right decisions. This indicates\n",
      "that more sophisticated strategies are needed to support humans in\n",
      "both situations. We discuss some potential strategies in Section 6.3.\n",
      "Differences in expertise between human and AI affects\n",
      "whether (or how much) AI assistance will help achieve com-\n",
      "plementary performance. To understand how differences in ex-\n",
      "pertise between the human and AI impact team performance, wecomputed the average accuracy of unassisted users on study exam-\n",
      "ples and overlaid the AI’s expertise (whether the recommendation\n",
      "was correct) in Figure 6. The figure helps explain why users ben-\n",
      "efited more from AI recommendations for both Beer and LSAT\n",
      "datasets. There was a significant fraction of examples that the AI\n",
      "predicted correctly but humans struggled with (green bars to the\n",
      "left), while the same was not true for Amzbook (where AI rec-\n",
      "ommendations did not help as much). Further, when the AI was\n",
      "incorrect, explaining predictions on Amzbook via Explain-Top-1\n",
      "improved the performance by 5% over showing confidence (Fig-\n",
      "ure 4B), but it decreased the performance for Beer andLSAT . OneCHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "Team (Conf)\n",
      "Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)ConditionBeer:AIAssistantisUseful Beer:Explanat ionisUseful\n",
      "-50% 0% 50%\n",
      "%Participants-50% 0% 50%\n",
      "%Participants\n",
      "Team (Conf)\n",
      "Team (Explain-Top-1,AI)\n",
      "Team (Explain-Top-2,AI)\n",
      "Team (Adapt ive,AI)\n",
      "Team (Adapt ive,Expert)ConditionAmzbook: AIAssistantisUseful Amzbook:Explanat ionisUseful\n",
      "-50% 0% 50%\n",
      "%Participants-50% 0% 50%\n",
      "%Participants\n",
      "Team (Conf)\n",
      "Team (ExplainTop-1,Exper t)\n",
      "Team (ExplainTop-2,Exper t)\n",
      "Team (Adapt ive,Expert)ConditionLSAT:AIAssistantisUseful LSAT:Explanat ionisUseful\n",
      "-50% 0% 50%\n",
      "%Participants-50% 0% 50%\n",
      "%ParticipantsStrongl ydisagr ee Disa gree Neitheragreenordisagree Agree Strongl yagree Response\n",
      "B A\n",
      "Figure 7: Analysis of participant responses to two statements: (A) “AI’s assistance ( e.g., the information it displayed) helped me\n",
      "solve the task”, and (B) “AI’s explanations in particular helped me solve the task.” Across datasets, a majority of participants\n",
      "found AI assistant to be useful, and they rated all the conditions similarly, with a slight preference towards Team (Adaptive,\n",
      "Expert). In contrast to AI’s overall usefulness, fewer participants rated explanations as useful, particularly Explain-Top-2\n",
      "explanations. Participants also had a clearer preference for higher-quality (expert) Adaptive explanations.\n",
      "possible explanation is that most AI mistakes were predicted cor-\n",
      "rectly by most humans on Amzbook (red bars were mostly towards\n",
      "the right). After observing clear model mistakes, participants may\n",
      "have learned to rely on them less, despite the convincing-effect of\n",
      "explanation. Participants’ self-reported collaboration approaches\n",
      "supported our guess – Amzbook participants reportedly ignored the\n",
      "AI’s assistance the most (Section 5.3). That said, other confounding\n",
      "effects such as the nature of the task ( e.g., binary classification vs.\n",
      "choosing between multiple options) should also be studied.\n",
      "5.2 Survey Responses on Likert Scale Questions\n",
      "Two of the questions in our post-task survey requested categorical\n",
      "ratings of AI and explanation usefulness.9\n",
      "AI usefulness : While participants generally rated AI assistance\n",
      "useful (Figure 7A), the improvements in ratings between most ex-\n",
      "planations and simply showing confidence were marginal. The\n",
      "difference was more clear for high-quality adaptive explanations;\n",
      "forBeer, 70% of the participants rated AI assistance useful with\n",
      "Team (Adaptive, Expert) in contrast to 57% with Team (Conf). We\n",
      "observed a similar pattern on Amzbook (65% vs. 49%) and LSAT (63%\n",
      "vs. 45%), though on LSAT , Team (Explain-Top-2, Expert) received\n",
      "slightly higher ratings than Team (Adaptive, Expert) (66% vs. 63%).\n",
      "9Since we did not pre-register hypotheses for the subjective ratings and only analyzed\n",
      "them post-hoc, we do not perform/claim statistical significant analysis on these metrics.Explanation usefulness : Figure 7B shows that participants’\n",
      "ratings for the usefulness of explanations were lower than the\n",
      "overall usefulness of AI’s assistance (in A). Again, expert-generated\n",
      "Adaptive explanations received higher ratings than AI-generated\n",
      "ones for Beer (53% vs. 38%) vs. Amzbook (50% vs. 40%). This could\n",
      "indicate that showing higher quality explanations improves users’\n",
      "perceived helpfulness of the system. However, it is worth noting\n",
      "that this increased preference did not translate to an improvement in\n",
      "team performance, which is consistent with observations made by\n",
      "Buçinca et al. [12] that show that people may prefer one explanation\n",
      "but make better decisions with another.\n",
      "5.3 Qualitative Analysis on Collaboration\n",
      "To better understand how users collaborated with the AI in different\n",
      "tasks, we coded their response to the prompt: “Describe how you\n",
      "used the information Marvin (the AI) provided.” Two annotators\n",
      "independently read a subset of the responses to identify emergent\n",
      "codes and, using a discussion period, created a codebook (Table 3).\n",
      "Using this codebook, for each team condition and dataset, they\n",
      "coded a sample of 30 random worker responses: 28 were unique\n",
      "and 2 overlapped between annotators, allowing us to compute inter-\n",
      "annotator agreement. Our final analysis used 409 unique responses\n",
      "after removing 11 responses deemed to be of poor quality (Table 3).\n",
      "We scored the inter-annotator agreement with both the Cohen’s 𝜅Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "Codes Definitions and Examples #Participants\n",
      "Overall Collaboration Approach (codes are mutually exclusive)\n",
      "Mostly Follow AI The participant mostly followed the AI.\n",
      "“I went with Marvin most times. ”23 (6%)\n",
      "AI as Prior Guide Used AI as a starting reference point.\n",
      "“I looked at his prediction and then I read the passage. ”190 (47%)\n",
      "AI as Post Check Double-checked after they made their own decisions.\n",
      "“I ignored it until I made my decision and then verified what it said. ”102 (25%)\n",
      "Mostly Ignore AI Mostly made their own decisions without the AI.\n",
      "“I didn’t. I figured out the paragraph for myself. ”90 (22%)\n",
      "The Usage of Explanation (codes can overlap)\n",
      "Used Expl. Explicitly acknowledged they used the explanation.\n",
      "“I skimmed his highlighted words. ”138 (42%)\n",
      "Speed Read Used explanations to quickly skim through the example.\n",
      "“I looked at Marvin’s review initially then speed read the review. ”29 (9%)\n",
      "Validate AI Used the explanation to validate AI’s reasoning.\n",
      "“Marvin focuses on the wrong points at times. This made me\n",
      "cautious when taking Marvin’s advice. ”17 (5%)\n",
      "The Usage of Confidence (codes can overlap)\n",
      "Used Conf. Explicitly acknowledged they used the confidence.\n",
      "“I mostly relied on Marvin’s confident levels to guide me. ”90 (22%)\n",
      "Conf. Threshold Was more likely to accept AI above the threshold.\n",
      "“If Marvin was above 85% confidence, I took his word for it. ”24 (6%)\n",
      "Others (codes can overlap)\n",
      "Fall Back to AI Followed the AI’s label if they failed to decide.\n",
      "“I used it if I was unsure of my own decision. ”54 (13%)\n",
      "Updated Strategy Changed their strategy as they interacted more.\n",
      "“I decided myself after seeing that sometimes Marvin failed me. ”12 (2%)\n",
      "Table 3: The codebook for participants’ descriptions of how they used the AI, with the number of self-reports.\n",
      "and the raw overlap between the coding. We achieved reasonably\n",
      "high agreements, with an average 𝜇(𝜅)=0.71, 𝜎(𝜅)=0.18(the\n",
      "average agreement was 93%±6.5%). We noticed the following,\n",
      "which echo the performance differences observed across datasets:\n",
      "Most participants used the AI’s recommendation as a prior\n",
      "or to double-check their answers. For all datasets, more than\n",
      "70% of the participants mentioned they would partially take AI’s\n",
      "recommendation into consideration rather than blindly following\n",
      "AI or fully ignoring it (Figure 8). Participants used the AI as a prior\n",
      "guide more than as a post-check for sentiment analysis, but not\n",
      "forLSAT , which aligns with our interface design: for LSAT , AI\n",
      "recommendations were on a separate pane, encouraging users to\n",
      "solve the task on their own before consulting the AI.\n",
      "Participants ignored the AI more on domains where AI\n",
      "expertise did not supplement their expertise. Figure 8 shows\n",
      "that while only 11% of LSAT participants claimed that they mostly\n",
      "ignored the AI, the ratio doubled ( Beer, 23%) or even tripled ( Amz-\n",
      "book , 30%) for sentiment analysis. As discussed in Figure 6, this\n",
      "may be due to correlation differences between human and AI errors\n",
      "for different datasets: Amzbook participants were less likely to see\n",
      "cases where AI was more correct than they were, and therefore they\n",
      "may have learned to rely less on it. For example, one participant in\n",
      "Amzbook mentioned, “I had initially tried to take Marvin’s adviceinto account for a few rounds, and stopped after I got 2 incorrect\n",
      "answers. After that I read all of the reviews carefully and followed\n",
      "my own discretion. ”\n",
      "In contrast, a Beer participant relied more on the AI once realizing\n",
      "it could be correct: “At first I tried reading the passages and making\n",
      "my own judgments, but then I got several items wrong. After that, I\n",
      "just switched to going with Marvin’s recommendation every time. ”\n",
      "In addition to the user’s collaboration behavior, these differ-\n",
      "ences between domains may have affected our quantitative ob-\n",
      "servations of team performance. For example, a small difference\n",
      "between human and AI expertise (distribution of errors) means\n",
      "that the improvement in performance when the AI is correct would\n",
      "be less substantial. In fact, in Figure 4B, if we compare the team\n",
      "performance when the AI is correct, the difference between team\n",
      "conditions and the human baseline is least substantial for Amzbook .\n",
      "Some participants developed mental models of the AI’s\n",
      "confidence score to determine when to trust the AI. Among\n",
      "participants who mentioned they used confidence scores (90 in\n",
      "total), 27% reported using an explicit confidence threshold, below\n",
      "which they were likely to distrust the AI. The threshold mostly\n",
      "varied between 80 to 100 ( 83±8forBeer,89±7forAmzbook , and\n",
      "90±0forLSAT ) but could go as low as 65, indicating that users\n",
      "built different mental models about when they considered AI toCHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "MostlyFollowAI\n",
      "AIasPriorGuide\n",
      "AIasPostCheck\n",
      "MostlyIgnoreAICollaborationBeer Amzbook LSAT\n",
      "0%10% 20% 30% 40% 50%\n",
      "%Participants0%10% 20% 30% 40% 50%\n",
      "%Participants0%10% 20% 30% 40% 50%\n",
      "%Participants\n",
      "Figure 8: Instead of ignoring or strictly following the AI, participants reported taking the AI information into consideration\n",
      "most of the time. They most frequently used AI as a prior guide in sentiment analysis, but used it as post-check in LSAT . They\n",
      "were also more likely to ignore the AI in sentiment analysis than in LSAT .\n",
      "Conf.\n",
      "Conf.+ Expl.conditionAmzbook Beer LSAT\n",
      "0% 5% 10%\n",
      "%Participants0% 5% 10% 15%\n",
      "%Participants0% 2% 4% 6%\n",
      "%ParticipantsConf.\n",
      "Conf.+ Expl.condition\n",
      "Figure 9: Comparing the occurrence of Used Conf. in just the confidence condition and in those with explanations, we saw a\n",
      "similar proportion of users that explicitly acknowledged using confidence, regardless of whether they saw an explanation.\n",
      "be “trustworthy.” While this observation empirically shows that\n",
      "end-users develop mental model of trust in AI-assisted decision\n",
      "making [ 5], it more importantly shows how the AI’s confidence is\n",
      "a simple, yet salient feature via which users create a mental model\n",
      "of the AI’s global behavior [ 23]. Note that across all three domains,\n",
      "the same proportion of participants self-reported using AI’s confi-\n",
      "dence scores regardless of whether they saw explanations (Figure 9).\n",
      "Furthermore, some participants consigned the task to AI when\n",
      "they were themselves uncertain. For example, 13% participants\n",
      "mentioned that they would go with the AI’s decision if they were on\n",
      "the fence by themselves: “There were some that I could go either way\n",
      "on, and I went with what Marvin suggested. ” These user behaviors are\n",
      "similar to observations in psychology literature on Truth-Default\n",
      "Theory [47], which shows that people exhibit truth-default behavior:\n",
      "by default, people are biased to assume that the speaker is being\n",
      "truthful, especially when triggers that raise suspicion are absent.\n",
      "Furthermore, our participants’ distrust in low-confidence recom-\n",
      "mendations is also consistent with examples of triggers that cause\n",
      "people to abandon the truth-default behavior.\n",
      "Explanations can help participants validate the AI’s deci-\n",
      "sions, and the inline-highlight format helped participants\n",
      "speed up their decision making. Among the participants who\n",
      "explicitly mentioned using explanations, 27% in Beer and 32% in\n",
      "Amzbook reported that they used them to read the review text faster.\n",
      "Since LSAT explanations required reading additional text, we did\n",
      "not expect LSAT users to find this benefit. Interestingly, for Beer\n",
      "and Amzbook , while a small percentage of users (17%) reported\n",
      "using the explanations to validate the AI’s decisions (see Figure 3),\n",
      "only 2% did so in LSAT . This could be because LSAT is a harder task\n",
      "than sentiment analysis, and verifying AI’s explanations is costlier.\n",
      "Other participants mostly mentioned that they would supplement\n",
      "their own reasoning with the AI’s: “I read the Marvin rationale and\n",
      "weighed it against my intuition and understanding. ”6 DISCUSSION & FUTURE DIRECTIONS\n",
      "Though conducted in a limited scope, our findings should help guide\n",
      "future work on explanations and other mechanisms for improving\n",
      "decision making with human-AI teams.\n",
      "6.1 Limitations\n",
      "As mentioned in Section 2, AI explanations have other motivations\n",
      "not addressed by this paper. Our work, as well as the papers listed\n",
      "in Table 1, evaluated team performance along one dimension: accu-\n",
      "racy of decisions. We did not explore the benefits on other metrics\n",
      "(e.g.increasing speed as reported by some users in Section 5.2), but\n",
      "in general, one may wish to achieve complementary performance\n",
      "on a multi-dimensional metric. In fact, research shows that large\n",
      "collaborative communities like Wikipedia require AI systems that\n",
      "balance multiple aspects, e.g., reducing human effort, improving\n",
      "trust and positive engagement [ 68]. We encourage future research\n",
      "to extend the definition of complementarity, and to evaluate the\n",
      "impact of explanations on those dimensions accordingly.\n",
      "Further, we restricted ourselves to tasks amenable to crowdsourc-\n",
      "ing (text classification and question answering), so our results may\n",
      "not generalize to high-stakes domains with expert users such as\n",
      "medical diagnosis. We also note that the effectiveness of expla-\n",
      "nations may depend on user expertise, a factor that we did not\n",
      "explore. Investigating this in our framework would either require\n",
      "recruiting lay and expert users for the same task [ 21] or utilizing a\n",
      "within-subject experimental design to measure user expertise.\n",
      "Finally, we only explored two possible ways to present expla-\n",
      "nations (highlighting keywords and natural language arguments).\n",
      "While these methods are widely adopted [ 24,43,49,75], alternative\n",
      "approaches may provide more benefit to team performance.\n",
      "6.2 Explaining AI for Appropriate Reliance\n",
      "One concerning observation was that explanations increased blind\n",
      "trust rather than appropriate reliance on AI. This is problematic\n",
      "especially in domains where humans are required in the loop forDoes the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "moral or legal reasons ( e.g., medical diagnosis) and suppose the\n",
      "presence of explanations simply soothes the experts ( e.g., doctors),\n",
      "making them more compliant so they blindly (or become more\n",
      "likely to) agree with the computer. Encouraging human-AI interac-\n",
      "tions like these seems deeply unsatisfactory and ethically fraught.\n",
      "Importantly, while prior works also observed instances of inap-\n",
      "propriate reliance on AI [ 18,38,58,74], our studies quantified its\n",
      "effect on team performance. Since the nature of the proxy tasks can\n",
      "significantly change the human behavior, they can lead to potential\n",
      "misleading conclusions [ 12]. The emphasis of the complementary\n",
      "team performance in end-to-end tasks can objectively evaluate the\n",
      "extent of such issues or about the effectiveness of a solution.\n",
      "Our Adaptive Explanation aims to encourage the human to think\n",
      "more carefully when the system had a low confidence. While the\n",
      "relative agreement rates showed that the Explain-Top-2 explanation\n",
      "might cue the humans to suspect the model’s veracity (Figure 5),\n",
      "the method was not sufficient to significantly increase the final\n",
      "team accuracy (Figure 4). This is perhaps because users still have\n",
      "to identify high-confidence mistakes (unknown-unknowns) and\n",
      "solve the task when the AI is uncertain (Section 5.1). A followup\n",
      "question is, then, what kind of interactions would help humans\n",
      "perform correctly when the AI is incorrect?\n",
      "Explanations should be informative, instead of just convincing.\n",
      "Our current expert explanations did not help any more than the AI\n",
      "explanations, which may indicate that having the ML produce the\n",
      "maximally convincing explanation — a common objective shared in\n",
      "the design of many AI explanation algorithms — might be a poor\n",
      "choice for complementary performance [ 12]. A more ideal goal is\n",
      "explanations that accurately inform the user – such that the user\n",
      "can quickly gauge through the explanation when the AI’s reasoning\n",
      "is correct and when it should raise suspicion. A successful example\n",
      "of this was seen with Generalized Additive Models (GAMs) for\n",
      "healthcare, where its global explanations helped medical experts\n",
      "suspect that the model had learned incorrect, spurious correlations\n",
      "(e.g.a history of asthma reduces the risk of dying from pneumo-\n",
      "nia [ 15]). We hope future research can produce explanations that\n",
      "better enable the human to effectively catch AI’s mistakes, rather\n",
      "than finding plausible justifications when it erred.\n",
      "High complementary performance may require adapting beyond\n",
      "confidence. Since approaches based on confidence scores make it\n",
      "difficult to spot unknown-unknowns, instead it may be worthwhile\n",
      "to design explanation strategies that adapt based on the frequency\n",
      "of agreement between the human and AI. For example, instead of\n",
      "explaining why it believes an answer to be true, the AI might play\n",
      "a devil’s advocate role, explaining its doubts — even when it agrees\n",
      "with the human. The doubts can even be expressed in an interac-\n",
      "tive fashion (as a back and forth conversation) than a set of static\n",
      "justifications, so to avoid cognitive overload. For example, even if\n",
      "the system agrees with the user, the system can present a high-level\n",
      "summary of evidence for top-K alternatives and let the user drill\n",
      "down, i.e., ask the system for more detailed evidence for the subset\n",
      "of alternatives that they now think are worth investigating.6.3 Rethinking AI’s Role in Human-AI Teams\n",
      "Comparable accuracy does not guarantee complementary part-\n",
      "ners. Rather, in an ideal team, the human and AI would have mini-\n",
      "mally overlapping mistakes so that there is a greater opportunity\n",
      "to correct each other’s mistakes. In one of our experiment domains\n",
      "(Amzbook ), AI errors correlated much more strongly with humans’\n",
      "than in others, and thus we saw relatively smaller gains in per-\n",
      "formance from AI assistance (Figure 6). As recent work has sug-\n",
      "gested [ 4,55,59,79], it may be useful to directly optimize for com-\n",
      "plementary behavior by accounting for the human behavior during\n",
      "training, who may have access to a different set of features [72].\n",
      "Furthermore, the human and AI could maximize their talents in\n",
      "different dimensions. For example, for grading exams, AI could use\n",
      "its computation power to quickly gather statistics and highlight\n",
      "commonly missed corner cases, whereas the human teacher could\n",
      "focus on ranking the intelligence of the student proposed algo-\n",
      "rithms [ 26]. Similarly, to maximize human performance at Quiz\n",
      "Bowl, Feng and Graber [ 21] designed interaction so that the AI\n",
      "memorized and quickly retrieved documents relevant to a ques-\n",
      "tion, a talent which humans lacked because of cognitive limitations;\n",
      "however, they left the task of combining found evidence and logical\n",
      "reasoning to human partners. Future research should explore other\n",
      "ways to increase synergy.\n",
      "The timing of AI recommendations is important. Besides the types\n",
      "of explanations, it is also important to carefully design when the\n",
      "AI provides its viewpoint. All of our methods used a workflow\n",
      "that showed the AI’s prediction (and its explanation) to the human,\n",
      "before they attempted to solve the problem on their own. However,\n",
      "by presenting an answer and accompanying justification upfront,\n",
      "and perhaps overlaid right onto the instance, our design makes it\n",
      "almost impossible for the human to reason independently, ignoring\n",
      "the AI’s opinion while considering the task. This approach risks\n",
      "invoking the anchor effect, studied in Psychology [ 20] and intro-\n",
      "duced to the AI explanation field by Wang et al. [ 74] — people rely\n",
      "heavily on the first information that is presented by others when\n",
      "making decisions. This effect was reflected in an increase in the\n",
      "use of the “ AI as Prior Guide ” collaboration approach in the\n",
      "sentiment analysis domain, compared to LSAT (Figure 8).\n",
      "Alternate approaches that present AI recommendations in an\n",
      "asynchronous fashion might increase independence and improve\n",
      "accuracy. For example, pairing humans with slower AIs (that wait\n",
      "or take more time to make recommendation) may provide humans\n",
      "with a better chance to reflect on their own decisions [ 63]. Methods\n",
      "that embody recommendations from management science for avoid-\n",
      "ing group-think [ 54] might also be effective, e.g., showing the AI’s\n",
      "prediction after the human’s initial answer or only having the AI\n",
      "present an explanation if it disagreed with the human’s choice. We\n",
      "note that these approaches correspond to the Update and Feedback\n",
      "methods of Green & Chen [ 27], which were effective, albeit not in\n",
      "the complementary zone. Another approach is to limit the AI’s capa-\n",
      "bilities. For example, one might design the AI to summarize the best\n",
      "evidence for all possible options, without giving hard predictions,\n",
      "by training evidence agents [64]. However, by delaying display of\n",
      "the AI’s recommendation until after the human has solved the task\n",
      "independently or restricting to only per class evidences, one mayCHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "preclude improvement to the speed of problem solving, which often\n",
      "correlates to the cost of performing the task.\n",
      "As a result, there is a strong tension between the competing\n",
      "objectives of speed, accuracy, and independence; We encourage\n",
      "the field to design and conduct experiments and explore different\n",
      "architectures for balancing these factors.\n",
      "7 CONCLUSIONS\n",
      "Previous work has shown that the accuracy of a human-AI team\n",
      "can be improved when the AI explains its suggestions, but these\n",
      "results are only obtained in situations where the AI, operating in-\n",
      "dependently, is better than either the human or the best human-AI\n",
      "team. We ask if AI explanations help achieve complementary team\n",
      "performance, i.e.whether the team is more accurate than either the\n",
      "AI or human acting independently. We conducted large-scale exper-\n",
      "iments with more than 1,500 participants. Importantly, we selected\n",
      "our study questions to ensure that our AI systems had accuracy\n",
      "comparable to humans and increased the opportunity for seeing\n",
      "complementary performance. While all human-AI teams showed\n",
      "complementarity, none of the explanation conditions produced an\n",
      "accuracy significantly higher than the simple baseline of showing\n",
      "the AI’s confidence — in contrast to prior work. Explanations in-\n",
      "creased team performance when the system was correct, but they\n",
      "decreased the accuracy on examples when the system was wrong,\n",
      "making the net improvement minimal.\n",
      "By highlighting critical challenges, we hope this paper will serve\n",
      "as a “Call to action” for the HCI and AI communities: and AI commu-\n",
      "nities. In future work, characterize when human-AI collaboration\n",
      "can be beneficial ( i.e., when both parties complement each other),\n",
      "developing explanation approaches and coordination strategies that\n",
      "result in a complementary team performance that exceeds what can\n",
      "be produced by simply showing AI’s confidence, and communicate\n",
      "explanations to increase understanding rather than just to persuade.\n",
      "At the highest level, we hope researchers can develop new interac-\n",
      "tion methods that increase complementary performance beyond\n",
      "having an AI telegraph its confidence.\n",
      "ACKNOWLEDGMENTS\n",
      "This material is based upon work supported by ONR grant N00014-\n",
      "18-1-2193, NSF RAPID grant 2040196, the University of Washington\n",
      "WRF/Cable Professorship, and the Allen Institute for Artificial In-\n",
      "telligence (AI2), and Microsoft Research. The authors thank Umang\n",
      "Bhatt, Jim Chen, Elena Glassman, Walter Lasecki, Qisheng Li, Eu-\n",
      "nice Jun, Sandy Kaplan, Younghoon Kim, Galen Weld, Amy Zhang,\n",
      "and anonymous reviewers for helpful discussions and comments.\n",
      "REFERENCES\n",
      "[1]Gregory D. Abowd, Anind K. Dey, Peter J. Brown, Nigel Davies, Mark Smith, and\n",
      "Pete Steggles. 1999. Towards a Better Understanding of Context and Context-\n",
      "Awareness. In Proceedings of the 1st International Symposium on Handheld and\n",
      "Ubiquitous Computing (Karlsruhe, Germany) (HUC ’99) . Springer-Verlag, Berlin,\n",
      "Heidelberg, 304–307.\n",
      "[2]Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine bias:\n",
      "There’s software across the country to predict future criminals and it’s biased\n",
      "against blacks.\n",
      "[3]Oisin Mac Aodha, Shihan Su, Yuxin Chen, Pietro Perona, and Yisong Yue. 2018.\n",
      "Teaching Categories to Human Learners With Visual Explanations. In 2018 IEEE\n",
      "Conference on Computer Vision and Pattern Recognition, CVPR 2018 . IEEE Com-\n",
      "puter Society, Salt Lake City, UT, USA, 3820–3828. https://doi.org/10.1109/CVPR.\n",
      "2018.00402[4]Gagan Bansal, Besmira Nushi, Ece Kamar, Eric Horvitz, and Daniel S. Weld. 2020.\n",
      "Optimizing AI for Teamwork. arXiv:2004.13102 [cs.AI]\n",
      "[5]Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S. Lasecki, Daniel S. Weld, and\n",
      "Eric Horvitz. 2019. Beyond Accuracy: The Role of Mental Models in Human-AI\n",
      "Team Performance. Proceedings of the AAAI Conference on Human Computation\n",
      "and Crowdsourcing 7, 1 (Oct. 2019), 2–11. https://ojs.aaai.org/index.php/HCOMP/\n",
      "article/view/5285\n",
      "[6]Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel S. Weld, Walter S. Lasecki, and\n",
      "Eric Horvitz. 2019. Updates in Human-AI Teams: Understanding and Addressing\n",
      "the Performance/Compatibility Tradeoff. Proceedings of the AAAI Conference on\n",
      "Artificial Intelligence 33, 01 (Jul. 2019), 2429–2437. https://doi.org/10.1609/aaai.\n",
      "v33i01.33012429\n",
      "[7]Richard E Barlow and Hugh D Brunk. 1972. The isotonic regression problem and\n",
      "its dual. J. Amer. Statist. Assoc. 67, 337 (1972), 140–147.\n",
      "[8]Mohsen Bayati, Mark Braverman, Michael Gillam, Karen M. Mack, George Ruiz,\n",
      "Mark S. Smith, and Eric Horvitz. 2014. Data-Driven Decisions for Reducing\n",
      "Readmissions for Heart Failure: General Methodology and Case Study. PLOS\n",
      "ONE 9, 10 (10 2014), 1–9. https://doi.org/10.1371/journal.pone.0109264\n",
      "[9]Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yun-\n",
      "han Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura, and Peter Eckersley.\n",
      "2020. Explainable Machine Learning in Deployment. In Proceedings of the\n",
      "2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain)\n",
      "(FAT* ’20) . Association for Computing Machinery, New York, NY, USA, 648–657.\n",
      "https://doi.org/10.1145/3351095.3375624\n",
      "[10] Mustafa Bilgic. 2005. Explaining Recommendations: Satisfaction vs. Promotion. ,\n",
      "13–18 pages.\n",
      "[11] Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and\n",
      "Nigel Shadbolt. 2018. ’It’s Reducing a Human Being to a Percentage’: Per-\n",
      "ceptions of Justice in Algorithmic Decisions. In Proceedings of the 2018 CHI\n",
      "Conference on Human Factors in Computing Systems (Montreal QC, Canada)\n",
      "(CHI ’18) . Association for Computing Machinery, New York, NY, USA, 1–14.\n",
      "https://doi.org/10.1145/3173574.3173951\n",
      "[12] Zana Buçinca, Phoebe Lin, Krzysztof Z. Gajos, and Elena L. Glassman. 2020. Proxy\n",
      "Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI\n",
      "Systems. In Proceedings of the 25th International Conference on Intelligent User\n",
      "Interfaces (Cagliari, Italy) (IUI ’20) . Association for Computing Machinery, New\n",
      "York, NY, USA, 454–464. https://doi.org/10.1145/3377325.3377498\n",
      "[13] Adrian Bussone, Simone Stumpf, and Dympna O’Sullivan. 2015. The role of\n",
      "explanations on trust and reliance in clinical decision support systems. In 2015\n",
      "International Conference on Healthcare Informatics . IEEE, IEEE, Dallas, Texas,\n",
      "160–169.\n",
      "[14] Carrie J Cai, Samantha Winter, David Steiner, Lauren Wilcox, and Michael Terry.\n",
      "2019. \" Hello AI\": Uncovering the Onboarding Needs of Medical Practitioners for\n",
      "Human-AI Collaborative Decision-Making. Proceedings of the ACM on Human-\n",
      "Computer Interaction 3, CSCW (2019), 1–24.\n",
      "[15] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie\n",
      "Elhadad. 2015. Intelligible Models for HealthCare: Predicting Pneumonia Risk\n",
      "and Hospital 30-Day Readmission. In Proceedings of the 21th ACM SIGKDD In-\n",
      "ternational Conference on Knowledge Discovery and Data Mining (Sydney, NSW,\n",
      "Australia) (KDD ’15) . Association for Computing Machinery, New York, NY, USA,\n",
      "1721–1730. https://doi.org/10.1145/2783258.2788613\n",
      "[16] Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay,\n",
      "and Devi Parikh. 2018. Do explanations make VQA models more predictable\n",
      "to a human?. In Proceedings of the 2018 Conference on Empirical Methods in\n",
      "Natural Language Processing . Association for Computational Linguistics, Brussels,\n",
      "Belgium, 1036–1042. https://doi.org/10.18653/v1/D18-1128\n",
      "[17] Elizabeth Clark, Anne Spencer Ross, Chenhao Tan, Yangfeng Ji, and Noah A.\n",
      "Smith. 2018. Creative Writing with a Machine in the Loop: Case Studies on\n",
      "Slogans and Stories. In 23rd International Conference on Intelligent User Interfaces\n",
      "(Tokyo, Japan) (IUI ’18) . Association for Computing Machinery, New York, NY,\n",
      "USA, 329–340. https://doi.org/10.1145/3172944.3172983\n",
      "[18] Pat Croskerry. 2009. Clinical cognition and diagnostic error: applications of a\n",
      "dual process model of reasoning. Advances in health sciences education 14, 1\n",
      "(2009), 27–35.\n",
      "[19] Xiao Dong and Caroline C Hayes. 2012. Uncertainty visualizations: Helping\n",
      "decision makers become more aware of uncertainty and its implications. Journal\n",
      "of Cognitive Engineering and Decision Making 6, 1 (2012), 30–56.\n",
      "[20] Birte Englich, Thomas Mussweiler, and Fritz Strack. 2006. Playing dice with crim-\n",
      "inal sentences: The influence of irrelevant anchors on experts’ judicial decision\n",
      "making. Personality and Social Psychology Bulletin 32, 2 (2006), 188–200.\n",
      "[21] Shi Feng and Jordan Boyd-Graber. 2019. What Can AI Do for Me? Evaluating\n",
      "Machine Learning Interpretations in Cooperative Play. In Proceedings of the 24th\n",
      "International Conference on Intelligent User Interfaces (Marina del Ray, California)\n",
      "(IUI ’19) . Association for Computing Machinery, New York, NY, USA, 229–239.\n",
      "https://doi.org/10.1145/3301275.3302265\n",
      "[22] Michael Fernandes, Logan Walls, Sean Munson, Jessica Hullman, and Matthew\n",
      "Kay. 2018. Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit\n",
      "Decision-Making. In Proceedings of the 2018 CHI Conference on Human Factors inDoes the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan\n",
      "Computing Systems (Montreal QC, Canada) (CHI ’18) . Association for Computing\n",
      "Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/3173574.3173718\n",
      "[23] Katy Ilonka Gero, Zahra Ashktorab, Casey Dugan, Qian Pan, James Johnson,\n",
      "Werner Geyer, Maria Ruiz, Sarah Miller, David R. Millen, Murray Campbell,\n",
      "Sadhana Kumaravel, and Wei Zhang. 2020. Mental Models of AI Agents in a\n",
      "Cooperative Game Setting. In Proceedings of the 2020 CHI Conference on Human\n",
      "Factors in Computing Systems (Honolulu, HI, USA) (CHI ’20) . Association for\n",
      "Computing Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/\n",
      "3313831.3376316\n",
      "[24] Reza Ghaeini, Xiaoli Fern, and Prasad Tadepalli. 2018. Interpreting Recurrent and\n",
      "Attention-Based Neural Models: a Case Study on Natural Language Inference.\n",
      "InProceedings of the 2018 Conference on Empirical Methods in Natural Language\n",
      "Processing . Association for Computational Linguistics, Brussels, Belgium, 4952–\n",
      "4957. https://doi.org/10.18653/v1/D18-1537\n",
      "[25] Dimitra Gkatzia, Oliver Lemon, and Verena Rieser. 2016. Natural Language\n",
      "Generation enhances human decision-making with uncertain information. In\n",
      "Proceedings of the 54th Annual Meeting of the Association for Computational\n",
      "Linguistics (Volume 2: Short Papers) . Association for Computational Linguistics,\n",
      "Berlin, Germany, 264–268. https://doi.org/10.18653/v1/P16-2043\n",
      "[26] Elena L Glassman, Jeremy Scott, Rishabh Singh, Philip J Guo, and Robert C Miller.\n",
      "2015. OverCode: Visualizing variation in student solutions to programming\n",
      "problems at scale. ACM Transactions on Computer-Human Interaction (TOCHI)\n",
      "22, 2 (2015), 1–35.\n",
      "[27] Ben Green and Yiling Chen. 2019. The principles and limits of algorithm-in-the-\n",
      "loop decision making. Proceedings of the ACM on Human-Computer Interaction 3,\n",
      "CSCW (2019), 1–24.\n",
      "[28] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On Calibration\n",
      "of Modern Neural Networks. In Proceedings of the 34th International Conference\n",
      "on Machine Learning - Volume 70 (ICML’17) . JMLR.org, Sydney, NSW, Australia,\n",
      "1321–1330.\n",
      "[29] Peter Hase and Mohit Bansal. 2020. Evaluating Explainable AI: Which Algorith-\n",
      "mic Explanations Help Users Predict Model Behavior?. In Proceedings of the 58th\n",
      "Annual Meeting of the Association for Computational Linguistics . Association for\n",
      "Computational Linguistics, Online, 5540–5552. https://doi.org/10.18653/v1/2020.\n",
      "acl-main.491\n",
      "[30] Yugo Hayashi and Kosuke Wakabayashi. 2017. Can AI Become Reliable Source\n",
      "to Support Human Decision Making in a Court Scene?. In Companion of the 2017\n",
      "ACM Conference on Computer Supported Cooperative Work and Social Computing\n",
      "(Portland, Oregon, USA) (CSCW ’17 Companion) . Association for Computing Ma-\n",
      "chinery, New York, NY, USA, 195–198. https://doi.org/10.1145/3022198.3026338\n",
      "[31] Ruining He and Julian McAuley. 2016. Ups and Downs: Modeling the Vi-\n",
      "sual Evolution of Fashion Trends with One-Class Collaborative Filtering. In\n",
      "Proceedings of the 25th International Conference on World Wide Web (Mon-\n",
      "tréal, Québec, Canada) (WWW ’16) . International World Wide Web Confer-\n",
      "ences Steering Committee, Republic and Canton of Geneva, CHE, 507–517.\n",
      "https://doi.org/10.1145/2872427.2883037\n",
      "[32] Dan Hendrycks and Kevin Gimpel. 2017. A Baseline for Detecting Misclassified\n",
      "and Out-of-Distribution Examples in Neural Networks. In 5th International Con-\n",
      "ference on Learning Representations, ICLR 2017 . OpenReview.net, Toulon, France,\n",
      "1–12. https://openreview.net/forum?id=Hkg4TI9xl\n",
      "[33] Chien-Ju Ho, Aleksandrs Slivkins, Siddharth Suri, and Jennifer Wortman Vaughan.\n",
      "2015. Incentivizing High Quality Crowdwork. In Proceedings of the 24th Interna-\n",
      "tional Conference on World Wide Web (Florence, Italy) (WWW ’15) . International\n",
      "World Wide Web Conferences Steering Committee, Republic and Canton of\n",
      "Geneva, CHE, 419–429. https://doi.org/10.1145/2736277.2741102\n",
      "[34] Eric Horvitz. 1999. Principles of Mixed-Initiative User Interfaces. In Proceedings\n",
      "of the SIGCHI Conference on Human Factors in Computing Systems (Pittsburgh,\n",
      "Pennsylvania, USA) (CHI ’99) . Association for Computing Machinery, New York,\n",
      "NY, USA, 159–166. https://doi.org/10.1145/302979.303030\n",
      "[35] Eric Horvitz and Tim Paek. 2007. Complementary computing: policies for trans-\n",
      "ferring callers from dialog systems to human receptionists. User Modeling and\n",
      "User-Adapted Interaction 17, 1-2 (2007), 159–182.\n",
      "[36] Shagun Jhaver, Amy Bruckman, and Eric Gilbert. 2019. Does transparency in\n",
      "moderation really matter? User behavior after content removal explanations on\n",
      "reddit. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019),\n",
      "1–27.\n",
      "[37] Susan Joslyn and Jared LeClerc. 2013. Decisions with uncertainty: The glass half\n",
      "full. Current Directions in Psychological Science 22, 4 (2013), 308–315.\n",
      "[38] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach,\n",
      "and Jennifer Wortman Vaughan. 2020. Interpreting Interpretability: Under-\n",
      "standing Data Scientists’ Use of Interpretability Tools for Machine Learning. In\n",
      "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems\n",
      "(Honolulu, HI, USA) (CHI ’20) . Association for Computing Machinery, New York,\n",
      "NY, USA, 1–14. https://doi.org/10.1145/3313831.3376219\n",
      "[39] Derek J Koehler. 1991. Explanation, imagination, and confidence in judgment.\n",
      "Psychological bulletin 110, 3 (1991), 499.\n",
      "[40] Pang Wei Koh and Percy Liang. 2017. Understanding Black-Box Predictions\n",
      "via Influence Functions. In Proceedings of the 34th International Conference onMachine Learning - Volume 70 (ICML’17) . JMLR.org, Sydney, NSW, Australia,\n",
      "1885–1894.\n",
      "[41] Johannes Kunkel, Tim Donkers, Lisa Michael, Catalin-Mihai Barbu, and Jür-\n",
      "gen Ziegler. 2019. Let Me Explain: Impact of Personal and Impersonal Ex-\n",
      "planations on Trust in Recommender Systems. In Proceedings of the 2019 CHI\n",
      "Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk)\n",
      "(CHI ’19) . Association for Computing Machinery, New York, NY, USA, 1–12.\n",
      "https://doi.org/10.1145/3290605.3300717\n",
      "[42] Vivian Lai, Han Liu, and Chenhao Tan. 2020. \"Why is ’Chicago’ Deceptive?\"\n",
      "Towards Building Model-Driven Tutorials for Humans. In Proceedings of the 2020\n",
      "CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA)\n",
      "(CHI ’20) . Association for Computing Machinery, New York, NY, USA, 1–13.\n",
      "https://doi.org/10.1145/3313831.3376873\n",
      "[43] Vivian Lai and Chenhao Tan. 2019. On Human Predictions with Explanations and\n",
      "Predictions of Machine Learning Models: A Case Study on Deception Detection.\n",
      "InProceedings of the Conference on Fairness, Accountability, and Transparency\n",
      "(Atlanta, GA, USA) (FAT* ’19) . Association for Computing Machinery, New York,\n",
      "NY, USA, 29–38. https://doi.org/10.1145/3287560.3287590\n",
      "[44] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Eric Horvitz. 2017. Iden-\n",
      "tifying Unknown Unknowns in the Open World: Representations and Policies\n",
      "for Guided Exploration. In Proceedings of the Thirty-First AAAI Conference on\n",
      "Artificial Intelligence (AAAI’17) . AAAI Press, San Francisco, California, USA,\n",
      "2124–2132.\n",
      "[45] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec.\n",
      "2017. Interpretable & Explorable Approximations of Black Box Models.\n",
      "arXiv:1707.01154 [cs.AI]\n",
      "[46] John D. Lee and Katrina A. See. 2004. Trust in Automation: Designing for\n",
      "Appropriate Reliance. Human Factors 46, 1 (2004), 50–80. https://doi.org/10.\n",
      "1518/hfes.46.1.50_30392\n",
      "[47] Timothy R Levine. 2014. Truth-default theory (TDT) a theory of human deception\n",
      "and deception detection. Journal of Language and Social Psychology 33, 4 (2014),\n",
      "378–392.\n",
      "[48] B. Lim, A. Dey, and D. Avrahami. 2009. Why and Why Not Explanations Improve\n",
      "the Intelligibility of Context-aware Intelligent Systems. In Proceedings of the\n",
      "SIGCHI Conference on Human Factors in Computing Systems (Boston, MA, USA)\n",
      "(CHI ’09) . ACM, New York, NY, USA, 2119–2128. https://doi.org/10.1145/1518701.\n",
      "1519023\n",
      "[49] Zhouhan Lin, Minwei Feng, Cícero Nogueira dos Santos, Mo Yu, Bing Xiang,\n",
      "Bowen Zhou, and Yoshua Bengio. 2017. A Structured Self-Attentive Sentence\n",
      "Embedding. In 5th International Conference on Learning Representations, ICLR\n",
      "2017, Conference Track Proceedings . OpenReview.net, Toulon, France, 1–15. https:\n",
      "//openreview.net/forum?id=BJC_jUqxe\n",
      "[50] Zachary C. Lipton. 2018. The Mythos of Model Interpretability: In Machine\n",
      "Learning, the Concept of Interpretability is Both Important and Slippery. Queue\n",
      "16, 3 (June 2018), 31–57. https://doi.org/10.1145/3236386.3241340\n",
      "[51] Angli Liu, Stephen Soderland, Jonathan Bragg, Christopher H. Lin, Xiao Ling,\n",
      "and Daniel S. Weld. 2016. Effective Crowd Annotation for Relation Extraction. In\n",
      "Proceedings of the 2016 Conference of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language Technologies . Association for\n",
      "Computational Linguistics, San Diego, California, 897–906. https://doi.org/10.\n",
      "18653/v1/N16-1104\n",
      "[52] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\n",
      "Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\n",
      "Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\n",
      "[53] Scott M. Lundberg, Bala Nair, Monica S. Vavilala, Mayumi Horibe, Michael J.\n",
      "Eisses, Trevor Adams, David E. Liston, Daniel King-Wai Low, Shu-Fang Newman,\n",
      "Jerry Kim, and Su-In Lee. 2018. Explainable machine-learning predictions for the\n",
      "prevention of hypoxaemia during surgery. Nature Biomedical Engineering 2, 10\n",
      "(01 Oct 2018), 749–760. https://doi.org/10.1038/s41551-018-0304-0\n",
      "[54] Les Macleod. 2011. Avoiding “groupthink” A manager’s challenge. Nursing\n",
      "management 42, 10 (2011), 44–48.\n",
      "[55] David Madras, Toniann Pitassi, and Richard Zemel. 2018. Predict Responsibly:\n",
      "Improving Fairness and Accuracy by Learning to Defer. In Proceedings of the\n",
      "32nd International Conference on Neural Information Processing Systems (Montréal,\n",
      "Canada) (NIPS’18) . Curran Associates Inc., Red Hook, NY, USA, 6150–6160.\n",
      "[56] Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012. Learning Attitudes\n",
      "and Attributes from Multi-Aspect Reviews. In Proceedings of the 2012 IEEE 12th\n",
      "International Conference on Data Mining (ICDM ’12) . IEEE Computer Society,\n",
      "USA, 1020–1025. https://doi.org/10.1109/ICDM.2012.110\n",
      "[57] T. Miller. 2018. Explanation in artificial intelligence: Insights from the social\n",
      "sciences. Artificial Intelligence 267 (February 2018), 1–38.\n",
      "[58] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,\n",
      "Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019.\n",
      "Model Cards for Model Reporting. In Proceedings of the Conference on Fairness,\n",
      "Accountability, and Transparency (Atlanta, GA, USA) (FAT* ’19) . Association for\n",
      "Computing Machinery, New York, NY, USA, 220–229. https://doi.org/10.1145/\n",
      "3287560.3287596CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al.\n",
      "[59] Hussein Mozannar and David Sontag. 2020. Consistent Estimators for Learning to\n",
      "Defer to an Expert. In Proceedings of the 37th International Conference on Machine\n",
      "Learning (Proceedings of Machine Learning Research, Vol. 119) , Hal Daumé III and\n",
      "Aarti Singh (Eds.). PMLR, Virtual, 7076–7087. http://proceedings.mlr.press/v119/\n",
      "mozannar20b.html\n",
      "[60] Limor Nadav-Greenberg and Susan L Joslyn. 2009. Uncertainty forecasts improve\n",
      "decision making among nonexperts. Journal of Cognitive Engineering and Decision\n",
      "Making 3, 3 (2009), 209–227.\n",
      "[61] Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, and Finale\n",
      "Doshi-Velez. 2018. How do Humans Understand Explanations from Machine\n",
      "Learning Systems? An Evaluation of the Human-Interpretability of Explanation.\n",
      "arXiv:1802.00682 [cs.AI]\n",
      "[62] Dong Nguyen. 2018. Comparing Automatic and Human Evaluation of Local\n",
      "Explanations for Text Classification. In Proceedings of the 2018 Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics: Human\n",
      "Language Technologies, Volume 1 (Long Papers) . Association for Computational\n",
      "Linguistics, New Orleans, Louisiana, 1069–1078. https://doi.org/10.18653/v1/N18-\n",
      "1097\n",
      "[63] Joon Sung Park, Rick Barber, Alex Kirlik, and Karrie Karahalios. 2019. A Slow\n",
      "Algorithm Improves Users’ Assessments of the Algorithm’s Accuracy. Proceedings\n",
      "of the ACM on Human-Computer Interaction 3, CSCW (2019), 1–15.\n",
      "[64] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and\n",
      "Kyunghyun Cho. 2019. Finding Generalizable Evidence by Learning to Convince\n",
      "Q&A Models. In Proceedings of the 2019 Conference on Empirical Methods in\n",
      "Natural Language Processing and the 9th International Joint Conference on Natural\n",
      "Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics,\n",
      "Hong Kong, China, 2402–2411. https://doi.org/10.18653/v1/D19-1244\n",
      "[65] Forough Poursabzi-Sangdeh, Daniel G. Goldstein, Jake M. Hofman, Jennifer Wort-\n",
      "man Vaughan, and Hanna Wallach. 2019. Manipulating and Measuring Model\n",
      "Interpretability. arXiv:1802.07810 [cs.AI]\n",
      "[66] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I\n",
      "Trust You?\": Explaining the Predictions of Any Classifier. In Proceedings of the\n",
      "22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\n",
      "Mining (San Francisco, California, USA) (KDD ’16) . Association for Computing\n",
      "Machinery, New York, NY, USA, 1135–1144. https://doi.org/10.1145/2939672.\n",
      "2939778\n",
      "[67] Philipp Schmidt and Felix Biessmann. 2019. Quantifying Interpretability and\n",
      "Trust in Machine Learning Systems. arXiv:1901.08558 [cs.LG]\n",
      "[68] C. Estelle Smith, Bowen Yu, Anjali Srivastava, Aaron Halfaker, Loren Terveen,\n",
      "and Haiyi Zhu. 2020. Keeping Community in the Loop: Understanding Wikipedia\n",
      "Stakeholder Values for Machine Learning-Based Systems. In Proceedings of the\n",
      "2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI,\n",
      "USA) (CHI ’20) . Association for Computing Machinery, New York, NY, USA, 1–14.\n",
      "https://doi.org/10.1145/3313831.3376783\n",
      "[69] Simone Stumpf, Vidya Rajaram, Lida Li, Weng-Keen Wong, Margaret Burnett,\n",
      "Thomas Dietterich, Erin Sullivan, and Jonathan Herlocker. 2009. Interacting\n",
      "meaningfully with machine learning systems: Three experiments. International\n",
      "Journal of Human-Computer Studies 67, 8 (2009), 639–662.\n",
      "[70] Sarah Tan, Julius Adebayo, Kori Inkpen, and Ece Kamar. 2018. Inves-\n",
      "tigating Human + Machine Complementarity for Recidivism Predictions.\n",
      "arXiv:1808.09123 [cs.LG][71] LSAT Prep Books Team. 2017. LSAT prep book study guide: quick study & practice\n",
      "test questions for the Law School Admissions council’s (LSAC) Law school admission\n",
      "test. Mometrix Test Preparation, Beaumont, TX.\n",
      "[72] Kush R. Varshney, Prashant Khanduri, Pranay Sharma, Shan Zhang, and\n",
      "Pramod K. Varshney. 2018. Why Interpretability in Machine Learn-\n",
      "ing? An Answer Using Distributed Detection and Data Fusion Theory.\n",
      "arXiv:1806.09710 [stat.ML]\n",
      "[73] J. von Neumann and O. Morgenstern. 1947. Theory of games and economic\n",
      "behavior . Princeton University Press, Princeton, New Jersey.\n",
      "[74] Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y. Lim. 2019. Design-\n",
      "ing Theory-Driven User-Centric Explainable AI. In Proceedings of the 2019 CHI\n",
      "Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk)\n",
      "(CHI ’19) . Association for Computing Machinery, New York, NY, USA, 1–15.\n",
      "https://doi.org/10.1145/3290605.3300831\n",
      "[75] Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. 2016. Attention-based\n",
      "LSTM for Aspect-level Sentiment Classification. In Proceedings of the 2016 Confer-\n",
      "ence on Empirical Methods in Natural Language Processing . Association for Com-\n",
      "putational Linguistics, Austin, Texas, 606–615. https://doi.org/10.18653/v1/D16-\n",
      "1058\n",
      "[76] Hilde J. P. Weerts, Werner van Ipenburg, and Mykola Pechenizkiy. 2019. A Human-\n",
      "Grounded Evaluation of SHAP for Alert Processing. arXiv:1907.03324 [cs.LG]\n",
      "[77] Daniel S. Weld and Gagan Bansal. 2019. The Challenge of Crafting Intelligible\n",
      "Intelligence. Commun. ACM 62, 6 (May 2019), 70–79. https://doi.org/10.1145/\n",
      "3282486\n",
      "[78] Jenna Wiens, John Guttag, and Eric Horvitz. 2016. Patient risk stratification\n",
      "with time-varying parameters: a multitask learning approach. JMLR 17, 1 (2016),\n",
      "2797–2819.\n",
      "[79] Bryan Wilder, Eric Horvitz, and Ece Kamar. 2020. Learning to Complement\n",
      "Humans. In Proceedings of the Twenty-Ninth International Joint Conference on\n",
      "Artificial Intelligence, IJCAI-20 , Christian Bessiere (Ed.). International Joint Con-\n",
      "ferences on Artificial Intelligence Organization, Yokohama, Japan, 1526–1533.\n",
      "https://doi.org/10.24963/ijcai.2020/212 Main track.\n",
      "[80] Fumeng Yang, Zhuanyi Huang, Jean Scholtz, and Dustin L. Arendt. 2020. How Do\n",
      "Visual Explanations Foster End Users’ Appropriate Trust in Machine Learning?.\n",
      "InProceedings of the 25th International Conference on Intelligent User Interfaces\n",
      "(Cagliari, Italy) (IUI ’20) . Association for Computing Machinery, New York, NY,\n",
      "USA, 189–201. https://doi.org/10.1145/3377325.3377480\n",
      "[81] Kun Yu, Shlomo Berkovsky, Ronnie Taib, Jianlong Zhou, and Fang Chen. 2019.\n",
      "Do I Trust My Machine Teammate? An Investigation from Perception to Decision.\n",
      "InProceedings of the 24th International Conference on Intelligent User Interfaces\n",
      "(Marina del Ray, California) (IUI ’19) . Association for Computing Machinery, New\n",
      "York, NY, USA, 460–468. https://doi.org/10.1145/3301275.3302277\n",
      "[82] Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. ReClor: A Reading\n",
      "Comprehension Dataset Requiring Logical Reasoning. In 8th International Con-\n",
      "ference on Learning Representations, ICLR 2020 . OpenReview.net, Addis Ababa,\n",
      "Ethiopia, 1–26. https://openreview.net/forum?id=HJgJtT4tvB\n",
      "[83] Yunfeng Zhang, Q. Vera Liao, and Rachel K. E. Bellamy. 2020. Effect of Confi-\n",
      "dence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision\n",
      "Making. In Proceedings of the 2020 Conference on Fairness, Accountability, and\n",
      "Transparency (Barcelona, Spain) (FAT* ’20) . Association for Computing Machin-\n",
      "ery, New York, NY, USA, 295–305. https://doi.org/10.1145/3351095.3372852\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 32768 tokens. However, your messages resulted in 50379 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m task4 \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mBased on the text in paper.txt, answer the following question.\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mThe answer must be 3 bullet points that can be inserted in a ppt slide.\u001b[39m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mHint: Print the contents of the pdf, and you your language ability to answer.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m user_proxy\u001b[39m.\u001b[39;49minitiate_chat(assistant, message\u001b[39m=\u001b[39;49mtask4, clear_history\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/flaml/autogen/agentchat/responsive_agent.py:521\u001b[0m, in \u001b[0;36mResponsiveAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[1;32m    509\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 521\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/flaml/autogen/agentchat/responsive_agent.py:324\u001b[0m, in \u001b[0;36mResponsiveAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    322\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    323\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 324\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    325\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/flaml/autogen/agentchat/responsive_agent.py:454\u001b[0m, in \u001b[0;36mResponsiveAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    452\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/flaml/autogen/agentchat/responsive_agent.py:324\u001b[0m, in \u001b[0;36mResponsiveAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    322\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    323\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 324\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    325\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/flaml/autogen/agentchat/responsive_agent.py:454\u001b[0m, in \u001b[0;36mResponsiveAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    452\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/flaml/autogen/agentchat/responsive_agent.py:324\u001b[0m, in \u001b[0;36mResponsiveAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    322\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    323\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 324\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    325\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/flaml/autogen/agentchat/responsive_agent.py:452\u001b[0m, in \u001b[0;36mResponsiveAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/flaml/autogen/agentchat/responsive_agent.py:764\u001b[0m, in \u001b[0;36mResponsiveAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 764\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    765\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    766\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/flaml/autogen/agentchat/responsive_agent.py:596\u001b[0m, in \u001b[0;36mResponsiveAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    593\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    595\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    597\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mllm_config\n\u001b[1;32m    598\u001b[0m )\n\u001b[1;32m    599\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/flaml/autogen/oai/completion.py:768\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    766\u001b[0m     base_config[\u001b[39m\"\u001b[39m\u001b[39mretry_timeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    767\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    769\u001b[0m         context,\n\u001b[1;32m    770\u001b[0m         use_cache,\n\u001b[1;32m    771\u001b[0m         raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mi \u001b[39m<\u001b[39;49m last \u001b[39mor\u001b[39;49;00m raise_on_ratelimit_or_timeout,\n\u001b[1;32m    772\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbase_config,\n\u001b[1;32m    773\u001b[0m     )\n\u001b[1;32m    774\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    775\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/flaml/autogen/oai/completion.py:799\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[1;32m    798\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/flaml/autogen/oai/completion.py:204\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n\u001b[0;32m--> 204\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n\u001b[1;32m    205\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(request_timeout\u001b[39m=\u001b[39mrequest_timeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/flaml4/lib/python3.9/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 32768 tokens. However, your messages resulted in 50379 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "task4 = \"\"\"Based on the text in paper.txt, answer the following question.\n",
    "The answer must be 3 bullet points that can be inserted in a ppt slide.\n",
    "\n",
    "Question: What is the key problem being discussed in the paper?\n",
    "\n",
    "Hint: Use previously printed contents of the pdf, and you your language ability to answer.\n",
    "\"\"\"\n",
    "user_proxy.initiate_chat(assistant, message=task4, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task5 = \"\"\"Based on the text in paper.txt, answer the following question.\n",
    "The answer must be 3 bullet points that can be inserted in a ppt slide.\n",
    "\n",
    "Question: What is they key related work discussed in the paper?\n",
    "\n",
    "Hint: Use previously printed contents of the pdf, and you your language ability to answer.\n",
    "\"\"\"\n",
    "user_proxy.initiate_chat(assistant, message=task5, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task6 = \"\"\"Based on the text in paper.txt, answer the following question.\n",
    "The answer must be 3 bullet points that can be inserted in a ppt slide.\n",
    "\n",
    "Question: What are they key results discussed in the experiments of the paper?\n",
    "\n",
    "Hint: Use previously printed contents of the pdf, and you your language ability to answer.\n",
    "\"\"\"\n",
    "user_proxy.initiate_chat(assistant, message=task6, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task7 = \"\"\"Based on the questions asked thus far and your responses,\n",
    "Create a pptx file with called reading_group.pptx.\n",
    "\n",
    "The pptx file must have four slides with the following titles:\n",
    "1. Title\n",
    "2. Problem\n",
    "3. Related Work\n",
    "4. Results\n",
    "\n",
    "Each slide should have the response we previously discussed as bullet points.\n",
    "\n",
    "Slides should be aesthetically pleasing and ready to present to a group of people.\n",
    "Hint: Write python code to accomplish the task. Use python packages that create ppt if you have to.\n",
    "If you use pptx package, use the api properly.\n",
    "\n",
    "\"\"\"\n",
    "user_proxy.initiate_chat(assistant, message=task7, clear_history=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
