{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet flaml\"[autogen]\"~=2.0.0 docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import autogen\n",
    "\n",
    "llm_config={\n",
    "    \"request_timeout\": 600,\n",
    "    \"seed\": 44,  # change the seed for different trials\n",
    "    \"config_list\": autogen.config_list_from_json(\"OAI_CONFIG_LIST\"),\n",
    "    \"temperature\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent instance named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"work_dir\",\n",
    "        \"use_docker\": [\"python:3.9.6\"],\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Download and save the pdf from the given URL to file called paper.pdf.\n",
      "URL: https://dl.acm.org/doi/pdf/10.1145/3411764.3445717 to\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Here is a Python script that uses the `requests` library to download the PDF file from the given URL and save it as `paper.pdf`.\n",
      "\n",
      "```python\n",
      "# filename: download_pdf.py\n",
      "\n",
      "import requests\n",
      "\n",
      "url = \"https://dl.acm.org/doi/pdf/10.1145/3411764.3445717\"\n",
      "response = requests.get(url)\n",
      "\n",
      "with open('paper.pdf', 'wb') as f:\n",
      "    f.write(response.content)\n",
      "\n",
      "print(\"PDF downloaded and saved as paper.pdf\")\n",
      "```\n",
      "\n",
      "You can run this script by saving it to a file named `download_pdf.py` and executing it with Python.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"download_pdf.py\", line 3, in <module>\n",
      "    import requests\n",
      "ModuleNotFoundError: No module named 'requests'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "It seems like the 'requests' module is not installed in your Python environment. You can install it using pip, which is a package manager for Python. Here is a shell command to install 'requests'.\n",
      "\n",
      "```sh\n",
      "pip install requests\n",
      "```\n",
      "\n",
      "After installing 'requests', you can run the previous Python script again.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (198 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2023.7.22 charset-normalizer-3.2.0 idna-3.4 requests-2.31.0 urllib3-2.0.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Great, the 'requests' module has been successfully installed. Now, let's try to run the Python script again to download the PDF.\n",
      "\n",
      "```python\n",
      "# filename: download_pdf.py\n",
      "\n",
      "import requests\n",
      "\n",
      "url = \"https://dl.acm.org/doi/pdf/10.1145/3411764.3445717\"\n",
      "response = requests.get(url)\n",
      "\n",
      "with open('paper.pdf', 'wb') as f:\n",
      "    f.write(response.content)\n",
      "\n",
      "print(\"PDF downloaded and saved as paper.pdf\")\n",
      "```\n",
      "\n",
      "You can run this script by saving it to a file named `download_pdf.py` and executing it with Python.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "PDF downloaded and saved as paper.pdf\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Great! The PDF has been successfully downloaded and saved as 'paper.pdf'. You should be able to find the file in the same directory where you ran the script. If you have any other tasks, feel free to ask. Otherwise, we can conclude here. \n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task1 = \"\"\"Download and save the pdf from the given URL to file called paper.pdf.\n",
    "URL: https://dl.acm.org/doi/pdf/10.1145/3411764.3445717 to\n",
    "\"\"\"\n",
    "\n",
    "user_proxy.initiate_chat(assistant, message=task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Convert the pdf to text and save the text to a file named paper.txt\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To convert a PDF to text, we can use the `PyPDF2` library in Python. If it's not already installed, you can install it using pip. Here is the Python code to perform the conversion:\n",
      "\n",
      "```python\n",
      "# filename: pdf_to_text.py\n",
      "\n",
      "import PyPDF2\n",
      "\n",
      "pdf_file = open('paper.pdf', 'rb')\n",
      "read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
      "number_of_pages = read_pdf.getNumPages()\n",
      "\n",
      "text_file = open('paper.txt', 'w')\n",
      "\n",
      "for page_number in range(number_of_pages):   \n",
      "    page = read_pdf.getPage(page_number)\n",
      "    page_content = page.extractText()\n",
      "    text_file.write(page_content)\n",
      "\n",
      "text_file.close()\n",
      "pdf_file.close()\n",
      "\n",
      "print(\"PDF converted to text and saved as paper.txt\")\n",
      "```\n",
      "\n",
      "You can run this script by saving it to a file named `pdf_to_text.py` and executing it with Python.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"pdf_to_text.py\", line 3, in <module>\n",
      "    import PyPDF2\n",
      "ModuleNotFoundError: No module named 'PyPDF2'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "It seems like the 'PyPDF2' module is not installed in your Python environment. You can install it using pip, which is a package manager for Python. Here is a shell command to install 'PyPDF2'.\n",
      "\n",
      "```sh\n",
      "pip install PyPDF2\n",
      "```\n",
      "\n",
      "After installing 'PyPDF2', you can run the previous Python script again.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Collecting typing_extensions>=3.10.0.0\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing-extensions, PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1 typing-extensions-4.7.1\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Great, the 'PyPDF2' module has been successfully installed. Now, let's try to run the Python script again to convert the PDF to text.\n",
      "\n",
      "```python\n",
      "# filename: pdf_to_text.py\n",
      "\n",
      "import PyPDF2\n",
      "\n",
      "pdf_file = open('paper.pdf', 'rb')\n",
      "read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
      "number_of_pages = read_pdf.getNumPages()\n",
      "\n",
      "text_file = open('paper.txt', 'w')\n",
      "\n",
      "for page_number in range(number_of_pages):   \n",
      "    page = read_pdf.getPage(page_number)\n",
      "    page_content = page.extractText()\n",
      "    text_file.write(page_content)\n",
      "\n",
      "text_file.close()\n",
      "pdf_file.close()\n",
      "\n",
      "print(\"PDF converted to text and saved as paper.txt\")\n",
      "```\n",
      "\n",
      "You can run this script by saving it to a file named `pdf_to_text.py` and executing it with Python.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"pdf_to_text.py\", line 6, in <module>\n",
      "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/PyPDF2/_reader.py\", line 1974, in __init__\n",
      "    deprecation_with_replacement(\"PdfFileReader\", \"PdfReader\", \"3.0.0\")\n",
      "  File \"/usr/local/lib/python3.9/site-packages/PyPDF2/_utils.py\", line 369, in deprecation_with_replacement\n",
      "    deprecation(DEPR_MSG_HAPPENED.format(old_name, removed_in, new_name))\n",
      "  File \"/usr/local/lib/python3.9/site-packages/PyPDF2/_utils.py\", line 351, in deprecation\n",
      "    raise DeprecationError(msg)\n",
      "PyPDF2.errors.DeprecationError: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "It seems like the `PdfFileReader` class has been deprecated in the version of PyPDF2 you have installed. We should use `PdfReader` instead. Here is the updated Python code:\n",
      "\n",
      "```python\n",
      "# filename: pdf_to_text.py\n",
      "\n",
      "from PyPDF2 import PdfReader\n",
      "\n",
      "pdf_file = open('paper.pdf', 'rb')\n",
      "read_pdf = PdfReader(pdf_file)\n",
      "number_of_pages = len(read_pdf.pages)\n",
      "\n",
      "text_file = open('paper.txt', 'w')\n",
      "\n",
      "for page_number in range(number_of_pages):   \n",
      "    page = read_pdf.pages[page_number]\n",
      "    page_content = page.extract_text()\n",
      "    text_file.write(page_content)\n",
      "\n",
      "text_file.close()\n",
      "pdf_file.close()\n",
      "\n",
      "print(\"PDF converted to text and saved as paper.txt\")\n",
      "```\n",
      "\n",
      "You can run this script by saving it to a file named `pdf_to_text.py` and executing it with Python.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "PDF converted to text and saved as paper.txt\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Great! The PDF has been successfully converted to text and saved as 'paper.txt'. You should be able to find the file in the same directory where you ran the script. If you have any other tasks, feel free to ask. Otherwise, we can conclude here. \n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task2 = \"Convert the pdf to text and save the text to a file named paper.txt\"\n",
    "user_proxy.initiate_chat(assistant, message=task2, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Based on the text in paper.txt, answer the following question.\n",
      "\n",
      "Question: What it the title of the paper?\n",
      "\n",
      "Hint: Print the contents of the pdf, and you your language ability to answer.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To read the contents of the text file and print it, we can use the following Python code:\n",
      "\n",
      "```python\n",
      "# filename: read_text.py\n",
      "\n",
      "with open('paper.txt', 'r') as file:\n",
      "    content = file.read()\n",
      "\n",
      "print(content)\n",
      "```\n",
      "\n",
      "This script will print the contents of 'paper.txt'. The title of the paper is typically at the beginning of the text. You can run this script by saving it to a file named `read_text.py` and executing it with Python.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Does the Whole Exceed its Parts? The Efect of AI Explanations \n",
      "on Complementary Team Performance \n",
      "Gagan Bansal∗ \n",
      "bansalg@cs.washington.edu \n",
      "University of Washington \n",
      "Raymond Fok† \n",
      "rayfok@cs.washington.edu \n",
      "University of Washington \n",
      "Marco Tulio Ribeiro \n",
      "marcotcr@microsoft.com \n",
      "Microsoft Research \n",
      "ABSTRACT \n",
      "Many researchers motivate explainable AI with studies showing \n",
      "that human-AI team performance on decision-making tasks im-\n",
      "proves when the AI explains its recommendations. However, prior \n",
      "studies observed improvements from explanations only when the \n",
      "AI, alone, outperformed both the human and the best team. Can \n",
      "explanations help lead to complementary performance, where team \n",
      "accuracy is higher than either the human or the AI working solo? \n",
      "We conduct mixed-method user studies on three datasets, where an \n",
      "AI with accuracy comparable to humans helps participants solve a \n",
      "task (explaining itself in some conditions). While we observed com-\n",
      "plementary improvements from AI augmentation, they were not in-\n",
      "creased by explanations. Rather, explanations increased the chance \n",
      "that humans will accept the AI’s recommendation, regardless of its \n",
      "correctness. Our result poses new challenges for human-centered \n",
      "AI: Can we develop explanatory approaches that encourage ap-\n",
      "propriate trust in AI, and therefore help generate (or improve) \n",
      "complementary performance? \n",
      "CCS CONCEPTS \n",
      "• Human-centered computing → Empirical studies in HCI; \n",
      "Interactive systems and tools;• Computing methodologies → \n",
      "Machine learning. \n",
      "∗Equal contribution.\n",
      "†Made especially large contributions. \n",
      "Permission to make digital or hard copies of all or part of this work for personal or \n",
      "classroom use is granted without fee provided that copies are not made or distributed \n",
      "for proft or commercial advantage and that copies bear this notice and the full citation \n",
      "on the frst page. Copyrights for components of this work owned by others than the \n",
      "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or \n",
      "republish, to post on servers or to redistribute to lists, requires prior specifc permission \n",
      "and/or a fee. Request permissions from permissions@acm.org. \n",
      "CHI ’21, May 8–13, 2021, Yokohama, Japan \n",
      "© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM. \n",
      "ACM ISBN 978-1-4503-8096-6/21/05...$15.00 \n",
      "https://doi.org/10.1145/3411764.3445717 Tongshuang Wu∗ \n",
      "wtshuang@cs.washington.edu \n",
      "University of Washington \n",
      "Besmira Nushi \n",
      "besmira.nushi@microsoft.com \n",
      "Microsoft Research Joyce Zhou† \n",
      "jyzhou15@cs.washington.edu \n",
      "University of Washington \n",
      "Ece Kamar \n",
      "eckamar@microsoft.com \n",
      "Microsoft Research \n",
      "Daniel S. Weld \n",
      "weld@cs.washington.edu \n",
      "University of Washington & \n",
      "Allen Institute for Artifcial \n",
      "Intelligence \n",
      "KEYWORDS \n",
      "Explainable AI, Human-AI teams, Augmented intelligence \n",
      "ACM Reference Format: \n",
      "Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, \n",
      "Ece Kamar, Marco Tulio Ribeiro, and Daniel S. Weld. 2021. Does the Whole \n",
      "Exceed its Parts? The Efect of AI Explanations on Complementary Team \n",
      "Performance. In CHI Conference on Human Factors in Computing Systems \n",
      "(CHI ’21), May 8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA, \n",
      "16 pages. https://doi.org/10.1145/3411764.3445717 \n",
      "1 INTRODUCTION \n",
      "Although the accuracy of Artifcial Intelligence (AI) systems is \n",
      "rapidly improving, in many cases, it remains risky for an AI to op-\n",
      "erate autonomously, e.g., in high-stakes domains or when legal and \n",
      "ethical matters prohibit full autonomy. A viable strategy for these \n",
      "scenarios is to form Human-AI teams, in which the AI system aug-\n",
      "ments one or more humans by recommending its predictions, but \n",
      "people retain agency and have accountability on the fnal decisions. \n",
      "Examples include AI systems that predict likely hospital readmis-\n",
      "sion to assist doctors with correlated care decisions [8, 13, 15, 78] \n",
      "and AIs that estimate recidivism to help judges decide whether \n",
      "to grant bail to defendants [2, 30]. In such scenarios, it is impor-\n",
      "tant that the human-AI team achieves complementary performance \n",
      "(i.e., performs better than either alone): From a decision-theoretic \n",
      "perspective, a rational developer would only deploy a team if it \n",
      "adds utility to the decision-making process [73]. For example, sig-\n",
      "nifcantly improving decision accuracy by closing defciencies in \n",
      "automated reasoning with human efort, and vice versa [35, 70]. \n",
      "Many researchers have argued that such human-AI teams would \n",
      "be improved if the AI systems could explain their reasoning. In addi-\n",
      "tion to increasing trust between humans and machines or improving \n",
      "the speed of decision making, one hopes that an explanation should \n",
      "help the responsible human know when to trust the AI’s sugges-\n",
      "tion and when to be skeptical, e.g., when the explanation doesn’t \n",
      "“make sense.” Such appropriate reliance [46] is crucial for users to \n",
      "leverage AI assistance and improve task performance [10]. Indeed, \n",
      "CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al. \n",
      "Recommendation (R)\n",
      "+ Explanation\n",
      "0.01.0\n",
      "A\n",
      "Input Decision\n",
      "Human AI\n",
      "Complementary zone \n",
      "( max (Human, AI) , 1 ]\n",
      "Teammates\n",
      "Prior work Ours> 0\n",
      "Change of performance\n",
      "0.0Accuracy of decisions1.0\n",
      "BAccuracy of decisions\n",
      "R+Explanation\n",
      "R only> 0R+Explanation\n",
      "R only?Team setting\n",
      "Figure 1: (Best viewed in color) Do AI explanations lead to complementary team performance? In a team setting, when given an \n",
      "input, the human uses (usually imperfect) recommendations from an AI model to make the fnal decision. We seek to under-\n",
      "stand if automatically generated explanations of the AI’s recommendation improve team performance compared to baselines, \n",
      "such as simply providing the AI’s recommendation, R, and confdence. (A) Most previous work concludes that explanations \n",
      "improve team performance (i.e., ∆A > 0); however, it usually considers settings where AI systems are much more accurate \n",
      "than people and even the human-AI team. (B) Our study considers settings where human and AI performance is comparable \n",
      "to allow room for complementary improvement. We ask, “Do explanations help in this context, and how do they compare to \n",
      "simple confdence-based strategies?” (Is ∆B > 0?). \n",
      "at frst glance, it appears that researchers have already confrmed \n",
      "the utility of explanations on tasks ranging from medical diagno-\n",
      "sis [14, 53], data annotation [67] to deception detection [43]. In \n",
      "each case, the papers show that, when the AI provides explanations, \n",
      "team accuracy reaches a level higher than human-alone. \n",
      "However, a careful reading of these papers shows another com-\n",
      "monality: in every situation, while explanations are shown to help \n",
      "raise team performance closer to that of the AI, one would achieve \n",
      "an even better result by stripping humans from the loop and let-\n",
      "ting the AI operate autonomously (Figure 1A & Table 1). Thus, \n",
      "the existing work suggests several important open questions for \n",
      "the AI and HCI community: Do explanations help achieve comple-\n",
      "mentary performance by enabling humans to anticipate when the \n",
      "AI is potentially incorrect? Furthermore, do explanations provide \n",
      "signifcant value over simpler strategies such as displaying the AI’s \n",
      "uncertainty? In the quest to build the best human-machine teams, \n",
      "such questions deserve critical attention. \n",
      "To explore these questions, we conduct new experiments where \n",
      "we control the study design, ensuring that the AI’s accuracy is \n",
      "comparable to the human’s (Figure 1B). Specifcally, we measure \n",
      "the human skill on our experiment tasks and then control AI ac-\n",
      "curacy by purposely selecting study samples where AI has compa-\n",
      "rable accuracy. This setting simulates situations where there is a \n",
      "strong incentive to deploy human-AI teams, e.g., because there ex-\n",
      "ists more potential for complementary performance (by correcting \n",
      "each other’s mistakes), and where simple heuristics such as blindly \n",
      "following the AI are unlikely to achieve the highest performance. \n",
      "We selected three common-sense tasks that can be tackled by \n",
      "crowd workers with little training: sentiment analysis of book and \n",
      "beer reviews and a set of LSAT questions that require logical reason-\n",
      "ing. We conducted large-scale studies using a variety of explanation \n",
      "sources (AI versus expert-generated) and strategies (explaining just \n",
      "the predicted class, or explaining other classes as well). We observed \n",
      "complementary performance on every task, but — surprisingly — explanations did not appear to ofer beneft compared to simply \n",
      "displaying the AI’s confdence. Notably, explanations increased \n",
      "reliance on recommendations even when the AI was incorrect. Our \n",
      "result echoes prior work on inappropriate trust on systems [38, 58], \n",
      "i.e., explanations can lead humans to either follow incorrect AI \n",
      "suggestions or ignore the correct ones [13, 69]. However, using \n",
      "end-to-end studies, we go one step further to quantify the impact \n",
      "of such over-reliance on objective metrics of team performance. \n",
      "As a frst attempt to tackle the problem of blind reliance on AI, \n",
      "we introduce Adaptive Explanation. Our mechanism tries to reduce \n",
      "human trust when the AI has low confdence: it only explains \n",
      "the predicted class when the AI is confdent, but also explains \n",
      "the alternative otherwise. While it failed to produce signifcant \n",
      "improvement in fnal team performance over other explanation \n",
      "types, there is suggestive evidence that the adaptive approach can \n",
      "push the agreement between AI predictions and human decisions \n",
      "towards the desired direction. \n",
      "Through extensive qualitative analysis, we also summarize po-\n",
      "tential factors that should be considered in experimental settings for \n",
      "studying human-AI complementary performance. For example, the \n",
      "diference in expertise between human and AI afects whether (or \n",
      "how much) AI assistance will help achieve complementary perfor-\n",
      "mance, and the display of the explanation may afect the human’s \n",
      "collaboration strategy. In summary: \n",
      "• We highlight an important limitation of previous work on \n",
      "explainable AI: While many studies show that explaining \n",
      "predictions of AI increases team performance (Table 1), they \n",
      "all consider cases where the AI system is signifcantly more \n",
      "accurate than both the human partner and the human-AI \n",
      "team. In response, we argue that AI explanations for decision-\n",
      "making should aim for complementary performance, where \n",
      "the human-AI team outperforms both solo human and AI. \n",
      "• To study complementary performance, we develop a new \n",
      "experimental setup and use it in studies with 1626 users on Does the Whole Exceed its Parts? The Efect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan \n",
      "three tasks1 to evaluate a variety of explanation sources and \n",
      "strategies. We observe complementary performance in every \n",
      "human-AI teaming condition. \n",
      "• However, surprisingly, we do not observe any signifcant \n",
      "increase in team performance by communicating explana-\n",
      "tions, compared to simply showing the AI’s confdence. Ex-\n",
      "planations often increased accuracy when the AI system \n",
      "was correct but, worryingly, decreased it when the AI erred, \n",
      "resulting in a minimal net change — even for our adaptive \n",
      "explanations. Through qualitative analysis, we discuss poten-\n",
      "tial causes for failure of explanations, behavioral diferences \n",
      "among tasks, and suggest directions for developing more \n",
      "efective AI explanations. \n",
      "2 BACKGROUND AND RELATED WORK \n",
      "Explanations can be useful in many scenarios where a human \n",
      "and AI interact: transparently communicating model predictions \n",
      "[10, 21, 38, 40, 66], teaching humans tasks like translation [3, 26] \n",
      "or content moderation [36], augmenting human analysis proce-\n",
      "dure [36] or creativity [17], legal imperatives [57, 77], etc. Various \n",
      "studies have evaluated the efect of explanations from diferent di-\n",
      "mensions, including whether the explanation improves users’ trust \n",
      "in the AI [41, 81] or enables users to simulate the model predic-\n",
      "tions [16, 65], or assists developers to debug models [9, 38]. \n",
      "In this paper, we focus explicitly on AI-assisted decision making \n",
      "scenarios [6, 74], where an AI assistant (e.g., a classifcation model) \n",
      "makes recommendations to a human (e.g., a judge), who is respon-\n",
      "sible for making fnal decisions (e.g., whether or not to grant bail). \n",
      "In particular, we assess performance in terms of the accuracy of the \n",
      "human-AI team. While other metrics can be used for evaluation \n",
      "(more discussed in Section 6.1), we directly evaluate end-to-end \n",
      "team accuracy for three reasons. First, deploying such a human-AI \n",
      "team is ideal if it achieves complementary performance, i.e., if it \n",
      "outperforms both the AI and the human acting alone. Second, eval-\n",
      "uating explanations using proxy tasks (such as whether humans \n",
      "can use it to guess the model’s prediction) can lead to diferent, \n",
      "misleading conclusions for achieving best team performance than \n",
      "an end-to-end evaluation [12]. Third, AI-assisted decision making \n",
      "is often listed as a major motivation for AI explanations. In recent \n",
      "years numerous papers have employed user studies to show that \n",
      "human accuracy increases if the AI system explains its reasoning \n",
      "for tasks as diverse as medical diagnosis, predicting loan defaults, \n",
      "and answering trivia questions. However, as summarized in Table \n",
      "1, complementary performance was not observed in any of these \n",
      "studies – in each case, adding the human to the loop decreased \n",
      "performance compared to if AI had acted alone. \n",
      "For example, in Lai et al. [42, 43], MTurk workers classifed de-\n",
      "ceptive hotel reviews with predictions from SVM and BERT-based \n",
      "models, as well as explanations in the form of inline-highlights. \n",
      "However, models outperformed every team (see Table 1 and Fig-\n",
      "ure 6 in [42]). Zhang et al. [83] noticed the superior behavior of \n",
      "the models in Lai et al. ’s work, and evaluated the accuracy and \n",
      "trust calibration where the gap between human and the AI per-\n",
      "formances was less severe. Still, on their task of income category \n",
      "1All the task examples and the collected experiment data are available at https://github. \n",
      "com/uw-hai/Complementary-Performance. prediction, their Gradient Boosted Trees model had 10% higher ac-\n",
      "curacy compared to their MTurk workers, which seemed borderline \n",
      "“comparable” at best. Furthermore, when run autonomously, their \n",
      "AI model performed just slightly better than the best team (see \n",
      "Section 4.2.2 and Figure 10 in [83]). A similar performance trend is \n",
      "observed on tasks other than classifcation. In Sangdeh et al. [65], \n",
      "MTurk workers predicted house price using various regression mod-\n",
      "els that generated explanations in terms of most salient features. \n",
      "Their models’ predictions resulted in lowest error (See Figure 6 \n",
      "in [65]). In Feng et al. [21], experts and novices played Quiz Bowl \n",
      "with recommendation from Elastic Search system. The system ex-\n",
      "plained its predictions by presenting training examples that were \n",
      "infuential, and using inline-highlights to explain the connection \n",
      "between question and evidence. However, Feng et al. do not report \n",
      "the exact performance of the AI on their study sample, but mention \n",
      "its superiority in Section 3.1 in [21] pointing out that it outper-\n",
      "forms top trivia players. One possible exception is Bligic & Mooney \n",
      "(2005) [10], who probably achieved complementary performance on \n",
      "their task of recommending books to users. However, they did not \n",
      "compare explanations against simple baselines, such as showing \n",
      "the book title or the system confdence (rating). \n",
      "At least two potential causes account for the absence of comple-\n",
      "mentary performance in these cases. First, task design may have \n",
      "hindered collaboration: previous researchers considered AI systems \n",
      "whose accuracy was substantially higher than the human’s, leading \n",
      "to a small zone with potential for complementary performance \n",
      "(see Figure 1). For example, this may have made it more likely that \n",
      "human errors were a superset of the AI’s, reducing the possibility of \n",
      "a human overseer spotting a machine mistake. Second, even when \n",
      "the task has the potential for complementary performance, it is \n",
      "unclear if the collaboration mechanisms under study supported it. \n",
      "Collaboration factors like incentives, the format of explanations, \n",
      "and whether AI’s uncertainty was displayed may drive the human \n",
      "towards simple, less collaborative heuristics, such as “always trust \n",
      "the AI” or “never trust the AI.” \n",
      "3 SETUP AND PILOT STUDIES \n",
      "To better understand the role of explanations in producing com-\n",
      "plementary performance, we enlarge the zone of potential comple-\n",
      "mentarity by matching AI accuracy to that of an average human,2 \n",
      "and investigate multiple explanation styles on several domains (Sec-\n",
      "tion 3.1). As Table 2 summarizes, we frst designed and conducted \n",
      "pilots studies (Sections 3.2) and used them to inform our fnal study \n",
      "and hypotheses (Section 4). \n",
      "3.1 Choice of Tasks and Explanations \n",
      "Since our interest is in AI-assisted decision making, we studied \n",
      "the efect of local explanations on team performance – that is, \n",
      "explaining each individual recommendation made by a model [66]. \n",
      "2Of course, complementary performance may be possible even in situations when one \n",
      "of the team partners is signifcantly more accurate than the other. For example, a low-\n",
      "accuracy team member may be valuable if their errors are independent, because they \n",
      "may be able to spot mistakes made by the team majority. However, it is more difcult \n",
      "to observe complementary performance in such settings, so we frst consider the case \n",
      "where humans and AI have similar accuracy. If explanations cannot provide value in \n",
      "such settings, it will be even more difcult to show complementary performance when \n",
      "teammates have disparate skills. CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al. \n",
      "Table 1: Recent studies that evaluate the efect of automatically generated explanations on human-AI team performance. \n",
      "While explanations did improve team accuracy, the performance was not complementary — acting autonomously, the AI \n",
      "would have performed even better. For papers with multiple domains or experiments, we took one sample with the most \n",
      "comparable human and AI performance. ↑ (or ↓) indicates whether the metric should be maximized (or minimized). \n",
      "Domain Task Performance \n",
      "Metric Human alone AI alone Team Complementary? \n",
      "Classifcation Deceptive review [43] \n",
      "Deceptive review [42] \n",
      "Income category [83] \n",
      "Loan defaults [27] \n",
      "Hypoxemia risk [53] \n",
      "Nutrition prediction [12] Accuracy ↑ \n",
      "Accuracy ↑ \n",
      "Accuracy ↑ \n",
      "Norm. Brier ↑ \n",
      "AUC ↑ \n",
      "Accuracy ↑ 51.1% 54.6% \n",
      "65% \n",
      "0 \n",
      "0.66 0.46 87.0% 86.3% \n",
      "75% \n",
      "1 \n",
      "0.81 0.75 74.6% 74.0% \n",
      "73% \n",
      "0.682 \n",
      "0.78 0.74 ✗ ✗ ✗ ✗ ✗ ✗ \n",
      "QA Quiz bowl [21] “AI outperforms top trivia players. ” ✗ \n",
      "Regression House price [65] Avg. Absolute Error ↓ $331k $200k $232k ✗ \n",
      "Table 2: An overview of our tasks, explanation strategies and \n",
      "sources. We ran our pilot studies (Section 3.2) with condi-\n",
      "tions marked with ●. Based on the pilot results, we added \n",
      "adaptive explanations and expert explanations (Section 3.3). \n",
      "Along with two additional domains, these form the condi-\n",
      "tions for our fnal study conditions (Section 4.1). \n",
      "Explain. Strategies Explain. Sources Tasks \n",
      "Explain-Top-1 ● \n",
      "Explain-Top-2 ● Adaptive AI ● Expert Beer ● \n",
      "Amzbook LSAT \n",
      "This contrasts with providing a global understanding of the full \n",
      "model all at once (e.g., [45]). \n",
      "We conducted experiments on two types of tasks: text classif-\n",
      "cation (sentiment analysis) and question answering. Text classif-\n",
      "cation because it is a popular task in natural language processing \n",
      "(NLP) that has been used in several previous studies on human-AI \n",
      "teaming [21, 29, 42, 50, 62, 83] and because it requires little domain \n",
      "expertise, and is thus amenable to crowdsourcing. Specifcally, we \n",
      "selected two sentiment analysis datasets to improve the generaliza-\n",
      "tion of our results: beer reviews [56] and book reviews [31]. More \n",
      "details about these datasets are in Section 4.2. While there exist \n",
      "various local explanation approaches for text classifcation, we rely \n",
      "on local saliency explanations, which explain a single prediction in \n",
      "terms of the importance of input features (e.g., each word) towards \n",
      "the model’s prediction (e.g., positive or negative sentiment). \n",
      "As commonly practiced in previous work [21, 42, 43], we dis-\n",
      "play explanations with inline-highlights, i.e., directly highlighting \n",
      "the explanation in the input text, so the user need not go back \n",
      "and forth between input and the explanation. While there exist \n",
      "other explanatory approaches, such as feature-importance visual-\n",
      "ization [11, 27, 53, 61, 76] (more suitable for tabular data) or com-\n",
      "municating infuential training examples [3, 40, 80] (more suitable \n",
      "for images), these techniques are not ideal for text because they \n",
      "add an additional cognitive cost to mapping the explanation to the \n",
      "respective text. Figure 2 shows one example beer review. We also experimented with Law School Admission Test (LSAT) \n",
      "questions3 because it is more challenging. In this task, every ques-\n",
      "tion contains four options with a unique correct answer (Figure 3). \n",
      "Again, answering LSAT questions requires no specialized knowl-\n",
      "edge except common-sense reasoning skills, such as recognizing \n",
      "logical connections and conficts between arguments [82]. Because \n",
      "in this case it is unclear how inline-highlights could be used to com-\n",
      "municate logical constructs (e.g., contradiction may not be visible \n",
      "by highlighting the input alone), we turned to narrative explana-\n",
      "tions which justify a candidate answer in natural language. We \n",
      "explain these in more detail in Section 4.2. \n",
      "3.2 Pilot Study on Sentiment Classifcation \n",
      "To iterate on the hypotheses and the associated explanation condi-\n",
      "tions for our main study (detailed later in Section 4), we conducted \n",
      "a pilot study on one of our datasets (Beer ). The between-subject \n",
      "pilot study asked crowdworkers to judge the sentiment of 50 beer \n",
      "reviews with assistance from a logistic regression classifer in three \n",
      "conditions, each condition with 50 workers. One condition only \n",
      "showed the model prediction and confdence; the other two also \n",
      "included the following common explanation strategies4: \n",
      "(1) Explain-Top-1 explains just the predicted class by highlight-\n",
      "ing the most infuential words for that class. \n",
      "(2) Explain-Top-2 explains the top two predicted classes, and un-\n",
      "like Explain-Top-1, it also color codes and highlights words \n",
      "for the other sentiment class. \n",
      "The two strategies closely align with the design in prior work [24, \n",
      "43, 49, 75], and have been shown to be benefcial (Table 1). Explain-\n",
      "Top-2 also corresponds to Wang et al. ’s suggestion to mitigate \n",
      "heuristic biases by explaining “multiple outcomes” [74]. \n",
      "Observations We summarize our fndings from the pilot study: \n",
      "(1) Contrary to many prior works, we observed no signifcant \n",
      "changes or improvements in aggregated team accuracy by \n",
      "displaying either type of explanations. \n",
      "3https://en.wikip edia.org/wiki/LawSchoolAdmissionTest \n",
      "4the saliency scores were based on feature weights learned by the linear model [27, 42] Does the Whole Exceed its Parts? The Efect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan \n",
      "(2) That said, explaining just the predicted class (Explain-Top-1) \n",
      "performed better than explaining both (Explain-Top-2). \n",
      "(3) We also observed that explanations increased reliance on rec-\n",
      "ommendations even when they were incorrect : explaining the \n",
      "predicted class slightly improved performance (compared to \n",
      "confdence only) when the recommendation was correct but \n",
      "decreased performance when it was incorrect. \n",
      "(4) This efect was less pronounced in Explain-Top-2, presum-\n",
      "ably because it encouraged users to consider alternatives and \n",
      "hence deterred over-reliance. In Figure 2, for example, if \n",
      "counter-argument (d) was not highlighted, participants could \n",
      "easily stop reading at the highlighted frst sentence and over-\n",
      "look the negative ending. \n",
      "(5) Finally, participants indicated that they wanted higher quality \n",
      "explanations. Crowd-workers were confused when explana-\n",
      "tions did not seem to correlate with model behavior. \n",
      "Because we made similar observations in our main study, we \n",
      "defer detailed discussions and implications of these observations to \n",
      "Section 5.1 and Figure 4. \n",
      "3.3 Additional Explanation Strategies/Sources \n",
      "Added Strategy: Adaptive Explanations. The pilot study indi-\n",
      "cated that Explain-Top-2 was more benefcial than Explain-Top-1 \n",
      "when the classifer made mistakes, but not otherwise. Relying on the \n",
      "commonly seen correlations between mistakes and low-confdence \n",
      "[32], we developed a new dynamic strategy, adaptive explanation, \n",
      "that switches between Explain-Top-1 and Explain-Top-2 depending \n",
      "on the AI’s confdence. This method explains the top-two classes \n",
      "only when the classifer confdence is below a task- and model-\n",
      "specifc threshold (described later in Section 4.2), explaining only \n",
      "the top prediction otherwise. Intuitively, it was inspired by an ef-\n",
      "cient assistant that divulges more information (confessing doubts \n",
      "and arguing for alternatives) only when it is unsure about its recom-\n",
      "mendation. Adaptive explanations can also be viewed as changing \n",
      "explanation according to context [1]. While we limit the our context \n",
      "to the AI’s confdence, in general, one could rely on more features \n",
      "of the human-AI team, such as the user, location, or time [34, 48]. \n",
      "Added Source: Expert-Generated Explanations. Users in our \n",
      "pilot study were confused when the explanations did not make in-\n",
      "tuitive sense, perhaps due to either the quality of the underlying \n",
      "linear model-based AI. While we test state-of-the-art models in \n",
      "the fnal study, we also added expert-generated explanations to \n",
      "serve as an upper bound on explanation quality. We describe their \n",
      "annotation process in Section 4.2. \n",
      "4 FINAL STUDY \n",
      "Based on our pilot studies, we formulated our fnal hypotheses and \n",
      "used them to inform our fnal conditions and their interface (Sec-\n",
      "tion 4.1). We then tested these hypotheses for several tasks and AI \n",
      "systems (Section 4.2) through crowdsourcing studies (Section 4.3). \n",
      "4.1 Hypotheses, Conditions, and Interface \n",
      "We formulated the following hypotheses for sentiment analysis: \n",
      "H1 Among current explanation strategies, explaining the predicted \n",
      "class will perform better than explaining both classes. H2 The better strategies, Explain-Top-1, will still perform similarly \n",
      "to simply showing confdence. \n",
      "H3 Our proposed Adaptive explanations, which combines benefts \n",
      "of existing strategies, will improve performance. \n",
      "H4 Adaptive explanations would perform even better if AI could \n",
      "generate higher quality explanations. \n",
      "Since generating AI explanations for LSAT was not feasible (Sec-\n",
      "tion 3.3), we slightly modifed the hypothesis for LSAT : we omitted \n",
      "the hypothesis on explanation quality (4.1) and tested the frst three \n",
      "hypotheses using expert- rather than AI-generated explanations. \n",
      "Conditions . For both domains, we ran two baseline conditions: \n",
      "unassisted users (Human ), as well as a simple AI assistance that \n",
      "shows the AI’s recommendation and confdence but no explanation \n",
      "(Team (Conf) ). We use this simple assistance because it serves as \n",
      "a stronger and broadly acknowledged baseline than the alterna-\n",
      "tive, i.e., displaying AI’s recommendation without confdence. First, \n",
      "most ML models can generate confdence scores that, in practice, \n",
      "correlate with the model’s true likelihood to err [32]. Second, dis-\n",
      "playing uncertainty in predictions can help users can make more \n",
      "optimal decisions [19, 22, 25, 37, 60]. Hence, we focus on evaluating \n",
      "whether the explanations provide additional value when shown \n",
      "alongside confdence scores. In the rest of the paper, we indicate the \n",
      "explanation conditions using the following template: Team (Strat-\n",
      "egy, Source) . For example, Team (Explain-Top-1, AI) indicates the \n",
      "condition that shows the AI’s explanations for the top prediction. \n",
      "Interface . Figure 2 shows an example UI for sentiment clas-\n",
      "sifcation for Team (Adaptive, AI). In all explanation conditions, \n",
      "explanations are displayed as inline highlights, with the background \n",
      "color aligned with the positive/negative label buttons. The highlight \n",
      "varies by condition, e.g., Team (Adaptive, AI) has a similar display to \n",
      "Figure 2, except that the AI picks multiple short phrases, instead of \n",
      "a full sentence. In Team (Explain-Top-1, AI) the counter-argument \n",
      "(d) is always missing, and in Team (Conf) no explanations are high-\n",
      "lighted. Figure 3 shows a screenshot of the user interface for LSAT \n",
      "in the Team (Adaptive, Expert) condition. \n",
      "4.2 AI Model, Study Samples and Explanations \n",
      "4.2.1 Sentiment Classification. \n",
      "Training data. To prepare each dataset (Beer and Amzbook ) for \n",
      "training classifcation models, we binarized the target labels, split \n",
      "the dataset into training and test sets (80/20 split), removed class \n",
      "imbalance from the train split by oversampling the minority class, \n",
      "and further split the training set to create a validation set. \n",
      "AI Model. For each dataset, we fne-tuned a RoBERTa-based [52] \n",
      "text classifer from AllenNLP5 on the training dataset and per-\n",
      "formed hyper-parameter selection on the validation set. \n",
      "Task examples. For each domain, we selected 50 examples from \n",
      "the test set to create our study sample. We frst conducted additional \n",
      "pilot studies to establish the accuracy of unassisted users, which we \n",
      "observed were 87% for Beer and 85% for Amzbook6 We then selected \n",
      "50 unambiguous examples so that the AI’s accuracy was 84% (i.e., \n",
      "comparable to human accuracy), with equal false positive and false \n",
      "5https://demo.allennlp.org/sentiment-analysis \n",
      "6Again, each condition containing 50 crowd-workers. We estimated the human accu-\n",
      "racy on all the three datasets with another 150 crowd-workers. CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al. \n",
      "a\n",
      "bc\n",
      "d\n",
      "Figure 2: A screenshot of the Team (Adaptive, Expert) condition for the Amzbook reviews dataset. Participants read the review \n",
      "(left pane) and used the buttons (right pane) to decide if the review was mostly positive or negative. The right pane also shows \n",
      "progress and accuracy (a). To make a recommendation, the AI (called “Marvin”) hovers above a button (b) and displays the \n",
      "confdence score under the button. In this case, the AI incorrectly recommended that this review was positive, with confdence \n",
      "62.7%. As part of the explanation, the AI highlighted the most positive sentence (c) in the same color as the positive button. \n",
      "Because confdence was low, the AI also highlights the most negative sentence (d) to provide a counter-argument. \n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "0 / 20\n",
      "Figure 3: A screenshot of Team (Adaptive, Expert) for LSAT . Similar to Figure 2, the interface contained a progress indicator (a), \n",
      "AI recommendation (b), and explanations for the top-2 predictions (c and d). To discourage participants from blindly following \n",
      "the AI, all AI information is displayed on the right. In (b), the confdence score is scaled so those for top-2 classes sum to 100%. \n",
      "negative rates. The fltering was for keeping the task objective: \n",
      "If the ground-truth answer was unclear, one cannot compute or \n",
      "compare the accuracy of decisions. \n",
      "Explanations. To generate saliency explanations, we used LIME, \n",
      "which is a popular post hoc method [66]. We chose this setup \n",
      "because the combination of RoBERTa and LIME was consistently \n",
      "ranked the highest among the various systems we tried in an ex-\n",
      "plainer comparison study with human judges (details in Appendix). \n",
      "Despite ofering accurate predictions, RoBERTa generated poorly \n",
      "calibrated confdence scores, a common issue with neural networks \n",
      "[28], which we mitigated with post hoc calibration (isotonic regres-\n",
      "sion [7]) on the validation set. \n",
      "In particular, for Adaptive explanation, we used the classifer’s \n",
      "median confdence as the threshold to have an equal number of \n",
      "25 examples displayed as Explain-Top-1 and Explain-Top-2, respec-\n",
      "tively. The thresholds were 89.2% for Beer and 88.9% for Amzbook . We happened to explain 18 correctly predicted and 7 incorrectly \n",
      "predicted examples with Explain-Top-2 for both datasets (leaving \n",
      "1 incorrect and 24 correct cases with Explain-Top-1). While one \n",
      "might learn a better threshold from the data, we leave that to future \n",
      "work. As for expert-generated explanations, one author created \n",
      "expert explanations by selecting one short, convincing text phrase \n",
      "span for each class (positive or negative). \n",
      "4.2.2 LSAT. \n",
      "AI Model. We fnetuned a RoBERTa model7 on ReClor [82], a logic-\n",
      "reasoning dataset that contains questions from standardized exams \n",
      "like the LSAT and GMAT.8 \n",
      "Task examples. We selected 20 examples from an LSAT prep \n",
      "book [71]. We verifed that our questions were not easily searchable \n",
      "7Based on the op ensource implementation: https://github.com/yuweihao/reclor. \n",
      "8https://en.wikipedia.org/wiki/GraduateManagementAdmissionTest Does the Whole Exceed its Parts? The Efect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan \n",
      "online and were not included in the training dataset. We selected \n",
      "fewer LSAT questions than for sentiment analysis, because they \n",
      "are more time consuming to answer and could fatigue participants: \n",
      "LSAT questions took around a minute to answer, compared to \n",
      "around 17 seconds for Beer and Amzbook . The RoBERTa model \n",
      "achieved 65% accuracy on these examples, comparable to the 67% \n",
      "human accuracy that we observed in our pilot study. \n",
      "Explanations. We found no automated method that could gen-\n",
      "erate reasonable explanations (unsurprising, given that explana-\n",
      "tions rely on prior knowledge and complex reasoning); instead, we \n",
      "used expert explanations exclusively, which is again based on the \n",
      "prep book. The book contains explanations for the correct answer, \n",
      "which one author condensed to a maximum of two sentences. Since \n",
      "the book did not provide explanations for alternative choices, we \n",
      "created these by manually crafting a logical supporting argument \n",
      "for each choice that adhered to the tone and level of conciseness of \n",
      "the other explanations. Experts only generated explanations and \n",
      "did not determine the model predictions or its uncertainties. \n",
      "4.3 Study Procedure \n",
      "Sentiment Classifcation . For the fnal study, participants went \n",
      "through the following steps: 1) A landing page frst explained the \n",
      "payment scheme; the classifcation task was presented (here, pre-\n",
      "dicting the sentiment of reviews); and they were shown dataset-\n",
      "specifc examples. 2) To familiarize them with the task and verify \n",
      "their understanding, a screening phase required the participant \n",
      "to correctly label four of six reviews [51]. Only participants who \n",
      "passed the gating test were allowed to proceed to the main task. 3) \n",
      "The main task randomly assigned participants to one of our study \n",
      "conditions (Section 3.3) and presented condition-specifc instruc-\n",
      "tions, including the meaning and positioning of AI’s prediction, \n",
      "confdence, and explanations. Participants then labeled all 50 study \n",
      "samples one-by-one. For a given dataset, all conditions used the \n",
      "same ordering of examples. The participants received immediate \n",
      "feedback on their correctness after each round of the task. 4) A \n",
      "post-task survey was administered, asking whether they found the \n",
      "model assistance to be helpful, their rating of the usefulness of \n",
      "explanations in particular (if they were present), and their strategy \n",
      "for using model assistance. \n",
      "We recruited participants from Amazon’s Mechanical Turk, lim-\n",
      "iting the pool to subjects from within the United States with a \n",
      "prior task approval rating of at least 97% and a minimum of 1,000 \n",
      "approved tasks. To ensure data quality, we removed data from par-\n",
      "ticipants whose median labeling time was less than 2 seconds or \n",
      "those who assigned the same label to all examples. In total, we \n",
      "recruited 566 (Beer ) and 552 (Amzbook ) crowd workers, and in both \n",
      "datasets, 84% of participants passed the screening and post-fltering. \n",
      "Eventually, we collected data from around 100 participants (ranging \n",
      "from 93 to 101 due to fltering) per condition. \n",
      "Study participants received a base pay of $0.50 for participating, \n",
      "a performance-based bonus for the main task, and a fxed bonus of \n",
      "$0.25 for completing the survey. Our performance-based bonus was \n",
      "a combination of linear and step functions on accuracy: we gave \n",
      "$0.05 for every correct decision in addition to an extra $0.50 if the to-\n",
      "tal accuracy exceeded 90% or $1.00 if it exceeded 95%. The assigned \n",
      "additional bonuses were intended to motivate workers to strive for performance in the complementary zone and improve over the AI-\n",
      "only performance [33]. Since we fxed the AI performance at 84%, \n",
      "humans could not obtain the bonus by blindly following the AI’s \n",
      "recommendations. Participants spent 13 minutes on average on the \n",
      "experiment and received an average payment of $3.35 (equivalent \n",
      "to an hourly wage of $15.77). \n",
      "Modifcations for LSAT . For the LSAT dataset, we used a very \n",
      "similar procedure but used two screening questions and required \n",
      "workers to answer both correctly. We used a stricter passing re-\n",
      "quirement to avoid low-quality workers who might cheat, which \n",
      "we observed more for this task in our pilots. We again used MTurk \n",
      "with the same flters as sentiment classifcation, and we post hoc \n",
      "removed data from participants whose median response time was \n",
      "less than three seconds. 508 crowd workers participated in our \n",
      "study, 35% of whom passed the screening and completed the main \n",
      "task, resulting in a total of 100 participants per condition. \n",
      "Participants received a base pay of $0.50 for participating, a \n",
      "performance-based bonus of $0.30 for each correct answer in the \n",
      "main task, and a fxed bonus of $0.25 for completing an exit survey. \n",
      "They received an additional bonus of $1.00, $2.00, and $3.00 for \n",
      "reaching an overall accuracy of 30%, 50%, and 85% to motivate \n",
      "workers to answer more questions correctly and perform their best. \n",
      "The average completion time for the LSAT task was 16 minutes, \n",
      "with an average payment of $6.30 (equals an hourly wage of $23.34). \n",
      "5 RESULTS \n",
      "5.1 Efect of Explanation on Team performance \n",
      "Figure 4A shows the team performance (i.e., accuracy of fnal deci-\n",
      "sion) for each domain and condition. We tested the signifcance of \n",
      "our results using Student’s T-tests with Bonferroni correction. \n",
      "The baseline team condition, Team (Conf), achieved com-\n",
      "plementary performance across tasks. For Beer, providing AI \n",
      "recommendations and confdence to users increased their perfor-\n",
      "mance to (µ = 0.89 ± σ = 0.05), surpassing both AI (0.84) and \n",
      "unassisted human accuracy (0.82 ± 0.09). Similarly, Team (Conf) \n",
      "achieved complementary performance for Amzbook and LSAT , with \n",
      "relative gains of 2.2% and 20.1% over unassisted workers (Figure 4A). \n",
      "We did not observe a signifcant diference between Explain-\n",
      "Top-1 and Explain-Top-2 , or that 4.1 was not supported. For ex-\n",
      "ample, in Figure 4A of Beer, explaining the top prediction performed \n",
      "marginally better than explaining the top-two predictions, but the \n",
      "diference was not signifcant ( z=0.85, p=.40). The same was true \n",
      "for Amzbook (z=0.81, p=.42) and LSAT (z=0.42, p=.68). \n",
      "We did not observe signifcant improvements over the con-\n",
      "fdence baseline by displaying explanations. For example, for \n",
      "Beer, Team (Conf) and Team (Explain-Top-1, AI) achieved similar \n",
      "performance, with the accuracy being 0.89 ± 0.05 vs. 0.88 ± 0.06 \n",
      "respectively; the diference was insignifcant (z=-1.18, p=.24). We \n",
      "observed the same pattern for Amzbook (z=1.23, p=.22) and LSAT \n",
      "(z=0.427, p=.64). As a result, we could not reject our hypothesis 4.1 \n",
      "that Explain-Top-1 performs similar to simply showing confdence \n",
      ". This result motivates the need to develop new AI systems and ex-\n",
      "planation methods that provide true value to team performance by \n",
      "supplementing the model’s confdence, perhaps working in tandem \n",
      "with confdence scores. CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al. \n",
      "0.75 0.80 0.85 0.90 0.95 1.00\n",
      "Decision A ccuracyHuman\n",
      "Team (C onf)\n",
      "Team (Explain-T op-1, AI)\n",
      "Team (Explain-T op-2, AI)\n",
      "Team (A daptive, AI)\n",
      "Team (A daptive, Expert)conditionBeer Decision Accur acy\n",
      "0.75 0.80 0.85 0.90 0.95 1.00\n",
      "Decision A ccuracyHuman\n",
      "Team (C onf)\n",
      "Team (Explain-T op-1, AI)\n",
      "Team (Explain-T op-2, AI)\n",
      "Team (A daptive, AI)\n",
      "Team (A daptive, Expert)conditionAmzbook Decision Accuracy\n",
      "0.50 0.55 0.60 0.65 0.70 0.75\n",
      "Decision A ccuracyHuman\n",
      "Team (C onf)\n",
      "Team (Explain T op-1, Expert)\n",
      "Team (Explain T op-2, Expert)\n",
      "Team (A daptive, Expert)conditionLSAT D ecision Accuracy0.4 0.5 0.6 0.7 0.8 0.9 1.0\n",
      "Decision A ccuracyHuman\n",
      "Team (C onf)\n",
      "Team (Explain-T op-1, AI)\n",
      "Team (Explain-T op-2, AI)\n",
      "Team (A daptive, AI)\n",
      "Team (A daptive, Expert)conditionBeer Decision Accur acy (sp lit)\n",
      "0.4 0.5 0.6 0.7 0.8 0.9 1.0\n",
      "Decision A ccuracyHuman\n",
      "Team (C onf)\n",
      "Team (Explain-T op-1, AI)\n",
      "Team (Explain-T op-2, AI)\n",
      "Team (A daptive, AI)\n",
      "Team (A daptive, Expert)conditionAmzbook Decision Accuracy (sp lit)\n",
      "0.3 0.4 0.5 0.6 0.7 0.8 0.9\n",
      "Decision A ccuracyHuman\n",
      "Team (C onf)\n",
      "Team (Explain T op-1, Expert)\n",
      "Team (Explain T op-2, Expert)\n",
      "Team (A daptive, Expert)conditionLSAT D ecision Accuracy (sp lit)correct inco rrect AI reco mmendation AI accuracy Complementary zoneA B\n",
      "Figure 4: Team performance (with average accuracy and 95% confdence interval) achieved by diferent explanation conditions \n",
      "and baselines for three datasets, with around 100 participants per condition. (A) Across every dataset, all team conditions \n",
      "achieved complementary performance. However, we did not observe signifcant improvements from using explanations over \n",
      "simply showing confdence scores. (B) Splitting the analysis based on the correctness of AI accuracy, we saw that for Beer and \n",
      "LSAT , Explain-Top-1 explanations worsened performance when the AI was incorrect, the impact of Explain-Top-1 and Explain-\n",
      "Top-2 explanations were correlated with the correctness of the AI’s recommendation, and Adaptive explanations seemed to \n",
      "have the potential to improve Explain-Top-1 when the AI was incorrect, and to retain the higher performance of Explain-Top-1 \n",
      "when the AI was correct. \n",
      "Though designed to alleviate the limitations of Explain-Top-1 \n",
      "and Explain-Top-2 in our experiments, we did not observe im-\n",
      "provements from using Adaptive explanations. For example, \n",
      "we did not observe any signifcant diferences between Team (Adap-\n",
      "tive, AI) and Team (Conf) for Beer (z=-1.02, p=.31) or Amzbook \n",
      "(z=1.08, p=.28). We did not observe signifcant diferences between \n",
      "Team (Adaptive, Expert) and Team (Conf) for LSAT (z=0.16, p=.87). \n",
      "More surprisingly, switching the source of Adaptive explanation to \n",
      "expert-generated did not signifcantly improve sentiment analysis \n",
      "results. For example, in Figure 4A, the diferences in performance \n",
      "between Team (Adaptive, Expert) and Team (Adaptive, AI) were \n",
      "insignifcant: Beer (z=1.31, p=.19) and Amzbook (z=-0.78, p=.43). As \n",
      "such, we could not reject the null hypotheses for either 4.1 or 4.1. \n",
      "While Adaptive explanation did not signifcantly improve \n",
      "team performance across domains, further analysis may \n",
      "point a way forward by combining the strengths of Explain-\n",
      "Top-1 and Explain-Top-2. Split the team performance by whether \n",
      "the AI made a mistake (Figure 4B), we observe that explaining the \n",
      "top prediction lead to better accuracy when the AI recommendation was correct but worse when the AI was incorrect, as in our pilot \n",
      "study. This is consistent with Psychology literature [39], which has \n",
      "shown that human explanations cause listeners to agree even when \n",
      "the explanation is wrong, and recent studies that showed explana-\n",
      "tions can mislead data scientists into overtrusting ML models for \n",
      "deployment [38]. While these results were obtained by measuring \n",
      "user’s subjective ratings of trust, to the best of our knowledge, our \n",
      "studies are the frst to show this phenomenon for explanation and \n",
      "end-to-end decision making with large-scale studies. As expected, \n",
      "in Beer, Adaptive explanations improved performance over Explain-\n",
      "Top-1 when the AI was incorrect and improved performance over \n",
      "Explain-Top-2 when the AI was correct, although the efect was \n",
      "smaller on other datasets. \n",
      "While Figure 4B shows team performance, the promising efects \n",
      "of Adaptive explanations are clearer if we study the agreement \n",
      "between AI predictions and human decisions (Figure 5). Adaptive \n",
      "explanations seem to encourage participants to consider the AI \n",
      "more when it is confdent and solve the task themselves other-\n",
      "wise. Unfortunately, as our experiments show, the efect of using Does the Whole Exceed its Parts? The Efect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan \n",
      ">= threshold<   thresholdPass adaptive threshold?Explain-Top-2Explain-Top-1Display\n",
      "0.6 0.7 0.8 0.9 1.0Team ( Explain-T op-1 , AI)\n",
      "Team ( Explain-T op-2 , AI)\n",
      "Team ( Adaptive , AI)\n",
      "Team ( Adaptive , Expert)condition\n",
      "0.6 0.7 0.8 0.9 1.0Team ( Explain-T op-1 , AI)\n",
      "Team ( Explain-T op-2 , AI)\n",
      "Team ( Adaptive , AI)\n",
      "Team ( Adaptive , Expert)condition\n",
      "0.5 0.6 0.7 0.8 0.9\n",
      "Human-AI AgreementTeam ( Explain T op-1 , Expert)\n",
      "Team ( Explain T op-2, Expert)\n",
      "Team ( Adaptive , Expert)conditionBeer\n",
      "Amzbook\n",
      "LSAT\n",
      "--\n",
      "Figure 5: Relative agreement rates between humans and AI (i.e., does the fnal human decision match the AI’s suggestion?) for \n",
      "various conditions, with examples split by whether AI’s confdence exceeded the threshold used for Adaptive explanations. \n",
      "Across the three datasets, Adaptive explanations successfully reduced the human’s tendency to blindly trust the AI (i.e., de-\n",
      "creased agreement) when it was uncertain and more likely to be incorrect. For example, comparing Team (Explain-Top-1, AI) \n",
      "and Team (Adaptive, AI) on low confdence examples that did not pass the threshold (rectangles), participants in Explain-Top-2 \n",
      "(pink rectangles) were less likely to agree with the AI compared to those who saw Explain-Top-1 (blue rectangles). \n",
      "0.00 0.20 0.40 0.60 0.80 1.00\n",
      "Human a ccuracy (Binned)0%20%40%% ExamplesBeer\n",
      "0.00 0.20 0.40 0.60 0.80 1.00\n",
      "Human a ccuracy (Binned)0%20%40%60%% ExamplesAmzbook\n",
      "0.00 0.20 0.40 0.60 0.80 1.00\n",
      "Human a ccuracy (Binned)0%5%10%15%20%% ExamplesLSATcorrect inco rrect AI Reco mmendation\n",
      "Figure 6: The distribution of study examples as a function of average human accuracy. For each domain, examples on the right \n",
      "were easy for most humans working alone. Both Beer and LSAT show a distribution that shows potential for complementary \n",
      "team performance: humans can correct easy questions mistaken by the AI (red bars towards the right), and, conversely, the \n",
      "AI may add value on examples where humans frequently err (green bars towards the left). In contrast, Amzbook showed less \n",
      "potential for this kind of human-AI synergy, with less “easy for human” questions (bars towards the left). \n",
      "Adaptive did not seem sufcient to increase the fnal team accuracy, \n",
      "possibly for two reasons: (1) in high confdence regions (circles \n",
      "in Figure 5), not only did workers have to agree more, but they \n",
      "also had to identify cases where the model failed with very high \n",
      "confdence (unknown unknowns [44]). Identifying unknown un-\n",
      "knowns could have been a difcult and time-consuming task for \n",
      "workers, and they may have needed other types of support that we \n",
      "did not provide. (2) In low confdence regions (rectangles), not only \n",
      "did workers have to disagree more, but they also had to be able to \n",
      "solve the task correctly when they disagreed. Explain-Top-2 expla-\n",
      "nations might have enabled them to suspect the model more, but it \n",
      "is unclear if they helped participants make the right decisions. This indicates that more sophisticated strategies are needed to support \n",
      "humans in both situations. We discuss some potential strategies in \n",
      "Section 6.3. \n",
      "Diferences in expertise between human and AI afects \n",
      "whether (or how much) AI assistance will help achieve com-\n",
      "plementary performance. To understand how diferences in ex-\n",
      "pertise between the human and AI impact team performance, we \n",
      "computed the average accuracy of unassisted users on study exam-\n",
      "ples and overlaid the AI’s expertise (whether the recommendation \n",
      "was correct) in Figure 6. The fgure helps explain why users ben-\n",
      "efted more from AI recommendations for both Beer and LSAT \n",
      "datasets. There was a signifcant fraction of examples that the AI CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al. \n",
      "Team (C onf)\n",
      "Team (Explain-T op-1, AI)\n",
      "Team (Explain-T op-2, AI)\n",
      "Team (A daptive, AI)\n",
      "Team (A daptive, Expert)ConditionBeer: AI Assist ant is Useful Beer: Expl anation is Useful\n",
      "-50% 0% 50%\n",
      "% Participants-50% 0% 50%\n",
      "% Participants\n",
      "Team (C onf)\n",
      "Team (Explain-T op-1, AI)\n",
      "Team (Explain-T op-2, AI)\n",
      "Team (A daptive, AI)\n",
      "Team (A daptive, Expert)ConditionAmzbook: AI Assist ant is Useful Amzbook: Expl anation is Useful\n",
      "-50% 0% 50%\n",
      "% Participants-50% 0% 50%\n",
      "% Participants\n",
      "Team (C onf)\n",
      "Team (Explain T op-1, Expert)\n",
      "Team (Explain T op-2, Expert)\n",
      "Team (A daptive, Expert)ConditionLSAT : AI Assistant is Useful LSAT : Expl anation is Useful\n",
      "-50% 0% 50%\n",
      "% Participants-50% 0% 50%\n",
      "% ParticipantsStrongly disagree Disagree Neither agree nor disagree Agree Strongly agree Response\n",
      "B A\n",
      "Figure 7: Analysis of participant responses to two statements: (A) “AI’s assistance (e.g., the information it displayed) helped me \n",
      "solve the task”, and (B) “AI’s explanations in particular helped me solve the task.” Across datasets, a majority of participants \n",
      "found AI assistant to be useful, and they rated all the conditions similarly, with a slight preference towards Team (Adaptive, \n",
      "Expert). In contrast to AI’s overall usefulness, fewer participants rated explanations as useful, particularly Explain-Top-2 \n",
      "explanations. Participants also had a clearer preference for higher-quality (expert) Adaptive explanations. \n",
      "predicted correctly but humans struggled with (green bars to the \n",
      "left), while the same was not true for Amzbook (where AI rec-\n",
      "ommendations did not help as much). Further, when the AI was \n",
      "incorrect, explaining predictions on Amzbook via Explain-Top-1 \n",
      "improved the performance by 5% over showing confdence (Fig-\n",
      "ure 4B), but it decreased the performance for Beer and LSAT . One \n",
      "possible explanation is that most AI mistakes were predicted cor-\n",
      "rectly by most humans on Amzbook (red bars were mostly towards \n",
      "the right). After observing clear model mistakes, participants may \n",
      "have learned to rely on them less, despite the convincing-efect of \n",
      "explanation. Participants’ self-reported collaboration approaches \n",
      "supported our guess – Amzbook participants reportedly ignored the \n",
      "AI’s assistance the most (Section 5.3). That said, other confounding \n",
      "efects such as the nature of the task (e.g., binary classifcation vs. \n",
      "choosing between multiple options) should also be studied. \n",
      "5.2 Survey Responses on Likert Scale Questions \n",
      "Two of the questions in our post-task survey requested categorical \n",
      "ratings of AI and explanation usefulness.9 \n",
      "AI usefulness : While participants generally rated AI assistance \n",
      "useful (Figure 7A), the improvements in ratings between most ex-\n",
      "planations and simply showing confdence were marginal. The \n",
      "9Since we did not pre-register hypotheses for the subjective ratings and only analyzed \n",
      "them post-hoc, we do not perform/claim statistical signifcant analysis on these metrics. diference was more clear for high-quality adaptive explanations; \n",
      "for Beer, 70% of the participants rated AI assistance useful with \n",
      "Team (Adaptive, Expert) in contrast to 57% with Team (Conf). We \n",
      "observed a similar pattern on Amzbook (65% vs. 49%) and LSAT \n",
      "(63% vs. 45%), though on LSAT , Team (Explain-Top-2, Expert) re-\n",
      "ceived slightly higher ratings than Team (Adaptive, Expert) (66% vs. \n",
      "63%). \n",
      "Explanation usefulness : Figure 7B shows that participants’ \n",
      "ratings for the usefulness of explanations were lower than the \n",
      "overall usefulness of AI’s assistance (in A). Again, expert-generated \n",
      "Adaptive explanations received higher ratings than AI-generated \n",
      "ones for Beer (53% vs. 38%) vs. Amzbook (50% vs. 40%). This could \n",
      "indicate that showing higher quality explanations improves users’ \n",
      "perceived helpfulness of the system. However, it is worth noting \n",
      "that this increased preference did not translate to an improvement in \n",
      "team performance, which is consistent with observations made by \n",
      "Buçinca et al. [12] that show that people may prefer one explanation \n",
      "but make better decisions with another. \n",
      "5.3 Qualitative Analysis on Collaboration \n",
      "To better understand how users collaborated with the AI in diferent \n",
      "tasks, we coded their response to the prompt: “Describe how you \n",
      "used the information Marvin (the AI) provided.” Two annotators \n",
      "independently read a subset of the responses to identify emergent Does the Whole Exceed its Parts? The Efect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan \n",
      "Table 3: The codebook for participants’ descriptions of how they used the AI, with the number of self-reports. \n",
      "Codes Defnitions and Examples #Participants \n",
      "Overall Collaboration Approach (codes are mutually exclusive) \n",
      "Mostly Follow AI The participant mostly followed the AI. \n",
      "“I went with Marvin most times. ” 23 (6%) \n",
      "AI as Prior Guide \n",
      "AI as Post Check \n",
      "Mostly Ignore AI Used AI as a starting reference point. \n",
      "“I looked at his prediction and then I read the passage. ” \n",
      "Double-checked after they made their own decisions. \n",
      "“I ignored it until I made my decision and then verifed what it said. ” \n",
      "Mostly made their own decisions without the AI. \n",
      "“I didn’t. I fgured out the paragraph for myself. ” 190 (47%) \n",
      "102 (25%) \n",
      "90 (22%) \n",
      "The Usage of Explanation (codes can overlap) \n",
      "Used Expl. Explicitly acknowledged they used the explanation. \n",
      "“I skimmed his highlighted words. ” \n",
      "Speed Read Used explanations to quickly skim through the example. \n",
      "“I looked at Marvin’s review initially then speed read the review. ” \n",
      "Validate AI Used the explanation to validate AI’s reasoning. \n",
      "“Marvin focuses on the wrong points at times. This made me \n",
      "cautious when taking Marvin’s advice. ” 138 (42%) \n",
      "29 (9%) \n",
      "17 (5%) \n",
      "The Usage of Confdence (codes can overlap) Used Conf. Explicitly acknowledged they used the confdence. \n",
      "“I mostly relied on Marvin’s confdent levels to guide me. ” \n",
      "Conf. Threshold Was more likely to accept AI above the threshold. \n",
      "“If Marvin was above 85% confdence, I took his word for it. ” 90 (22%) \n",
      "24 (6%) \n",
      "Others (codes can overlap) \n",
      "Fall Back to AI \n",
      "Updated Strategy Followed the AI’s label if they failed to decide. \n",
      "“I used it if I was unsure of my own decision. ” \n",
      "Changed their strategy as they interacted more. \n",
      "“I decided myself after seeing that sometimes Marvin failed me. ” 54 (13%) \n",
      "12 (2%) \n",
      "codes and, using a discussion period, created a codebook (Table 3). \n",
      "Using this codebook, for each team condition and dataset, they \n",
      "coded a sample of 30 random worker responses: 28 were unique \n",
      "and 2 overlapped between annotators, allowing us to compute inter-\n",
      "annotator agreement. Our fnal analysis used 409 unique responses \n",
      "after removing 11 responses deemed to be of poor quality (Table 3). \n",
      "We scored the inter-annotator agreement with both the Cohen’s κ \n",
      "and the raw overlap between the coding. We achieved reasonably \n",
      "high agreements, with an average µ(κ) = 0.71, σ (κ) = 0.18 (the \n",
      "average agreement was 93% ± 6.5%). We noticed the following, \n",
      "which echo the performance diferences observed across datasets: \n",
      "Most participants used the AI’s recommendation as a prior \n",
      "or to double-check their answers. For all datasets, more than \n",
      "70% of the participants mentioned they would partially take AI’s \n",
      "recommendation into consideration rather than blindly following \n",
      "AI or fully ignoring it (Figure 8). Participants used the AI as a prior \n",
      "guide more than as a post-check for sentiment analysis, but not \n",
      "for LSAT , which aligns with our interface design: for LSAT , AI \n",
      "recommendations were on a separate pane, encouraging users to \n",
      "solve the task on their own before consulting the AI. Participants ignored the AI more on domains where AI \n",
      "expertise did not supplement their expertise. Figure 8 shows \n",
      "that while only 11% of LSAT participants claimed that they mostly \n",
      "ignored the AI, the ratio doubled (Beer , 23%) or even tripled (Amz-\n",
      "book , 30%) for sentiment analysis. As discussed in Figure 6, this \n",
      "may be due to correlation diferences between human and AI errors \n",
      "for diferent datasets: Amzbook participants were less likely to see \n",
      "cases where AI was more correct than they were, and therefore they \n",
      "may have learned to rely less on it. For example, one participant in \n",
      "Amzbook mentioned, “I had initially tried to take Marvin’s advice \n",
      "into account for a few rounds, and stopped after I got 2 incorrect \n",
      "answers. After that I read all of the reviews carefully and followed \n",
      "my own discretion. ” \n",
      "In contrast, a Beer participant relied more on the AI once realizing \n",
      "it could be correct: “At frst I tried reading the passages and making \n",
      "my own judgments, but then I got several items wrong. After that, I \n",
      "just switched to going with Marvin’s recommendation every time. ” \n",
      "In addition to the user’s collaboration behavior, these difer-\n",
      "ences between domains may have afected our quantitative ob-\n",
      "servations of team performance. For example, a small diference CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al. \n",
      "Mostly Fo llow AI\n",
      "AI as Prior Guide\n",
      "AI as Post C heck\n",
      "Mostly I gnore AICollaborationBeer Amzbook LSAT\n",
      "0%10% 20% 30% 40% 50%\n",
      "% Participants0%10% 20% 30% 40% 50%\n",
      "% Participants0%10% 20% 30% 40% 50%\n",
      "% Participants\n",
      "Figure 8: Instead of ignoring or strictly following the AI, participants reported taking the AI information into consideration \n",
      "most of the time. They most frequently used AI as a prior guide in sentiment analysis, but used it as post-check in LSAT . They \n",
      "were also more likely to ignore the AI in sentiment analysis than in LSAT . \n",
      "Conf.\n",
      "Conf.+Expl.conditionAmzbook Beer LSAT\n",
      "0% 5% 10%\n",
      "% Participants0% 5% 10% 15%\n",
      "% Participants0% 2% 4% 6%\n",
      "% ParticipantsConf.\n",
      "Conf.+Expl.condition\n",
      "Figure 9: Comparing the occurrence of Used Conf. in just the confdence condition and in those with explanations, we saw a \n",
      "similar proportion of users that explicitly acknowledged using confdence, regardless of whether they saw an explanation. \n",
      "between human and AI expertise (distribution of errors) means \n",
      "that the improvement in performance when the AI is correct would \n",
      "be less substantial. In fact, in Figure 4B, if we compare the team \n",
      "performance when the AI is correct, the diference between team \n",
      "conditions and the human baseline is least substantial for Amzbook . \n",
      "Some participants developed mental models of the AI’s \n",
      "confdence score to determine when to trust the AI. Among \n",
      "participants who mentioned they used confdence scores (90 in \n",
      "total), 27% reported using an explicit confdence threshold, below \n",
      "which they were likely to distrust the AI. The threshold mostly \n",
      "varied between 80 to 100 (83 ± 8 for Beer, 89 ± 7 for Amzbook , and \n",
      "90 ± 0 for LSAT ) but could go as low as 65, indicating that users \n",
      "built diferent mental models about when they considered AI to \n",
      "be “trustworthy.” While this observation empirically shows that \n",
      "end-users develop mental model of trust in AI-assisted decision \n",
      "making [5], it more importantly shows how the AI’s confdence is \n",
      "a simple, yet salient feature via which users create a mental model \n",
      "of the AI’s global behavior [23]. Note that across all three domains, \n",
      "the same proportion of participants self-reported using AI’s conf-\n",
      "dence scores regardless of whether they saw explanations (Figure 9). \n",
      "Furthermore, some participants consigned the task to AI when \n",
      "they were themselves uncertain. For example, 13% participants \n",
      "mentioned that they would go with the AI’s decision if they were on \n",
      "the fence by themselves: “There were some that I could go either way \n",
      "on, and I went with what Marvin suggested. ” These user behaviors are \n",
      "similar to observations in psychology literature on Truth-Default \n",
      "Theory [47], which shows that people exhibit truth-default behav-\n",
      "ior: by default, people are biased to assume that the speaker is being \n",
      "truthful, especially when triggers that raise suspicion are absent. \n",
      "Furthermore, our participants’ distrust in low-confdence recom-\n",
      "mendations is also consistent with examples of triggers that cause \n",
      "people to abandon the truth-default behavior. \n",
      "Explanations can help participants validate the AI’s deci-\n",
      "sions, and the inline-highlight format helped participants \n",
      "speed up their decision making. Among the participants who \n",
      "explicitly mentioned using explanations, 27% in Beer and 32% in Amzbook reported that they used them to read the review text faster. \n",
      "Since LSAT explanations required reading additional text, we did \n",
      "not expect LSAT users to fnd this beneft. Interestingly, for Beer \n",
      "and Amzbook , while a small percentage of users (17%) reported \n",
      "using the explanations to validate the AI’s decisions (see Figure 3), \n",
      "only 2% did so in LSAT . This could be because LSAT is a harder task \n",
      "than sentiment analysis, and verifying AI’s explanations is costlier. \n",
      "Other participants mostly mentioned that they would supplement \n",
      "their own reasoning with the AI’s: “I read the Marvin rationale and \n",
      "weighed it against my intuition and understanding. ” \n",
      "6 DISCUSSION & FUTURE DIRECTIONS \n",
      "Though conducted in a limited scope, our fndings should help guide \n",
      "future work on explanations and other mechanisms for improving \n",
      "decision making with human-AI teams. \n",
      "6.1 Limitations \n",
      "As mentioned in Section 2, AI explanations have other motivations \n",
      "not addressed by this paper. Our work, as well as the papers listed \n",
      "in Table 1, evaluated team performance along one dimension: accu-\n",
      "racy of decisions. We did not explore the benefts on other metrics \n",
      "(e.g.increasing speed as reported by some users in Section 5.2), but \n",
      "in general, one may wish to achieve complementary performance \n",
      "on a multi-dimensional metric. In fact, research shows that large \n",
      "collaborative communities like Wikipedia require AI systems that \n",
      "balance multiple aspects, e.g., reducing human efort, improving \n",
      "trust and positive engagement [68]. We encourage future research \n",
      "to extend the defnition of complementarity, and to evaluate the \n",
      "impact of explanations on those dimensions accordingly. \n",
      "Further, we restricted ourselves to tasks amenable to crowd-\n",
      "sourcing (text classifcation and question answering), so our re-\n",
      "sults may not generalize to high-stakes domains with expert users \n",
      "such as medical diagnosis. We also note that the efectiveness of \n",
      "explanations may depend on user expertise, a factor that we did Does the Whole Exceed its Parts? The Efect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan \n",
      "not explore. Investigating this in our framework would either re-\n",
      "quire recruiting lay and expert users for the same task [21] or \n",
      "utilizing a within-subject experimental design to measure user \n",
      "expertise. \n",
      "Finally, we only explored two possible ways to present expla-\n",
      "nations (highlighting keywords and natural language arguments). \n",
      "While these methods are widely adopted [24, 43, 49, 75], alternative \n",
      "approaches may provide more beneft to team performance. \n",
      "6.2 Explaining AI for Appropriate Reliance \n",
      "One concerning observation was that explanations increased blind \n",
      "trust rather than appropriate reliance on AI. This is problematic \n",
      "especially in domains where humans are required in the loop for \n",
      "moral or legal reasons (e.g., medical diagnosis) and suppose the \n",
      "presence of explanations simply soothes the experts (e.g., doctors), \n",
      "making them more compliant so they blindly (or become more \n",
      "likely to) agree with the computer. Encouraging human-AI interac-\n",
      "tions like these seems deeply unsatisfactory and ethically fraught. \n",
      "Importantly, while prior works also observed instances of inap-\n",
      "propriate reliance on AI [18, 38, 58, 74], our studies quantifed its \n",
      "efect on team performance. Since the nature of the proxy tasks \n",
      "can signifcantly change the human behavior, they can lead to \n",
      "potential misleading conclusions [12]. The emphasis of the com-\n",
      "plementary team performance in end-to-end tasks can objectively \n",
      "evaluate the extent of such issues or about the efectiveness of a \n",
      "solution. \n",
      "Our Adaptive Explanation aims to encourage the human to think \n",
      "more carefully when the system had a low confdence. While the \n",
      "relative agreement rates showed that the Explain-Top-2 explanation \n",
      "might cue the humans to suspect the model’s veracity (Figure 5), \n",
      "the method was not sufcient to signifcantly increase the fnal \n",
      "team accuracy (Figure 4). This is perhaps because users still have \n",
      "to identify high-confdence mistakes (unknown-unknowns) and \n",
      "solve the task when the AI is uncertain (Section 5.1). A followup \n",
      "question is, then, what kind of interactions would help humans \n",
      "perform correctly when the AI is incorrect? \n",
      "Explanations should be informative, instead of just convincing. \n",
      "Our current expert explanations did not help any more than the AI \n",
      "explanations, which may indicate that having the ML produce the \n",
      "maximally convincing explanation — a common objective shared in \n",
      "the design of many AI explanation algorithms — might be a poor \n",
      "choice for complementary performance [ 12]. A more ideal goal is \n",
      "explanations that accurately inform the user – such that the user \n",
      "can quickly gauge through the explanation when the AI’s reasoning \n",
      "is correct and when it should raise suspicion. A successful example \n",
      "of this was seen with Generalized Additive Models (GAMs) for \n",
      "healthcare, where its global explanations helped medical experts \n",
      "suspect that the model had learned incorrect, spurious correlations \n",
      "(e.g.a history of asthma reduces the risk of dying from pneumo-\n",
      "nia [15]). We hope future research can produce explanations that \n",
      "better enable the human to efectively catch AI’s mistakes, rather \n",
      "than fnding plausible justifcations when it erred. \n",
      "High complementary performance may require adapting beyond \n",
      "confdence. Since approaches based on confdence scores make it \n",
      "difcult to spot unknown-unknowns, instead it may be worthwhile to design explanation strategies that adapt based on the frequency \n",
      "of agreement between the human and AI. For example, instead of \n",
      "explaining why it believes an answer to be true, the AI might play \n",
      "a devil’s advocate role, explaining its doubts — even when it agrees \n",
      "with the human. The doubts can even be expressed in an interac-\n",
      "tive fashion (as a back and forth conversation) than a set of static \n",
      "justifcations, so to avoid cognitive overload. For example, even if \n",
      "the system agrees with the user, the system can present a high-level \n",
      "summary of evidence for top-K alternatives and let the user drill \n",
      "down, i.e., ask the system for more detailed evidence for the subset \n",
      "of alternatives that they now think are worth investigating. \n",
      "6.3 Rethinking AI’s Role in Human-AI Teams \n",
      "Comparable accuracy does not guarantee complementary part-\n",
      "ners. Rather, in an ideal team, the human and AI would have mini-\n",
      "mally overlapping mistakes so that there is a greater opportunity \n",
      "to correct each other’s mistakes. In one of our experiment domains \n",
      "(Amzbook ), AI errors correlated much more strongly with humans’ \n",
      "than in others, and thus we saw relatively smaller gains in per-\n",
      "formance from AI assistance (Figure 6). As recent work has sug-\n",
      "gested [4, 55, 59, 79], it may be useful to directly optimize for com-\n",
      "plementary behavior by accounting for the human behavior during \n",
      "training, who may have access to a diferent set of features [72]. \n",
      "Furthermore, the human and AI could maximize their talents in \n",
      "diferent dimensions. For example, for grading exams, AI could use \n",
      "its computation power to quickly gather statistics and highlight \n",
      "commonly missed corner cases, whereas the human teacher could \n",
      "focus on ranking the intelligence of the student proposed algo-\n",
      "rithms [26]. Similarly, to maximize human performance at Quiz \n",
      "Bowl, Feng and Graber [21] designed interaction so that the AI \n",
      "memorized and quickly retrieved documents relevant to a ques-\n",
      "tion, a talent which humans lacked because of cognitive limitations; \n",
      "however, they left the task of combining found evidence and logical \n",
      "reasoning to human partners. Future research should explore other \n",
      "ways to increase synergy. \n",
      "The timing of AI recommendations is important. Besides the types \n",
      "of explanations, it is also important to carefully design when the \n",
      "AI provides its viewpoint. All of our methods used a workfow \n",
      "that showed the AI’s prediction (and its explanation) to the human, \n",
      "before they attempted to solve the problem on their own. However, \n",
      "by presenting an answer and accompanying justifcation upfront, \n",
      "and perhaps overlaid right onto the instance, our design makes it \n",
      "almost impossible for the human to reason independently, ignoring \n",
      "the AI’s opinion while considering the task. This approach risks \n",
      "invoking the anchor efect, studied in Psychology [20] and intro-\n",
      "duced to the AI explanation feld by Wang et al. [74] — people rely \n",
      "heavily on the frst information that is presented by others when \n",
      "making decisions. This efect was refected in an increase in the \n",
      "use of the “AI as Prior Guide ” collaboration approach in the \n",
      "sentiment analysis domain, compared to LSAT (Figure 8). \n",
      "Alternate approaches that present AI recommendations in an \n",
      "asynchronous fashion might increase independence and improve \n",
      "accuracy. For example, pairing humans with slower AIs (that wait \n",
      "or take more time to make recommendation) may provide humans \n",
      "with a better chance to refect on their own decisions [63]. Methods CHI ’21, May 8–13, 2021, Yokohama, Japan G. Bansal et al. \n",
      "that embody recommendations from management science for avoid-\n",
      "ing group-think [54] might also be efective, e.g., showing the AI’s \n",
      "prediction after the human’s initial answer or only having the AI \n",
      "present an explanation if it disagreed with the human’s choice. We \n",
      "note that these approaches correspond to the Update and Feedback \n",
      "methods of Green & Chen [ 27], which were efective, albeit not in \n",
      "the complementary zone. Another approach is to limit the AI’s capa-\n",
      "bilities. For example, one might design the AI to summarize the best \n",
      "evidence for all possible options, without giving hard predictions, \n",
      "by training evidence agents [64]. However, by delaying display of \n",
      "the AI’s recommendation until after the human has solved the task \n",
      "independently or restricting to only per class evidences, one may \n",
      "preclude improvement to the speed of problem solving, which often \n",
      "correlates to the cost of performing the task. \n",
      "As a result, there is a strong tension between the competing \n",
      "objectives of speed, accuracy, and independence; We encourage \n",
      "the feld to design and conduct experiments and explore diferent \n",
      "architectures for balancing these factors. \n",
      "7 CONCLUSIONS \n",
      "Previous work has shown that the accuracy of a human-AI team \n",
      "can be improved when the AI explains its suggestions, but these \n",
      "results are only obtained in situations where the AI, operating in-\n",
      "dependently, is better than either the human or the best human-AI \n",
      "team. We ask if AI explanations help achieve complementary team \n",
      "performance, i.e.whether the team is more accurate than either the \n",
      "AI or human acting independently. We conducted large-scale exper-\n",
      "iments with more than 1,500 participants. Importantly, we selected \n",
      "our study questions to ensure that our AI systems had accuracy \n",
      "comparable to humans and increased the opportunity for seeing \n",
      "complementary performance. While all human-AI teams showed \n",
      "complementarity, none of the explanation conditions produced an \n",
      "accuracy signifcantly higher than the simple baseline of showing \n",
      "the AI’s confdence — in contrast to prior work. Explanations in-\n",
      "creased team performance when the system was correct, but they \n",
      "decreased the accuracy on examples when the system was wrong, \n",
      "making the net improvement minimal. \n",
      "By highlighting critical challenges, we hope this paper will serve \n",
      "as a “Call to action” for the HCI and AI communities: and AI commu-\n",
      "nities. In future work, characterize when human-AI collaboration \n",
      "can be benefcial (i.e., when both parties complement each other), \n",
      "developing explanation approaches and coordination strategies that \n",
      "result in a complementary team performance that exceeds what can \n",
      "be produced by simply showing AI’s confdence, and communicate \n",
      "explanations to increase understanding rather than just to persuade. \n",
      "At the highest level, we hope researchers can develop new interac-\n",
      "tion methods that increase complementary performance beyond \n",
      "having an AI telegraph its confdence. \n",
      "ACKNOWLEDGMENTS \n",
      "This material is based upon work supported by ONR grant N00014-\n",
      "18-1-2193, NSF RAPID grant 2040196, the University of Washington \n",
      "WRF/Cable Professorship, and the Allen Institute for Artifcial In-\n",
      "telligence (AI2), and Microsoft Research. The authors thank Umang\n",
      "Bhatt, Jim Chen, Elena Glassman, Walter Lasecki, Qisheng Li, Eu-\n",
      "nice Jun, Sandy Kaplan, Younghoon Kim, Galen Weld, Amy Zhang, \n",
      "and anonymous reviewers for helpful discussions and comments. REFERENCES \n",
      "[1] Gregory D. Abowd, Anind K. Dey, Peter J. Brown, Nigel Davies, Mark Smith, and \n",
      "Pete Steggles. 1999. Towards a Better Understanding of Context and Context-\n",
      "Awareness. In Proceedings of the 1st International Symposium on Handheld and \n",
      "Ubiquitous Computing (Karlsruhe, Germany) (HUC ’99). Springer-Verlag, Berlin, \n",
      "Heidelberg, 304–307. \n",
      "[2] Julia Angwin, Jef Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine bias: \n",
      "There’s software across the country to predict future criminals and it’s biased \n",
      "against blacks. \n",
      "[3] Oisin Mac Aodha, Shihan Su, Yuxin Chen, Pietro Perona, and Yisong Yue. 2018. \n",
      "Teaching Categories to Human Learners With Visual Explanations. In 2018 IEEE \n",
      "Conference on Computer Vision and Pattern Recognition, CVPR 2018. IEEE Com-\n",
      "puter Society, Salt Lake City, UT, USA, 3820–3828. https://doi.org/10.1109/CVPR. \n",
      "2018.00402 \n",
      "[4] Gagan Bansal, Besmira Nushi, Ece Kamar, Eric Horvitz, and Daniel S. Weld. 2020. \n",
      "Optimizing AI for Teamwork. arXiv:2004.13102 [cs.AI] \n",
      "[5] Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S. Lasecki, Daniel S. Weld, and \n",
      "Eric Horvitz. 2019. Beyond Accuracy: The Role of Mental Models in Human-AI \n",
      "Team Performance. Proceedings of the AAAI Conference on Human Computation \n",
      "and Crowdsourcing 7, 1 (Oct. 2019), 2–11. https://ojs.aaai.org/index.php/HCOMP/ \n",
      "article/view/5285 \n",
      "[6] Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel S. Weld, Walter S. Lasecki, and \n",
      "Eric Horvitz. 2019. Updates in Human-AI Teams: Understanding and Addressing \n",
      "the Performance/Compatibility Tradeof. Proceedings of the AAAI Conference on \n",
      "Artifcial Intelligence 33, 01 (Jul. 2019), 2429–2437. https://doi.org/10.1609/aaai. \n",
      "v33i01.33012429 \n",
      "[7] Richard E Barlow and Hugh D Brunk. 1972. The isotonic regression problem and \n",
      "its dual. J. Amer. Statist. Assoc. 67, 337 (1972), 140–147. \n",
      "[8] Mohsen Bayati, Mark Braverman, Michael Gillam, Karen M. Mack, George Ruiz, \n",
      "Mark S. Smith, and Eric Horvitz. 2014. Data-Driven Decisions for Reducing \n",
      "Readmissions for Heart Failure: General Methodology and Case Study. PLOS \n",
      "ONE 9, 10 (10 2014), 1–9. https://doi.org/10.1371/journal.pone.0109264 \n",
      "[9] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yun-\n",
      "han Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura, and Peter Eckersley. \n",
      "2020. Explainable Machine Learning in Deployment. In Proceedings of the \n",
      "2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) \n",
      "(FAT* ’20). Association for Computing Machinery, New York, NY, USA, 648–657. \n",
      "https://doi.org/10.1145/3351095.3375624 \n",
      "[10] Mustafa Bilgic. 2005. Explaining Recommendations: Satisfaction vs. Promotion. , \n",
      "13–18 pages. \n",
      "[11] Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and \n",
      "Nigel Shadbolt. 2018. ’It’s Reducing a Human Being to a Percentage’: Per-\n",
      "ceptions of Justice in Algorithmic Decisions. In Proceedings of the 2018 CHI \n",
      "Conference on Human Factors in Computing Systems (Montreal QC, Canada) \n",
      "(CHI ’18). Association for Computing Machinery, New York, NY, USA, 1–14. \n",
      "https://doi.org/10.1145/3173574.3173951 \n",
      "[12] Zana Buçinca, Phoebe Lin, Krzysztof Z. Gajos, and Elena L. Glassman. 2020. Proxy \n",
      "Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI \n",
      "Systems. In Proceedings of the 25th International Conference on Intelligent User \n",
      "Interfaces (Cagliari, Italy) (IUI ’20). Association for Computing Machinery, New \n",
      "York, NY, USA, 454–464. https://doi.org/10.1145/3377325.3377498 \n",
      "[13] Adrian Bussone, Simone Stumpf, and Dympna O’Sullivan. 2015. The role of \n",
      "explanations on trust and reliance in clinical decision support systems. In 2015 \n",
      "International Conference on Healthcare Informatics. IEEE, IEEE, Dallas, Texas, \n",
      "160–169. \n",
      "[14] Carrie J Cai, Samantha Winter, David Steiner, Lauren Wilcox, and Michael Terry. \n",
      "2019. \" Hello AI\": Uncovering the Onboarding Needs of Medical Practitioners for \n",
      "Human-AI Collaborative Decision-Making. Proceedings of the ACM on Human-\n",
      "Computer Interaction 3, CSCW (2019), 1–24. \n",
      "[15] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie \n",
      "Elhadad. 2015. Intelligible Models for HealthCare: Predicting Pneumonia Risk \n",
      "and Hospital 30-Day Readmission. In Proceedings of the 21th ACM SIGKDD In-\n",
      "ternational Conference on Knowledge Discovery and Data Mining (Sydney, NSW, \n",
      "Australia) (KDD ’15). Association for Computing Machinery, New York, NY, USA, \n",
      "1721–1730. https://doi.org/10.1145/2783258.2788613 \n",
      "[16] Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, \n",
      "and Devi Parikh. 2018. Do explanations make VQA models more predictable \n",
      "to a human?. In Proceedings of the 2018 Conference on Empirical Methods in \n",
      "Natural Language Processing. Association for Computational Linguistics, Brussels, \n",
      "Belgium, 1036–1042. https://doi.org/10.18653/v1/D18-1128 \n",
      "[17] Elizabeth Clark, Anne Spencer Ross, Chenhao Tan, Yangfeng Ji, and Noah A. \n",
      "Smith. 2018. Creative Writing with a Machine in the Loop: Case Studies on \n",
      "Slogans and Stories. In 23rd International Conference on Intelligent User Interfaces \n",
      "(Tokyo, Japan) (IUI ’18). Association for Computing Machinery, New York, NY, \n",
      "USA, 329–340. https://doi.org/10.1145/3172944.3172983 Does the Whole Exceed its Parts? The Efect of AI Explanations on Complementary Team Performance CHI ’21, May 8–13, 2021, Yokohama, Japan \n",
      "[18] Pat Croskerry. 2009. Clinical cognition and diagnostic error: applications of a \n",
      "dual process model of reasoning. Advances in health sciences education 14, 1 \n",
      "(2009), 27–35. \n",
      "[19] Xiao Dong and Caroline C Hayes. 2012. Uncertainty visualizations: Helping \n",
      "decision makers become more aware of uncertainty and its implications. Journal \n",
      "of Cognitive Engineering and Decision Making 6, 1 (2012), 30–56. \n",
      "[20] Birte Englich, Thomas Mussweiler, and Fritz Strack. 2006. Playing dice with crim-\n",
      "inal sentences: The infuence of irrelevant anchors on experts’ judicial decision \n",
      "making. Personality and Social Psychology Bulletin 32, 2 (2006), 188–200. \n",
      "[21] Shi Feng and Jordan Boyd-Graber. 2019. What Can AI Do for Me? Evaluating \n",
      "Machine Learning Interpretations in Cooperative Play. In Proceedings of the 24th \n",
      "International Conference on Intelligent User Interfaces (Marina del Ray, California) \n",
      "(IUI ’19). Association for Computing Machinery, New York, NY, USA, 229–239. \n",
      "https://doi.org/10.1145/3301275.3302265 \n",
      "[22] Michael Fernandes, Logan Walls, Sean Munson, Jessica Hullman, and Matthew \n",
      "Kay. 2018. Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit \n",
      "Decision-Making. In Proceedings of the 2018 CHI Conference on Human Factors in \n",
      "Computing Systems (Montreal QC, Canada) (CHI ’18). Association for Computing \n",
      "Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/3173574.3173718 \n",
      "[23] Katy Ilonka Gero, Zahra Ashktorab, Casey Dugan, Qian Pan, James Johnson, \n",
      "Werner Geyer, Maria Ruiz, Sarah Miller, David R. Millen, Murray Campbell, \n",
      "Sadhana Kumaravel, and Wei Zhang. 2020. Mental Models of AI Agents in a \n",
      "Cooperative Game Setting. In Proceedings of the 2020 CHI Conference on Human \n",
      "Factors in Computing Systems (Honolulu, HI, USA) (CHI ’20). Association for \n",
      "Computing Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/ \n",
      "3313831.3376316 \n",
      "[24] Reza Ghaeini, Xiaoli Fern, and Prasad Tadepalli. 2018. Interpreting Recurrent and \n",
      "Attention-Based Neural Models: a Case Study on Natural Language Inference. \n",
      "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language \n",
      "Processing. Association for Computational Linguistics, Brussels, Belgium, 4952– \n",
      "4957. https://doi.org/10.18653/v1/D18-1537 \n",
      "[25] Dimitra Gkatzia, Oliver Lemon, and Verena Rieser. 2016. Natural Language \n",
      "Generation enhances human decision-making with uncertain information. In \n",
      "Proceedings of the 54th Annual Meeting of the Association for Computational \n",
      "Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, \n",
      "Berlin, Germany, 264–268. https://doi.org/10.18653/v1/P16-2043 \n",
      "[26] Elena L Glassman, Jeremy Scott, Rishabh Singh, Philip J Guo, and Robert C Miller. \n",
      "2015. OverCode: Visualizing variation in student solutions to programming \n",
      "problems at scale. ACM Transactions on Computer-Human Interaction (TOCHI) \n",
      "22, 2 (2015), 1–35. \n",
      "[27] Ben Green and Yiling Chen. 2019. The principles and limits of algorithm-in-the-\n",
      "loop decision making. Proceedings of the ACM on Human-Computer Interaction 3, \n",
      "CSCW (2019), 1–24. \n",
      "[28] Chuan Guo, Geof Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On Calibration \n",
      "of Modern Neural Networks. In Proceedings of the 34th International Conference \n",
      "on Machine Learning - Volume 70 (ICML’17). JMLR.org, Sydney, NSW, Australia, \n",
      "1321–1330. \n",
      "[29] Peter Hase and Mohit Bansal. 2020. Evaluating Explainable AI: Which Algorith-\n",
      "mic Explanations Help Users Predict Model Behavior?. In Proceedings of the 58th \n",
      "Annual Meeting of the Association for Computational Linguistics. Association for \n",
      "Computational Linguistics, Online, 5540–5552. https://doi.org/10.18653/v1/2020. \n",
      "acl-main.491 \n",
      "[30] Yugo Hayashi and Kosuke Wakabayashi. 2017. Can AI Become Reliable Source \n",
      "to Support Human Decision Making in a Court Scene?. In Companion of the 2017 \n",
      "ACM Conference on Computer Supported Cooperative Work and Social Computing \n",
      "(Portland, Oregon, USA) (CSCW ’17 Companion). Association for Computing Ma-\n",
      "chinery, New York, NY, USA, 195–198. https://doi.org/10.1145/3022198.3026338 \n",
      "[31] Ruining He and Julian McAuley. 2016. Ups and Downs: Modeling the Vi-\n",
      "sual Evolution of Fashion Trends with One-Class Collaborative Filtering. In \n",
      "Proceedings of the 25th International Conference on World Wide Web (Mon-\n",
      "tréal, Québec, Canada) (WWW ’16). International World Wide Web Confer-\n",
      "ences Steering Committee, Republic and Canton of Geneva, CHE, 507–517. \n",
      "https://doi.org/10.1145/2872427.2883037 \n",
      "[32] Dan Hendrycks and Kevin Gimpel. 2017. A Baseline for Detecting Misclassifed \n",
      "and Out-of-Distribution Examples in Neural Networks. In 5th International Con-\n",
      "ference on Learning Representations, ICLR 2017. OpenReview.net, Toulon, France, \n",
      "1–12. https://openreview.net/forum?id=Hkg4TI9xl \n",
      "[33] Chien-Ju Ho, Aleksandrs Slivkins, Siddharth Suri, and Jennifer Wortman Vaughan. \n",
      "2015. Incentivizing High Quality Crowdwork. In Proceedings of the 24th Interna-\n",
      "tional Conference on World Wide Web (Florence, Italy) (WWW ’15). International \n",
      "World Wide Web Conferences Steering Committee, Republic and Canton of \n",
      "Geneva, CHE, 419–429. https://doi.org/10.1145/2736277.2741102 \n",
      "[34] Eric Horvitz. 1999. Principles of Mixed-Initiative User Interfaces. In Proceedings \n",
      "of the SIGCHI Conference on Human Factors in Computing Systems (Pittsburgh, \n",
      "Pennsylvania, USA) (CHI ’99). Association for Computing Machinery, New York, \n",
      "NY, USA, 159–166. https://doi.org/10.1145/302979.303030 \n",
      "[35] Eric Horvitz and Tim Paek. 2007. Complementary computing: policies for trans-\n",
      "ferring callers from dialog systems to human receptionists. User Modeling and User-Adapted Interaction 17, 1-2 (2007), 159–182. \n",
      "[36] Shagun Jhaver, Amy Bruckman, and Eric Gilbert. 2019. Does transparency in \n",
      "moderation really matter? User behavior after content removal explanations on \n",
      "reddit. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), \n",
      "1–27. \n",
      "[37] Susan Joslyn and Jared LeClerc. 2013. Decisions with uncertainty: The glass half \n",
      "full. Current Directions in Psychological Science 22, 4 (2013), 308–315. \n",
      "[38] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, \n",
      "and Jennifer Wortman Vaughan. 2020. Interpreting Interpretability: Under-\n",
      "standing Data Scientists’ Use of Interpretability Tools for Machine Learning. In \n",
      "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems \n",
      "(Honolulu, HI, USA) (CHI ’20). Association for Computing Machinery, New York, \n",
      "NY, USA, 1–14. https://doi.org/10.1145/3313831.3376219 \n",
      "[39] Derek J Koehler. 1991. Explanation, imagination, and confdence in judgment. \n",
      "Psychological bulletin 110, 3 (1991), 499. \n",
      "[40] Pang Wei Koh and Percy Liang. 2017. Understanding Black-Box Predictions \n",
      "via Infuence Functions. In Proceedings of the 34th International Conference on \n",
      "Machine Learning - Volume 70 (ICML’17). JMLR.org, Sydney, NSW, Australia, \n",
      "1885–1894. \n",
      "[41] Johannes Kunkel, Tim Donkers, Lisa Michael, Catalin-Mihai Barbu, and Jür-\n",
      "gen Ziegler. 2019. Let Me Explain: Impact of Personal and Impersonal Ex-\n",
      "planations on Trust in Recommender Systems. In Proceedings of the 2019 CHI \n",
      "Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) \n",
      "(CHI ’19). Association for Computing Machinery, New York, NY, USA, 1–12. \n",
      "https://doi.org/10.1145/3290605.3300717 \n",
      "[42] Vivian Lai, Han Liu, and Chenhao Tan. 2020. \"Why is ’Chicago’ Deceptive?\" \n",
      "Towards Building Model-Driven Tutorials for Humans. In Proceedings of the 2020 \n",
      "CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) \n",
      "(CHI ’20). Association for Computing Machinery, New York, NY, USA, 1–13. \n",
      "https://doi.org/10.1145/3313831.3376873 \n",
      "[43] Vivian Lai and Chenhao Tan. 2019. On Human Predictions with Explanations and \n",
      "Predictions of Machine Learning Models: A Case Study on Deception Detection. \n",
      "In Proceedings of the Conference on Fairness, Accountability, and Transparency \n",
      "(Atlanta, GA, USA) (FAT* ’19). Association for Computing Machinery, New York, \n",
      "NY, USA, 29–38. https://doi.org/10.1145/3287560.3287590 \n",
      "[44] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Eric Horvitz. 2017. Iden-\n",
      "tifying Unknown Unknowns in the Open World: Representations and Policies \n",
      "for Guided Exploration. In Proceedings of the Thirty-First AAAI Conference on \n",
      "Artifcial Intelligence (AAAI’17). AAAI Press, San Francisco, California, USA, \n",
      "2124–2132. \n",
      "[45] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. \n",
      "2017. Interpretable & Explorable Approximations of Black Box Models. \n",
      "arXiv:1707.01154 [cs.AI] \n",
      "[46] John D. Lee and Katrina A. See. 2004. Trust in Automation: Designing for \n",
      "Appropriate Reliance. Human Factors 46, 1 (2004), 50–80. https://doi.org/10. \n",
      "1518/hfes.46.1.50_30392 \n",
      "[47] Timothy R Levine. 2014. Truth-default theory (TDT) a theory of human deception \n",
      "and deception detection. Journal of Language and Social Psychology 33, 4 (2014), \n",
      "378–392. \n",
      "[48] B. Lim, A. Dey, and D. Avrahami. 2009. Why and Why Not Explanations Improve \n",
      "the Intelligibility of Context-aware Intelligent Systems. In Proceedings of the \n",
      "SIGCHI Conference on Human Factors in Computing Systems (Boston, MA, USA) \n",
      "(CHI ’09). ACM, New York, NY, USA, 2119–2128. https://doi.org/10.1145/1518701. \n",
      "1519023 \n",
      "[49] Zhouhan Lin, Minwei Feng, Cícero Nogueira dos Santos, Mo Yu, Bing Xiang, \n",
      "Bowen Zhou, and Yoshua Bengio. 2017. A Structured Self-Attentive Sentence \n",
      "Embedding. In 5th International Conference on Learning Representations, ICLR \n",
      "2017, Conference Track Proceedings. OpenReview.net, Toulon, France, 1–15. https: \n",
      "//openreview.net/forum?id=BJC_jUqxe \n",
      "[50] Zachary C. Lipton. 2018. The Mythos of Model Interpretability: In Machine \n",
      "Learning, the Concept of Interpretability is Both Important and Slippery. Queue \n",
      "16, 3 (June 2018), 31–57. https://doi.org/10.1145/3236386.3241340 \n",
      "[51] Angli Liu, Stephen Soderland, Jonathan Bragg, Christopher H. Lin, Xiao Ling, \n",
      "and Daniel S. Weld. 2016. Efective Crowd Annotation for Relation Extraction. In \n",
      "Proceedings of the 2016 Conference of the North American Chapter of the Association \n",
      "for Computational Linguistics: Human Language Technologies. Association for \n",
      "Computational Linguistics, San Diego, California, 897–906. https://doi.org/10. \n",
      "18653/v1/N16-1104 \n",
      "[52] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer \n",
      "Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A \n",
      "Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL] \n",
      "[53] Scott M. Lundberg, Bala Nair, Monica S. Vavilala, Mayumi Horibe, Michael J. \n",
      "Eisses, Trevor Adams, David E. Liston, Daniel King-Wai Low, Shu-Fang Newman, \n",
      "Jerry Kim, and Su-In Lee. 2018. Explainable machine-learning predictions for the \n",
      "prevention of hypoxaemia during surgery. Nature Biomedical Engineering 2, 10 \n",
      "(01 Oct 2018), 749–760. https://doi.org/10.1038/s41551-018-0304-0 \n",
      "[54] Les Macleod. 2011. Avoiding “groupthink” A manager’s challenge. Nursing \n",
      "management 42, 10 (2011), 44–48. CHI ’21, May 8–13, 2021, Yokohama, Japan \n",
      "[55] David Madras, Toniann Pitassi, and Richard Zemel. 2018. Predict Responsibly: \n",
      "Improving Fairness and Accuracy by Learning to Defer. In Proceedings of the \n",
      "32nd International Conference on Neural Information Processing Systems (Montréal, \n",
      "Canada) (NIPS’18). Curran Associates Inc., Red Hook, NY, USA, 6150–6160. \n",
      "[56] Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012. Learning Attitudes \n",
      "and Attributes from Multi-Aspect Reviews. In Proceedings of the 2012 IEEE 12th \n",
      "International Conference on Data Mining (ICDM ’12). IEEE Computer Society, \n",
      "USA, 1020–1025. https://doi.org/10.1109/ICDM.2012.110 \n",
      "[57] T. Miller. 2018. Explanation in artifcial intelligence: Insights from the social \n",
      "sciences. Artifcial Intelligence 267 (February 2018), 1–38. \n",
      "[58] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, \n",
      "Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. \n",
      "Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, \n",
      "Accountability, and Transparency (Atlanta, GA, USA) (FAT* ’19). Association for \n",
      "Computing Machinery, New York, NY, USA, 220–229. https://doi.org/10.1145/ \n",
      "3287560.3287596 \n",
      "[59] Hussein Mozannar and David Sontag. 2020. Consistent Estimators for Learning to \n",
      "Defer to an Expert. In Proceedings of the 37th International Conference on Machine \n",
      "Learning (Proceedings of Machine Learning Research, Vol. 119), Hal Daumé III and \n",
      "Aarti Singh (Eds.). PMLR, Virtual, 7076–7087. http://proceedings.mlr.press/v119/ \n",
      "mozannar20b.html \n",
      "[60] Limor Nadav-Greenberg and Susan L Joslyn. 2009. Uncertainty forecasts improve \n",
      "decision making among nonexperts. Journal of Cognitive Engineering and Decision \n",
      "Making 3, 3 (2009), 209–227. \n",
      "[61] Menaka Narayanan, Emily Chen, Jefrey He, Been Kim, Sam Gershman, and Finale \n",
      "Doshi-Velez. 2018. How do Humans Understand Explanations from Machine \n",
      "Learning Systems? An Evaluation of the Human-Interpretability of Explanation. \n",
      "arXiv:1802.00682 [cs.AI] \n",
      "[62] Dong Nguyen. 2018. Comparing Automatic and Human Evaluation of Local \n",
      "Explanations for Text Classifcation. In Proceedings of the 2018 Conference of the \n",
      "North American Chapter of the Association for Computational Linguistics: Human \n",
      "Language Technologies, Volume 1 (Long Papers). Association for Computational \n",
      "Linguistics, New Orleans, Louisiana, 1069–1078. https://doi.org/10.18653/v1/N18-\n",
      "1097 \n",
      "[63] Joon Sung Park, Rick Barber, Alex Kirlik, and Karrie Karahalios. 2019. A Slow \n",
      "Algorithm Improves Users’ Assessments of the Algorithm’s Accuracy. Proceedings \n",
      "of the ACM on Human-Computer Interaction 3, CSCW (2019), 1–15. \n",
      "[64] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and \n",
      "Kyunghyun Cho. 2019. Finding Generalizable Evidence by Learning to Convince \n",
      "Q&A Models. In Proceedings of the 2019 Conference on Empirical Methods in \n",
      "Natural Language Processing and the 9th International Joint Conference on Natural \n",
      "Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, \n",
      "Hong Kong, China, 2402–2411. https://doi.org/10.18653/v1/D19-1244 \n",
      "[65] Forough Poursabzi-Sangdeh, Daniel G. Goldstein, Jake M. Hofman, Jennifer Wort-\n",
      "man Vaughan, and Hanna Wallach. 2019. Manipulating and Measuring Model \n",
      "Interpretability. arXiv:1802.07810 [cs.AI] \n",
      "[66] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I \n",
      "Trust You?\": Explaining the Predictions of Any Classifer. In Proceedings of the \n",
      "22nd ACM SIGKDD International Conference on Knowledge Discovery and Data \n",
      "Mining (San Francisco, California, USA) (KDD ’16). Association for Computing \n",
      "Machinery, New York, NY, USA, 1135–1144. https://doi.org/10.1145/2939672. \n",
      "2939778 \n",
      "[67] Philipp Schmidt and Felix Biessmann. 2019. Quantifying Interpretability and \n",
      "Trust in Machine Learning Systems. arXiv:1901.08558 [cs.LG] \n",
      "[68] C. Estelle Smith, Bowen Yu, Anjali Srivastava, Aaron Halfaker, Loren Terveen, \n",
      "and Haiyi Zhu. 2020. Keeping Community in the Loop: Understanding Wikipedia \n",
      "Stakeholder Values for Machine Learning-Based Systems. In Proceedings of the \n",
      "2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, \n",
      "USA) (CHI ’20). Association for Computing Machinery, New York, NY, USA, 1–14. G. Bansal et al. \n",
      "https://doi.org/10.1145/3313831.3376783 \n",
      "[69] Simone Stumpf, Vidya Rajaram, Lida Li, Weng-Keen Wong, Margaret Burnett, \n",
      "Thomas Dietterich, Erin Sullivan, and Jonathan Herlocker. 2009. Interacting \n",
      "meaningfully with machine learning systems: Three experiments. International \n",
      "Journal of Human-Computer Studies 67, 8 (2009), 639–662. \n",
      "[70] Sarah Tan, Julius Adebayo, Kori Inkpen, and Ece Kamar. 2018. Inves-\n",
      "tigating Human + Machine Complementarity for Recidivism Predictions. \n",
      "arXiv:1808.09123 [cs.LG] \n",
      "[71] LSAT Prep Books Team. 2017. LSAT prep book study guide: quick study & practice \n",
      "test questions for the Law School Admissions council’s (LSAC) Law school admission \n",
      "test. Mometrix Test Preparation, Beaumont, TX. \n",
      "[72] Kush R. Varshney, Prashant Khanduri, Pranay Sharma, Shan Zhang, and \n",
      "Pramod K. Varshney. 2018. Why Interpretability in Machine Learn-\n",
      "ing? An Answer Using Distributed Detection and Data Fusion Theory. \n",
      "arXiv:1806.09710 [stat.ML] \n",
      "[73] J. von Neumann and O. Morgenstern. 1947. Theory of games and economic \n",
      "behavior. Princeton University Press, Princeton, New Jersey. \n",
      "[74] Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y. Lim. 2019. Design-\n",
      "ing Theory-Driven User-Centric Explainable AI. In Proceedings of the 2019 CHI \n",
      "Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) \n",
      "(CHI ’19). Association for Computing Machinery, New York, NY, USA, 1–15. \n",
      "https://doi.org/10.1145/3290605.3300831 \n",
      "[75] Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. 2016. Attention-based \n",
      "LSTM for Aspect-level Sentiment Classifcation. In Proceedings of the 2016 Confer-\n",
      "ence on Empirical Methods in Natural Language Processing. Association for Com-\n",
      "putational Linguistics, Austin, Texas, 606–615. https://doi.org/10.18653/v1/D16-\n",
      "1058 \n",
      "[76] Hilde J. P. Weerts, Werner van Ipenburg, and Mykola Pechenizkiy. 2019. A Human-\n",
      "Grounded Evaluation of SHAP for Alert Processing. arXiv:1907.03324 [cs.LG] \n",
      "[77] Daniel S. Weld and Gagan Bansal. 2019. The Challenge of Crafting Intelligible \n",
      "Intelligence. Commun. ACM 62, 6 (May 2019), 70–79. https://doi.org/10.1145/ \n",
      "3282486 \n",
      "[78] Jenna Wiens, John Guttag, and Eric Horvitz. 2016. Patient risk stratifcation \n",
      "with time-varying parameters: a multitask learning approach. JMLR 17, 1 (2016), \n",
      "2797–2819. \n",
      "[79] Bryan Wilder, Eric Horvitz, and Ece Kamar. 2020. Learning to Complement \n",
      "Humans. In Proceedings of the Twenty-Ninth International Joint Conference on \n",
      "Artifcial Intelligence, IJCAI-20, Christian Bessiere (Ed.). International Joint Con-\n",
      "ferences on Artifcial Intelligence Organization, Yokohama, Japan, 1526–1533. \n",
      "https://doi.org/10.24963/ijcai.2020/212 Main track. \n",
      "[80] Fumeng Yang, Zhuanyi Huang, Jean Scholtz, and Dustin L. Arendt. 2020. How Do \n",
      "Visual Explanations Foster End Users’ Appropriate Trust in Machine Learning?. \n",
      "In Proceedings of the 25th International Conference on Intelligent User Interfaces \n",
      "(Cagliari, Italy) (IUI ’20). Association for Computing Machinery, New York, NY, \n",
      "USA, 189–201. https://doi.org/10.1145/3377325.3377480 \n",
      "[81] Kun Yu, Shlomo Berkovsky, Ronnie Taib, Jianlong Zhou, and Fang Chen. 2019. \n",
      "Do I Trust My Machine Teammate? An Investigation from Perception to Decision. \n",
      "In Proceedings of the 24th International Conference on Intelligent User Interfaces \n",
      "(Marina del Ray, California) (IUI ’19). Association for Computing Machinery, New \n",
      "York, NY, USA, 460–468. https://doi.org/10.1145/3301275.3302277 \n",
      "[82] Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. ReClor: A Reading \n",
      "Comprehension Dataset Requiring Logical Reasoning. In 8th International Con-\n",
      "ference on Learning Representations, ICLR 2020. OpenReview.net, Addis Ababa, \n",
      "Ethiopia, 1–26. https://openreview.net/forum?id=HJgJtT4tvB \n",
      "[83] Yunfeng Zhang, Q. Vera Liao, and Rachel K. E. Bellamy. 2020. Efect of Conf-\n",
      "dence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision \n",
      "Making. In Proceedings of the 2020 Conference on Fairness, Accountability, and \n",
      "Transparency (Barcelona, Spain) (FAT* ’20). Association for Computing Machin-\n",
      "ery, New York, NY, USA, 295–305. https://doi.org/10.1145/3351095.3372852 \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The title of the paper is \"Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance.\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task3 = \"\"\"Based on the text in paper.txt, answer the following question.\n",
    "\n",
    "Question: What it the title of the paper?\n",
    "\n",
    "Hint: Print the contents of the pdf, and you your language ability to answer.\n",
    "\"\"\"\n",
    "user_proxy.initiate_chat(assistant, message=task3, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Based on the text in paper.txt, answer the following question.\n",
      "The answer must be 3 bullet points that can be inserted in a ppt slide.\n",
      "\n",
      "Question: What is the key problem being discussed in the paper?\n",
      "\n",
      "Hint: Print the contents of the pdf, and you your language ability to answer.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Here is a summary of the key problem being discussed in the paper:\n",
      "\n",
      "- Investigating the effectiveness of AI explanations in improving human-AI team performance\n",
      "- Understanding whether AI explanations lead to complementary performance, where the team outperforms both the human and AI working individually\n",
      "- Addressing the challenge of developing explanatory approaches that encourage appropriate trust in AI and enhance complementary performance\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task4 = \"\"\"Based on the text in paper.txt, answer the following question.\n",
    "The answer must be 3 bullet points that can be inserted in a ppt slide.\n",
    "\n",
    "Question: What is the key problem being discussed in the paper?\n",
    "\n",
    "Hint: Print the contents of the pdf, and you your language ability to answer.\n",
    "\"\"\"\n",
    "user_proxy.initiate_chat(assistant, message=task4, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Based on the text in paper.txt, answer the following question.\n",
      "The answer must be 3 bullet points that can be inserted in a ppt slide.\n",
      "\n",
      "Question: What is they key related work discussed in the paper?\n",
      "\n",
      "Hint: Use previously printed contents of the pdf, and you your language ability to answer.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Here is a summary of the key related work discussed in the paper:\n",
      "\n",
      "- Prior studies on explainable AI that show improvements in human-AI team performance when AI provides explanations\n",
      "- Research on human-centered AI, focusing on developing explanatory approaches that encourage appropriate trust in AI systems\n",
      "- Exploration of various explanation strategies and sources, such as Explain-Top-1, Explain-Top-2, and Adaptive explanations, as well as AI-generated and expert-generated explanations\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task5 = \"\"\"Based on the text in paper.txt, answer the following question.\n",
    "The answer must be 3 bullet points that can be inserted in a ppt slide.\n",
    "\n",
    "Question: What is they key related work discussed in the paper?\n",
    "\n",
    "Hint: Use previously printed contents of the pdf, and you your language ability to answer.\n",
    "\"\"\"\n",
    "user_proxy.initiate_chat(assistant, message=task5, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Based on the text in paper.txt, answer the following question.\n",
      "The answer must be 3 bullet points that can be inserted in a ppt slide.\n",
      "\n",
      "Question: What are they key results discussed in the experiments of the paper?\n",
      "\n",
      "Hint: Use previously printed contents of the pdf, and you your language ability to answer.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Here is a summary of the key results discussed in the experiments of the paper:\n",
      "\n",
      "- Complementary performance was observed in human-AI teams, but explanations did not significantly increase performance compared to simply displaying the AI's confidence\n",
      "- Explanations increased reliance on AI recommendations even when the AI was incorrect, posing challenges for human-centered AI\n",
      "- Adaptive explanations, which combine the benefits of existing strategies, showed potential for improvement but did not significantly increase team performance\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task6 = \"\"\"Based on the text in paper.txt, answer the following question.\n",
    "The answer must be 3 bullet points that can be inserted in a ppt slide.\n",
    "\n",
    "Question: What are they key results discussed in the experiments of the paper?\n",
    "\n",
    "Hint: Use previously printed contents of the pdf, and you your language ability to answer.\n",
    "\"\"\"\n",
    "user_proxy.initiate_chat(assistant, message=task6, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Based on the questions asked thus far and your responses,\n",
      "Create a pptx file with called reading_group.pptx.\n",
      "\n",
      "The pptx file must have four slides with the following titles:\n",
      "1. Title\n",
      "2. Problem\n",
      "3. Related Work\n",
      "4. Results\n",
      "\n",
      "Each slide should have the response we previously discussed as bullet points.\n",
      "\n",
      "Slides should be aesthetically pleasing and ready to present to a group of people.\n",
      "Hint: Write python code to accomplish the task. Use python packages that create ppt if you have to.\n",
      "If you use pptx package, use the api properly.\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To create a PowerPoint presentation with the specified slides and content, we can use the `python-pptx` library. First, you need to install the library using pip:\n",
      "\n",
      "```sh\n",
      "pip install python-pptx\n",
      "```\n",
      "\n",
      "After installing the library, you can use the following Python code to create the PowerPoint presentation:\n",
      "\n",
      "```python\n",
      "# filename: create_presentation.py\n",
      "\n",
      "from pptx import Presentation\n",
      "from pptx.util import Inches\n",
      "\n",
      "# Create a presentation object\n",
      "presentation = Presentation()\n",
      "\n",
      "# Slide 1: Title\n",
      "slide1 = presentation.slides.add_slide(presentation.slide_layouts[0])\n",
      "slide1_title = slide1.shapes.title\n",
      "slide1_title.text = \"AI Explanations and Complementary Team Performance\"\n",
      "\n",
      "# Slide 2: Problem\n",
      "slide2 = presentation.slides.add_slide(presentation.slide_layouts[1])\n",
      "slide2_title = slide2.shapes.title\n",
      "slide2_title.text = \"Problem\"\n",
      "slide2_content = slide2.shapes.placeholders[1].text_frame\n",
      "slide2_content.text = \"Investigating the effectiveness of AI explanations in improving human-AI team performance\"\n",
      "slide2_content.add_paragraph().text = \"Understanding whether AI explanations lead to complementary performance\"\n",
      "slide2_content.add_paragraph().text = \"Developing explanatory approaches that encourage appropriate trust in AI and enhance complementary performance\"\n",
      "\n",
      "# Slide 3: Related Work\n",
      "slide3 = presentation.slides.add_slide(presentation.slide_layouts[1])\n",
      "slide3_title = slide3.shapes.title\n",
      "slide3_title.text = \"Related Work\"\n",
      "slide3_content = slide3.shapes.placeholders[1].text_frame\n",
      "slide3_content.text = \"Prior studies on explainable AI that show improvements in human-AI team performance when AI provides explanations\"\n",
      "slide3_content.add_paragraph().text = \"Research on human-centered AI, focusing on developing explanatory approaches that encourage appropriate trust in AI systems\"\n",
      "slide3_content.add_paragraph().text = \"Exploration of various explanation strategies and sources, such as Explain-Top-1, Explain-Top-2, and Adaptive explanations, as well as AI-generated and expert-generated explanations\"\n",
      "\n",
      "# Slide 4: Results\n",
      "slide4 = presentation.slides.add_slide(presentation.slide_layouts[1])\n",
      "slide4_title = slide4.shapes.title\n",
      "slide4_title.text = \"Results\"\n",
      "slide4_content = slide4.shapes.placeholders[1].text_frame\n",
      "slide4_content.text = \"Complementary performance observed in human-AI teams, but explanations did not significantly increase performance compared to simply displaying the AI's confidence\"\n",
      "slide4_content.add_paragraph().text = \"Explanations increased reliance on AI recommendations even when the AI was incorrect, posing challenges for human-centered AI\"\n",
      "slide4_content.add_paragraph().text = \"Adaptive explanations showed potential for improvement but did not significantly increase team performance\"\n",
      "\n",
      "# Save the presentation\n",
      "presentation.save(\"reading_group.pptx\")\n",
      "```\n",
      "\n",
      "You can run this script by saving it to a file named `create_presentation.py` and executing it with Python. This will create a PowerPoint presentation named `reading_group.pptx` with the specified slides and content.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Collecting python-pptx\n",
      "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
      "Collecting lxml>=3.1.0\n",
      "  Downloading lxml-4.9.3-cp39-cp39-manylinux_2_28_aarch64.whl (7.8 MB)\n",
      "Collecting Pillow>=3.3.2\n",
      "  Downloading Pillow-10.0.0-cp39-cp39-manylinux_2_28_aarch64.whl (3.2 MB)\n",
      "Collecting XlsxWriter>=0.5.7\n",
      "  Downloading XlsxWriter-3.1.2-py3-none-any.whl (153 kB)\n",
      "Building wheels for collected packages: python-pptx\n",
      "  Building wheel for python-pptx (setup.py): started\n",
      "  Building wheel for python-pptx (setup.py): finished with status 'done'\n",
      "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470950 sha256=f7cd494e08adb49f716e87b0591866287b3e8e4be4eb55fef507a5fb3031aee4\n",
      "  Stored in directory: /root/.cache/pip/wheels/0e/4a/ed/9653bc799915f52dce3f04d14946fbd85cce9c3cdedc9cfa71\n",
      "Successfully built python-pptx\n",
      "Installing collected packages: XlsxWriter, Pillow, lxml, python-pptx\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Successfully installed Pillow-10.0.0 XlsxWriter-3.1.2 lxml-4.9.3 python-pptx-0.6.21\n",
      "WARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Great! The required packages have been installed successfully, and the PowerPoint presentation named `reading_group.pptx` should have been created with the specified slides and content. You can now use this presentation for your reading group or any other purpose. If you have any other tasks or questions, feel free to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task7 = \"\"\"Based on the questions asked thus far and your responses,\n",
    "Create a pptx file with called reading_group.pptx.\n",
    "\n",
    "The pptx file must have four slides with the following titles:\n",
    "1. Title\n",
    "2. Problem\n",
    "3. Related Work\n",
    "4. Results\n",
    "\n",
    "Each slide should have the response we previously discussed as bullet points.\n",
    "\n",
    "Slides should be aesthetically pleasing and ready to present to a group of people.\n",
    "Hint: Write python code to accomplish the task. Use python packages that create ppt if you have to.\n",
    "If you use pptx package, use the api properly.\n",
    "\n",
    "\"\"\"\n",
    "user_proxy.initiate_chat(assistant, message=task7, clear_history=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
