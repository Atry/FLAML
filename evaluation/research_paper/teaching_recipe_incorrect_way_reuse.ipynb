{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet flaml\"[autogen]\"~=2.0.0 docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import autogen\n",
    "\n",
    "llm_config={\n",
    "    \"request_timeout\": 600,\n",
    "    \"seed\": 44,  # change the seed for different trials\n",
    "    \"config_list\": autogen.config_list_from_json(\"OAI_CONFIG_LIST\",  filter_dict={\n",
    "        \"model\": [\"gpt-4-32k\"],\n",
    "    },),\n",
    "    \"temperature\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent instance named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    is_termination_msg= lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg= lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"work_dir\",\n",
    "        \"use_docker\": False,\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "This recipe is available for you to reuse..\n",
      "\n",
      "<begin recipe>\n",
      "\n",
      "1. Search for relevant papers on arXiv using a query.\n",
      "2. Extract and analyze the abstracts of the papers to identify application domains.\n",
      "3. Count the number of papers in each application domain.\n",
      "4. Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\n",
      "\n",
      "To perform similar tasks in the future, I suggest the following well-documented, generalized Python functions:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import feedparser\n",
      "import matplotlib.pyplot as plt\n",
      "from typing import List, Dict\n",
      "\n",
      "def search_arxiv(query: str, max_results: int = 10) -> List:\n",
      "    \"\"\"Search for papers on arXiv using a query and return a list of papers.\"\"\"\n",
      "    base_url = \"http://export.arxiv.org/api/query?\"\n",
      "    search_query = f\"search_query=all:{query}\"\n",
      "    start = 0\n",
      "    max_results = f\"max_results={max_results}\"\n",
      "    url = f\"{base_url}{search_query}&start={start}&{max_results}\"\n",
      "    response = requests.get(url)\n",
      "    feed = feedparser.parse(response.content)\n",
      "    return feed.entries\n",
      "\n",
      "def analyze_abstracts(papers: List) -> Dict[str, int]:\n",
      "    \"\"\"Analyze the abstracts of the papers and return a dictionary of application domains and their counts.\"\"\"\n",
      "    # Replace this with your own analysis function based on the specific task\n",
      "    pass\n",
      "\n",
      "def generate_bar_chart(domains: Dict[str, int], output_file: str) -> None:\n",
      "    \"\"\"Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\"\"\"\n",
      "    fig, ax = plt.subplots()\n",
      "    ax.bar(domains.keys(), domains.values())\n",
      "    plt.xticks(rotation=45, ha=\"right\")\n",
      "    plt.xlabel(\"Application Domains\")\n",
      "    plt.ylabel(\"Number of Papers\")\n",
      "    plt.title(\"Number of Papers per Application Domain\")\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(output_file)\n",
      "    plt.show()\n",
      "\n",
      "# Example usage:\n",
      "query = \"trust calibration AI\"\n",
      "papers = search_arxiv(query)\n",
      "domains = analyze_abstracts(papers)  # Replace this with your own analysis function\n",
      "output_file = \"domains_bar_chart.png\"\n",
      "generate_bar_chart(domains, output_file)\n",
      "```\n",
      "\n",
      "You can use these functions as a starting point and modify them according to your specific needs. The `search_arxiv` function searches for papers on arXiv using a query, the `analyze_abstracts` function should be replaced with your own analysis function based on the specific task, and the `generate_bar_chart` function generates a bar chart of the application domains and the number of papers in each domain, saving it as an image file.\n",
      "</end recipe>\n",
      "\n",
      "\n",
      "Here is a new task:\n",
      "Plot a chart for application domains of GPT models\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To plot a chart for application domains of GPT models, we will follow the recipe provided with some modifications. We will use the `search_arxiv` function to search for GPT-related papers, and then we will create a new function `extract_domains` to extract application domains from the abstracts. Finally, we will use the `generate_bar_chart` function to create a bar chart of the application domains and the number of papers in each domain.\n",
      "\n",
      "Here's the plan:\n",
      "\n",
      "1. Use the `search_arxiv` function to search for GPT-related papers.\n",
      "2. Create a new function `extract_domains` to extract application domains from the abstracts.\n",
      "3. Use the `generate_bar_chart` function to create a bar chart of the application domains and the number of papers in each domain.\n",
      "\n",
      "Please execute the following Python code:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import feedparser\n",
      "import matplotlib.pyplot as plt\n",
      "from collections import Counter\n",
      "from typing import List, Dict\n",
      "\n",
      "def search_arxiv(query: str, max_results: int = 50) -> List:\n",
      "    base_url = \"http://export.arxiv.org/api/query?\"\n",
      "    search_query = f\"search_query=all:{query}\"\n",
      "    start = 0\n",
      "    max_results = f\"max_results={max_results}\"\n",
      "    url = f\"{base_url}{search_query}&start={start}&{max_results}\"\n",
      "    response = requests.get(url)\n",
      "    feed = feedparser.parse(response.content)\n",
      "    return feed.entries\n",
      "\n",
      "def extract_domains(papers: List) -> Dict[str, int]:\n",
      "    domains = []\n",
      "    for paper in papers:\n",
      "        abstract = paper.summary.lower()\n",
      "        if \"natural language processing\" in abstract or \"nlp\" in abstract:\n",
      "            domains.append(\"NLP\")\n",
      "        if \"computer vision\" in abstract:\n",
      "            domains.append(\"Computer Vision\")\n",
      "        if \"reinforcement learning\" in abstract or \"rl\" in abstract:\n",
      "            domains.append(\"Reinforcement Learning\")\n",
      "        if \"generative modeling\" in abstract:\n",
      "            domains.append(\"Generative Modeling\")\n",
      "        if \"transfer learning\" in abstract:\n",
      "            domains.append(\"Transfer Learning\")\n",
      "        if \"speech recognition\" in abstract:\n",
      "            domains.append(\"Speech Recognition\")\n",
      "    return dict(Counter(domains))\n",
      "\n",
      "def generate_bar_chart(domains: Dict[str, int], output_file: str) -> None:\n",
      "    fig, ax = plt.subplots()\n",
      "    ax.bar(domains.keys(), domains.values())\n",
      "    plt.xticks(rotation=45, ha=\"right\")\n",
      "    plt.xlabel(\"Application Domains\")\n",
      "    plt.ylabel(\"Number of Papers\")\n",
      "    plt.title(\"Number of Papers per Application Domain for GPT Models\")\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(output_file)\n",
      "    plt.show()\n",
      "\n",
      "query = \"GPT\"\n",
      "papers = search_arxiv(query)\n",
      "domains = extract_domains(papers)\n",
      "output_file = \"gpt_domains_bar_chart.png\"\n",
      "generate_bar_chart(domains, output_file)\n",
      "```\n",
      "\n",
      "This code will search for GPT-related papers, extract application domains from the abstracts, and generate a bar chart of the application domains and the number of papers in each domain. The chart will be saved as \"gpt_domains_bar_chart.png\".\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Figure(640x480)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "I'm glad the code executed successfully and generated the bar chart for application domains of GPT models. The chart should be saved as \"gpt_domains_bar_chart.png\" in your current working directory. You can now view the chart to see the distribution of papers across different application domains for GPT models.\n",
      "\n",
      "If you have any further questions or need assistance with another task, feel free to ask.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task1 = '''\n",
    "This recipe is available for you to reuse..\n",
    "\n",
    "<begin recipe>\n",
    "\n",
    "1. Search for relevant papers on arXiv using a query.\n",
    "2. Extract and analyze the abstracts of the papers to identify application domains.\n",
    "3. Count the number of papers in each application domain.\n",
    "4. Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\n",
    "\n",
    "To perform similar tasks in the future, I suggest the following well-documented, generalized Python functions:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import feedparser\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "\n",
    "def search_arxiv(query: str, max_results: int = 10) -> List:\n",
    "    \"\"\"Search for papers on arXiv using a query and return a list of papers.\"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    search_query = f\"search_query=all:{query}\"\n",
    "    start = 0\n",
    "    max_results = f\"max_results={max_results}\"\n",
    "    url = f\"{base_url}{search_query}&start={start}&{max_results}\"\n",
    "    response = requests.get(url)\n",
    "    feed = feedparser.parse(response.content)\n",
    "    return feed.entries\n",
    "\n",
    "def analyze_abstracts(papers: List) -> Dict[str, int]:\n",
    "    \"\"\"Analyze the abstracts of the papers and return a dictionary of application domains and their counts.\"\"\"\n",
    "    # Replace this with your own analysis function based on the specific task\n",
    "    pass\n",
    "\n",
    "def generate_bar_chart(domains: Dict[str, int], output_file: str) -> None:\n",
    "    \"\"\"Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(domains.keys(), domains.values())\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Application Domains\")\n",
    "    plt.ylabel(\"Number of Papers\")\n",
    "    plt.title(\"Number of Papers per Application Domain\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "query = \"trust calibration AI\"\n",
    "papers = search_arxiv(query)\n",
    "domains = analyze_abstracts(papers)  # Replace this with your own analysis function\n",
    "output_file = \"domains_bar_chart.png\"\n",
    "generate_bar_chart(domains, output_file)\n",
    "```\n",
    "\n",
    "You can use these functions as a starting point and modify them according to your specific needs. The `search_arxiv` function searches for papers on arXiv using a query, the `analyze_abstracts` function should be replaced with your own analysis function based on the specific task, and the `generate_bar_chart` function generates a bar chart of the application domains and the number of papers in each domain, saving it as an image file.\n",
    "</end recipe>\n",
    "\n",
    "\n",
    "Here is a new task:\n",
    "Plot a chart for application domains of GPT models\n",
    "'''\n",
    "\n",
    "user_proxy.initiate_chat(assistant, message=task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "This recipe is available for you to reuse..\n",
      "\n",
      "<begin recipe>\n",
      "\n",
      "1. Search for relevant papers on arXiv using a query.\n",
      "2. Extract and analyze the abstracts of the papers to identify application domains.\n",
      "3. Count the number of papers in each application domain.\n",
      "4. Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\n",
      "\n",
      "To perform similar tasks in the future, I suggest the following well-documented, generalized Python functions:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import feedparser\n",
      "import matplotlib.pyplot as plt\n",
      "from typing import List, Dict\n",
      "\n",
      "def search_arxiv(query: str, max_results: int = 10) -> List:\n",
      "    \"\"\"Search for papers on arXiv using a query and return a list of papers.\"\"\"\n",
      "    base_url = \"http://export.arxiv.org/api/query?\"\n",
      "    search_query = f\"search_query=all:{query}\"\n",
      "    start = 0\n",
      "    max_results = f\"max_results={max_results}\"\n",
      "    url = f\"{base_url}{search_query}&start={start}&{max_results}\"\n",
      "    response = requests.get(url)\n",
      "    feed = feedparser.parse(response.content)\n",
      "    return feed.entries\n",
      "\n",
      "def analyze_abstracts(papers: List) -> Dict[str, int]:\n",
      "    \"\"\"Analyze the abstracts of the papers and return a dictionary of application domains and their counts.\"\"\"\n",
      "    # Replace this with your own analysis function based on the specific task\n",
      "    pass\n",
      "\n",
      "def generate_bar_chart(domains: Dict[str, int], output_file: str) -> None:\n",
      "    \"\"\"Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\"\"\"\n",
      "    fig, ax = plt.subplots()\n",
      "    ax.bar(domains.keys(), domains.values())\n",
      "    plt.xticks(rotation=45, ha=\"right\")\n",
      "    plt.xlabel(\"Application Domains\")\n",
      "    plt.ylabel(\"Number of Papers\")\n",
      "    plt.title(\"Number of Papers per Application Domain\")\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(output_file)\n",
      "    plt.show()\n",
      "\n",
      "# Example usage:\n",
      "query = \"trust calibration AI\"\n",
      "papers = search_arxiv(query)\n",
      "domains = analyze_abstracts(papers)  # Replace this with your own analysis function\n",
      "output_file = \"domains_bar_chart.png\"\n",
      "generate_bar_chart(domains, output_file)\n",
      "```\n",
      "\n",
      "You can use these functions as a starting point and modify them according to your specific needs. The `search_arxiv` function searches for papers on arXiv using a query, the `analyze_abstracts` function should be replaced with your own analysis function based on the specific task, and the `generate_bar_chart` function generates a bar chart of the application domains and the number of papers in each domain, saving it as an image file.\n",
      "</end recipe>\n",
      "\n",
      "\n",
      "Here is a new task:\n",
      "Plot a chart for application domains of generative AI in medicine\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To plot a chart for application domains of generative AI in medicine, we will follow the recipe provided and modify the `analyze_abstracts` function to identify application domains in the abstracts. We will use the Natural Language Toolkit (nltk) library to tokenize and analyze the abstracts. Here's the plan:\n",
      "\n",
      "1. Search for relevant papers on arXiv using a query related to generative AI in medicine.\n",
      "2. Extract and analyze the abstracts of the papers to identify application domains using the nltk library.\n",
      "3. Count the number of papers in each application domain.\n",
      "4. Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\n",
      "\n",
      "First, let's install the nltk library:\n",
      "\n",
      "```python\n",
      "!pip install nltk\n",
      "```\n",
      "\n",
      "Now, execute the following code to search for papers, analyze the abstracts, and generate the bar chart:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import feedparser\n",
      "import matplotlib.pyplot as plt\n",
      "import nltk\n",
      "from typing import List, Dict\n",
      "from collections import Counter\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "nltk.download(\"punkt\")\n",
      "nltk.download(\"stopwords\")\n",
      "\n",
      "def search_arxiv(query: str, max_results: int = 50) -> List:\n",
      "    base_url = \"http://export.arxiv.org/api/query?\"\n",
      "    search_query = f\"search_query=all:{query}\"\n",
      "    start = 0\n",
      "    max_results = f\"max_results={max_results}\"\n",
      "    url = f\"{base_url}{search_query}&start={start}&{max_results}\"\n",
      "    response = requests.get(url)\n",
      "    feed = feedparser.parse(response.content)\n",
      "    return feed.entries\n",
      "\n",
      "def analyze_abstracts(papers: List) -> Dict[str, int]:\n",
      "    stop_words = set(stopwords.words(\"english\"))\n",
      "    domains = []\n",
      "\n",
      "    for paper in papers:\n",
      "        abstract = paper.summary.lower()\n",
      "        tokens = word_tokenize(abstract)\n",
      "        filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
      "        domains.extend(filtered_tokens)\n",
      "\n",
      "    domain_counts = Counter(domains)\n",
      "    return dict(domain_counts.most_common(10))\n",
      "\n",
      "def generate_bar_chart(domains: Dict[str, int], output_file: str) -> None:\n",
      "    fig, ax = plt.subplots()\n",
      "    ax.bar(domains.keys(), domains.values())\n",
      "    plt.xticks(rotation=45, ha=\"right\")\n",
      "    plt.xlabel(\"Application Domains\")\n",
      "    plt.ylabel(\"Number of Papers\")\n",
      "    plt.title(\"Number of Papers per Application Domain\")\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(output_file)\n",
      "    plt.show()\n",
      "\n",
      "query = \"generative AI medicine\"\n",
      "papers = search_arxiv(query)\n",
      "domains = analyze_abstracts(papers)\n",
      "output_file = \"generative_ai_medicine_domains_bar_chart.png\"\n",
      "generate_bar_chart(domains, output_file)\n",
      "```\n",
      "\n",
      "This code will search for papers related to generative AI in medicine, analyze the abstracts to identify application domains, and generate a bar chart of the top 10 most common application domains and the number of papers in each domain. The chart will be saved as \"generative_ai_medicine_domains_bar_chart.png\".\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "  File \"\", line 1\n",
      "    !pip install nltk\n",
      "    ^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "I apologize for the confusion. The installation command for the nltk library should be executed in a shell script block. Please execute the following shell script block to install the nltk library:\n",
      "\n",
      "```sh\n",
      "pip install nltk\n",
      "```\n",
      "\n",
      "After installing the nltk library, execute the following Python code to search for papers, analyze the abstracts, and generate the bar chart:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import feedparser\n",
      "import matplotlib.pyplot as plt\n",
      "import nltk\n",
      "from typing import List, Dict\n",
      "from collections import Counter\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "nltk.download(\"punkt\")\n",
      "nltk.download(\"stopwords\")\n",
      "\n",
      "def search_arxiv(query: str, max_results: int = 50) -> List:\n",
      "    base_url = \"http://export.arxiv.org/api/query?\"\n",
      "    search_query = f\"search_query=all:{query}\"\n",
      "    start = 0\n",
      "    max_results = f\"max_results={max_results}\"\n",
      "    url = f\"{base_url}{search_query}&start={start}&{max_results}\"\n",
      "    response = requests.get(url)\n",
      "    feed = feedparser.parse(response.content)\n",
      "    return feed.entries\n",
      "\n",
      "def analyze_abstracts(papers: List) -> Dict[str, int]:\n",
      "    stop_words = set(stopwords.words(\"english\"))\n",
      "    domains = []\n",
      "\n",
      "    for paper in papers:\n",
      "        abstract = paper.summary.lower()\n",
      "        tokens = word_tokenize(abstract)\n",
      "        filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
      "        domains.extend(filtered_tokens)\n",
      "\n",
      "    domain_counts = Counter(domains)\n",
      "    return dict(domain_counts.most_common(10))\n",
      "\n",
      "def generate_bar_chart(domains: Dict[str, int], output_file: str) -> None:\n",
      "    fig, ax = plt.subplots()\n",
      "    ax.bar(domains.keys(), domains.values())\n",
      "    plt.xticks(rotation=45, ha=\"right\")\n",
      "    plt.xlabel(\"Application Domains\")\n",
      "    plt.ylabel(\"Number of Papers\")\n",
      "    plt.title(\"Number of Papers per Application Domain\")\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(output_file)\n",
      "    plt.show()\n",
      "\n",
      "query = \"generative AI medicine\"\n",
      "papers = search_arxiv(query)\n",
      "domains = analyze_abstracts(papers)\n",
      "output_file = \"generative_ai_medicine_domains_bar_chart.png\"\n",
      "generate_bar_chart(domains, output_file)\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/gaganbansal/miniconda3/envs/flaml4/lib/python3.9/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /Users/gaganbansal/miniconda3/envs/flaml4/lib/python3.9/site-packages (from nltk) (4.65.0)\n",
      "Installing collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1\n",
      "\n",
      "Figure(640x480)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "I'm glad the code executed successfully. The bar chart of the top 10 most common application domains in generative AI in medicine and the number of papers in each domain should have been displayed and saved as \"generative_ai_medicine_domains_bar_chart.png\". You can now view the chart to analyze the application domains.\n",
      "\n",
      "If you have any further questions or need assistance with another task, feel free to ask.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task2 = '''\n",
    "This recipe is available for you to reuse..\n",
    "\n",
    "<begin recipe>\n",
    "\n",
    "1. Search for relevant papers on arXiv using a query.\n",
    "2. Extract and analyze the abstracts of the papers to identify application domains.\n",
    "3. Count the number of papers in each application domain.\n",
    "4. Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\n",
    "\n",
    "To perform similar tasks in the future, I suggest the following well-documented, generalized Python functions:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import feedparser\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "\n",
    "def search_arxiv(query: str, max_results: int = 10) -> List:\n",
    "    \"\"\"Search for papers on arXiv using a query and return a list of papers.\"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    search_query = f\"search_query=all:{query}\"\n",
    "    start = 0\n",
    "    max_results = f\"max_results={max_results}\"\n",
    "    url = f\"{base_url}{search_query}&start={start}&{max_results}\"\n",
    "    response = requests.get(url)\n",
    "    feed = feedparser.parse(response.content)\n",
    "    return feed.entries\n",
    "\n",
    "def analyze_abstracts(papers: List) -> Dict[str, int]:\n",
    "    \"\"\"Analyze the abstracts of the papers and return a dictionary of application domains and their counts.\"\"\"\n",
    "    # Replace this with your own analysis function based on the specific task\n",
    "    pass\n",
    "\n",
    "def generate_bar_chart(domains: Dict[str, int], output_file: str) -> None:\n",
    "    \"\"\"Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(domains.keys(), domains.values())\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Application Domains\")\n",
    "    plt.ylabel(\"Number of Papers\")\n",
    "    plt.title(\"Number of Papers per Application Domain\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "query = \"trust calibration AI\"\n",
    "papers = search_arxiv(query)\n",
    "domains = analyze_abstracts(papers)  # Replace this with your own analysis function\n",
    "output_file = \"domains_bar_chart.png\"\n",
    "generate_bar_chart(domains, output_file)\n",
    "```\n",
    "\n",
    "You can use these functions as a starting point and modify them according to your specific needs. The `search_arxiv` function searches for papers on arXiv using a query, the `analyze_abstracts` function should be replaced with your own analysis function based on the specific task, and the `generate_bar_chart` function generates a bar chart of the application domains and the number of papers in each domain, saving it as an image file.\n",
    "</end recipe>\n",
    "\n",
    "\n",
    "Here is a new task:\n",
    "Plot a chart for application domains of generative AI in medicine\n",
    "'''\n",
    "user_proxy.initiate_chat(assistant, message=task2, clear_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "This recipe is available for you to reuse..\n",
      "\n",
      "<begin recipe>\n",
      "\n",
      "1. Search for relevant papers on arXiv using a query.\n",
      "2. Extract and analyze the abstracts of the papers to identify application domains.\n",
      "3. Count the number of papers in each application domain.\n",
      "4. Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\n",
      "\n",
      "To perform similar tasks in the future, I suggest the following well-documented, generalized Python functions:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import feedparser\n",
      "import matplotlib.pyplot as plt\n",
      "from typing import List, Dict\n",
      "\n",
      "def search_arxiv(query: str, max_results: int = 10) -> List:\n",
      "    \"\"\"Search for papers on arXiv using a query and return a list of papers.\"\"\"\n",
      "    base_url = \"http://export.arxiv.org/api/query?\"\n",
      "    search_query = f\"search_query=all:{query}\"\n",
      "    start = 0\n",
      "    max_results = f\"max_results={max_results}\"\n",
      "    url = f\"{base_url}{search_query}&start={start}&{max_results}\"\n",
      "    response = requests.get(url)\n",
      "    feed = feedparser.parse(response.content)\n",
      "    return feed.entries\n",
      "\n",
      "def analyze_abstracts(papers: List) -> Dict[str, int]:\n",
      "    \"\"\"Analyze the abstracts of the papers and return a dictionary of application domains and their counts.\"\"\"\n",
      "    # Replace this with your own analysis function based on the specific task\n",
      "    pass\n",
      "\n",
      "def generate_bar_chart(domains: Dict[str, int], output_file: str) -> None:\n",
      "    \"\"\"Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\"\"\"\n",
      "    fig, ax = plt.subplots()\n",
      "    ax.bar(domains.keys(), domains.values())\n",
      "    plt.xticks(rotation=45, ha=\"right\")\n",
      "    plt.xlabel(\"Application Domains\")\n",
      "    plt.ylabel(\"Number of Papers\")\n",
      "    plt.title(\"Number of Papers per Application Domain\")\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(output_file)\n",
      "    plt.show()\n",
      "\n",
      "# Example usage:\n",
      "query = \"trust calibration AI\"\n",
      "papers = search_arxiv(query)\n",
      "domains = analyze_abstracts(papers)  # Replace this with your own analysis function\n",
      "output_file = \"domains_bar_chart.png\"\n",
      "generate_bar_chart(domains, output_file)\n",
      "```\n",
      "\n",
      "You can use these functions as a starting point and modify them according to your specific needs. The `search_arxiv` function searches for papers on arXiv using a query, the `analyze_abstracts` function should be replaced with your own analysis function based on the specific task, and the `generate_bar_chart` function generates a bar chart of the application domains and the number of papers in each domain, saving it as an image file.\n",
      "</end recipe>\n",
      "\n",
      "\n",
      "Here is a new task:\n",
      "Plot a chart for application domains of large language models\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To plot a chart for application domains of large language models, we will follow the steps in the provided recipe. First, we will search for relevant papers on arXiv using a query related to large language models. Then, we will extract and analyze the abstracts of the papers to identify application domains. After that, we will count the number of papers in each application domain. Finally, we will generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\n",
      "\n",
      "Please note that the `analyze_abstracts` function in the recipe is a placeholder and should be replaced with your own analysis function based on the specific task. For this task, I will provide a simple analysis function that uses keyword matching to identify application domains. This function may not be perfect, but it should give you a starting point for your analysis.\n",
      "\n",
      "Here's the code to execute:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import feedparser\n",
      "import matplotlib.pyplot as plt\n",
      "from typing import List, Dict\n",
      "from collections import Counter\n",
      "import re\n",
      "\n",
      "def search_arxiv(query: str, max_results: int = 10) -> List:\n",
      "    base_url = \"http://export.arxiv.org/api/query?\"\n",
      "    search_query = f\"search_query=all:{query}\"\n",
      "    start = 0\n",
      "    max_results = f\"max_results={max_results}\"\n",
      "    url = f\"{base_url}{search_query}&start={start}&{max_results}\"\n",
      "    response = requests.get(url)\n",
      "    feed = feedparser.parse(response.content)\n",
      "    return feed.entries\n",
      "\n",
      "def analyze_abstracts(papers: List) -> Dict[str, int]:\n",
      "    domains = []\n",
      "    keywords = [\"natural language processing\", \"computer vision\", \"speech recognition\", \"reinforcement learning\", \"translation\", \"summarization\", \"question answering\", \"sentiment analysis\", \"image recognition\", \"object detection\"]\n",
      "\n",
      "    for paper in papers:\n",
      "        abstract = paper.summary.lower()\n",
      "        for keyword in keywords:\n",
      "            if keyword in abstract:\n",
      "                domains.append(keyword)\n",
      "\n",
      "    return Counter(domains)\n",
      "\n",
      "def generate_bar_chart(domains: Dict[str, int], output_file: str) -> None:\n",
      "    fig, ax = plt.subplots()\n",
      "    ax.bar(domains.keys(), domains.values())\n",
      "    plt.xticks(rotation=45, ha=\"right\")\n",
      "    plt.xlabel(\"Application Domains\")\n",
      "    plt.ylabel(\"Number of Papers\")\n",
      "    plt.title(\"Number of Papers per Application Domain\")\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(output_file)\n",
      "    plt.show()\n",
      "\n",
      "query = \"large language models\"\n",
      "papers = search_arxiv(query, max_results=50)\n",
      "domains = analyze_abstracts(papers)\n",
      "output_file = \"large_language_models_domains_bar_chart.png\"\n",
      "generate_bar_chart(domains, output_file)\n",
      "```\n",
      "\n",
      "This code will search for 50 papers related to large language models, analyze their abstracts to identify application domains, and generate a bar chart of the application domains and the number of papers in each domain. The chart will be saved as \"large_language_models_domains_bar_chart.png\".\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Figure(640x480)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "I'm glad the code executed successfully and generated the bar chart for application domains of large language models. The chart should be saved as \"large_language_models_domains_bar_chart.png\" in your current working directory. You can now view the chart to analyze the distribution of papers across different application domains.\n",
      "\n",
      "If you have any further questions or need assistance with another task, feel free to ask.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task3 = '''\n",
    "This recipe is available for you to reuse..\n",
    "\n",
    "<begin recipe>\n",
    "\n",
    "1. Search for relevant papers on arXiv using a query.\n",
    "2. Extract and analyze the abstracts of the papers to identify application domains.\n",
    "3. Count the number of papers in each application domain.\n",
    "4. Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\n",
    "\n",
    "To perform similar tasks in the future, I suggest the following well-documented, generalized Python functions:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import feedparser\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "\n",
    "def search_arxiv(query: str, max_results: int = 10) -> List:\n",
    "    \"\"\"Search for papers on arXiv using a query and return a list of papers.\"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    search_query = f\"search_query=all:{query}\"\n",
    "    start = 0\n",
    "    max_results = f\"max_results={max_results}\"\n",
    "    url = f\"{base_url}{search_query}&start={start}&{max_results}\"\n",
    "    response = requests.get(url)\n",
    "    feed = feedparser.parse(response.content)\n",
    "    return feed.entries\n",
    "\n",
    "def analyze_abstracts(papers: List) -> Dict[str, int]:\n",
    "    \"\"\"Analyze the abstracts of the papers and return a dictionary of application domains and their counts.\"\"\"\n",
    "    # Replace this with your own analysis function based on the specific task\n",
    "    pass\n",
    "\n",
    "def generate_bar_chart(domains: Dict[str, int], output_file: str) -> None:\n",
    "    \"\"\"Generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(domains.keys(), domains.values())\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Application Domains\")\n",
    "    plt.ylabel(\"Number of Papers\")\n",
    "    plt.title(\"Number of Papers per Application Domain\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "query = \"trust calibration AI\"\n",
    "papers = search_arxiv(query)\n",
    "domains = analyze_abstracts(papers)  # Replace this with your own analysis function\n",
    "output_file = \"domains_bar_chart.png\"\n",
    "generate_bar_chart(domains, output_file)\n",
    "```\n",
    "\n",
    "You can use these functions as a starting point and modify them according to your specific needs. The `search_arxiv` function searches for papers on arXiv using a query, the `analyze_abstracts` function should be replaced with your own analysis function based on the specific task, and the `generate_bar_chart` function generates a bar chart of the application domains and the number of papers in each domain, saving it as an image file.\n",
    "</end recipe>\n",
    "\n",
    "\n",
    "Here is a new task:\n",
    "Plot a chart for application domains of large language models\n",
    "'''\n",
    "user_proxy.initiate_chat(assistant, message=task3, clear_history=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
