{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/autogen_agentchat_groupchat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Group Chat\n",
    "\n",
    "`flaml.autogen` offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framwork allows tool use and human participance through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/FLAML/docs/Use-Cases/Autogen#agents).\n",
    "\n",
    "This notebook is modified based on https://github.com/microsoft/FLAML/blob/4ea686af5c3e8ff24d9076a7a626c8b28ab5b1d7/notebook/autogen_multiagent_roleplay_chat.ipynb\n",
    "\n",
    "## Requirements\n",
    "\n",
    "FLAML requires `Python>=3.8`. To run this notebook example, please install flaml with the [autogen] option:\n",
    "```bash\n",
    "pip install flaml[autogen]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install flaml[autogen]~=2.0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/FLAML/docs/reference/autogen/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import autogen\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "# config_list_gpt35 = autogen.config_list_from_json(\n",
    "#     \"OAI_CONFIG_LIST\",\n",
    "#     filter_dict={\n",
    "#         \"model\": {\n",
    "#             \"gpt-3.5-turbo\",\n",
    "#             \"gpt-3.5-turbo-16k\",\n",
    "#             \"gpt-3.5-turbo-0301\",\n",
    "#             \"chatgpt-35-turbo-0301\",\n",
    "#             \"gpt-35-turbo-v0301\",\n",
    "#         },\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\"config_list\": config_list_gpt4}\n",
    "human = autogen.UserProxyAgent(\n",
    "   name=\"Human\",\n",
    "   system_message=\"A human admin.\",\n",
    "   code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    ")\n",
    "alice = autogen.AssistantAgent(\n",
    "    name=\"Alice\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "bob = autogen.AssistantAgent(\n",
    "    name=\"Bob\",\n",
    "    system_message=\"Code and answer reviewer.\"\n",
    "    \"For code, prevent code execution if unsafe or missing important details, e.g., sort order in arxiv API. Suggest changes. Otherwise, approve and return the final code to execute.\"\n",
    "    \"For answer, carefully check the interpretation of code result and fix any errors. If the interpretation is correct, approve and return the final answer to the user.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "groupchat = autogen.GroupChat(agents=[human, alice, bob], messages=[], max_round=12)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mHuman\u001b[0m (to chat_manager):\n",
      "\n",
      "find a latest paper about gpt-4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAlice\u001b[0m (to chat_manager):\n",
      "\n",
      "To find the latest paper about GPT-4, we can use the arXiv API, an interface for accessing arXiv's raw metadata. We send a query specifying the subject - in this case 'GPT-4' - and retrieve the list of papers about this subject. From that list, we select the most recent paper.\n",
      "\n",
      "Step 1: I will use Python code to send an HTTP request to the arXiv API, requesting papers about 'GPT-4'. I will print the raw results.\n",
      "\n",
      "Step 2: I will examine the results, which are XML files containing metadata about each paper. Normally, the papers are ordered with the most recently updated paper coming first, so the latest paper about GPT-4 should be at the top of the list.\n",
      "\n",
      "The Python code to execute this task is:\n",
      "\n",
      "```python\n",
      "import feedparser\n",
      "\n",
      "def fetch_papers(search_query='gpt-4', max_results=10):\n",
      "    base_url = 'http://export.arxiv.org/api/query?'\n",
      "    query = f'search_query=all:{search_query}&start=0&max_results={max_results}'\n",
      "    url = base_url + query\n",
      "    feed = feedparser.parse(url)\n",
      "    \n",
      "    for entry in feed.entries:\n",
      "        print(f'Title: {entry.title}')\n",
      "        print(f'Author: {\", \".join(author.name for author in entry.authors)}')\n",
      "        print(f'Link: {entry.link}')\n",
      "        print('--------------------------------------------------')\n",
      "\n",
      "fetch_papers()\n",
      "```\n",
      "\n",
      "When executed, this code will fetch and print information about the top 10 most recent papers related to GPT-4 from the arXiv repository.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mBob\u001b[0m (to chat_manager):\n",
      "\n",
      "The Python code is generally correct but missing one important detail. The papers returned by the arXiv API are not guaranteed to be in the order of publication. Therefore, we should enhance the Python code to include an explicit sort order in the query string.\n",
      "\n",
      "Here is the updated Python code:\n",
      "\n",
      "```python\n",
      "import feedparser\n",
      "\n",
      "def fetch_papers(search_query='gpt-4', max_results=10):\n",
      "    base_url = 'http://export.arxiv.org/api/query?'\n",
      "    query = f'search_query=all:{search_query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\n",
      "    url = base_url + query\n",
      "    feed = feedparser.parse(url)\n",
      "    \n",
      "    for entry in feed.entries:\n",
      "        print(f'Title: {entry.title}')\n",
      "        print(f'Author: {\", \".join(author.name for author in entry.authors)}')\n",
      "        print(f'Link: {entry.link}')\n",
      "        print(f'Published Date: {entry.published}')\n",
      "        print('--------------------------------------------------')\n",
      "\n",
      "fetch_papers()\n",
      "```\n",
      "\n",
      "This updated code now requests that the API sort the results by submitted date, in descending order, guaranteeing that the most recent papers appear first. Also, it prints out the published date of each paper.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mHuman\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: Time Travel in LLMs: Tracing Data Contamination in Large Language Models\n",
      "Author: Shahriar Golchin, Mihai Surdeanu\n",
      "Link: http://arxiv.org/abs/2308.08493v1\n",
      "Published Date: 2023-08-16T16:48:57Z\n",
      "--------------------------------------------------\n",
      "Title: Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with\n",
      "  Code-based Self-Verification\n",
      "Author: Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li\n",
      "Link: http://arxiv.org/abs/2308.07921v1\n",
      "Published Date: 2023-08-15T17:58:45Z\n",
      "--------------------------------------------------\n",
      "Title: Neural Authorship Attribution: Stylometric Analysis on Large Language\n",
      "  Models\n",
      "Author: Tharindu Kumarage, Huan Liu\n",
      "Link: http://arxiv.org/abs/2308.07305v1\n",
      "Published Date: 2023-08-14T17:46:52Z\n",
      "--------------------------------------------------\n",
      "Title: Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt\n",
      "  Optimization for Few-shot Learning\n",
      "Author: Chengzhengxu Li, Xiaoming Liu, Yichen Wang, Duyi Li, Yu Lan, Chao Shen\n",
      "Link: http://arxiv.org/abs/2308.07272v1\n",
      "Published Date: 2023-08-14T16:58:50Z\n",
      "--------------------------------------------------\n",
      "Title: Large Language Models for Information Retrieval: A Survey\n",
      "Author: Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, Ji-Rong Wen\n",
      "Link: http://arxiv.org/abs/2308.07107v2\n",
      "Published Date: 2023-08-14T12:47:22Z\n",
      "--------------------------------------------------\n",
      "Title: Chatbots in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug\n",
      "  Development with ChatGPT\n",
      "Author: Rui Wang, Hongsong Feng, Guo-Wei Wei\n",
      "Link: http://arxiv.org/abs/2308.06920v1\n",
      "Published Date: 2023-08-14T03:43:57Z\n",
      "--------------------------------------------------\n",
      "Title: VisIT-Bench: A Benchmark for Vision-Language Instruction Following\n",
      "  Inspired by Real-World Use\n",
      "Author: Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, Ludwig Schimdt\n",
      "Link: http://arxiv.org/abs/2308.06595v1\n",
      "Published Date: 2023-08-12T15:27:51Z\n",
      "--------------------------------------------------\n",
      "Title: GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher\n",
      "Author: Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, Zhaopeng Tu\n",
      "Link: http://arxiv.org/abs/2308.06463v1\n",
      "Published Date: 2023-08-12T04:05:57Z\n",
      "--------------------------------------------------\n",
      "Title: Assessing Student Errors in Experimentation Using Artificial\n",
      "  Intelligence and Large Language Models: A Comparative Study with Human Raters\n",
      "Author: Arne Bewersdorff, Kathrin Seßler, Armin Baur, Enkelejda Kasneci, Claudia Nerdel\n",
      "Link: http://arxiv.org/abs/2308.06088v1\n",
      "Published Date: 2023-08-11T12:03:12Z\n",
      "--------------------------------------------------\n",
      "Title: Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math\n",
      "  and science problems\n",
      "Author: Ernest Davis, Scott Aaronson\n",
      "Link: http://arxiv.org/abs/2308.05713v2\n",
      "Published Date: 2023-08-10T17:22:28Z\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mBob\u001b[0m (to chat_manager):\n",
      "\n",
      "The query to the arXiv API worked as expected and printed the top 10 most recent papers regarding 'GPT-4'. The list includes the paper's title, author, link to the paper on the arXiv website, and the date it was published. As the dates are returned in descending order – the most recent paper is displayed first. \n",
      "\n",
      "The most recent paper about GPT-4 in the output is:\n",
      "\n",
      "\"Time Travel in LLMs: Tracing Data Contamination in Large Language Models\" by Shahriar Golchin and Mihai Surdeanu, published on 2023-08-16. \n",
      "\n",
      "You can read the paper by following the provided link: [http://arxiv.org/abs/2308.08493v1](http://arxiv.org/abs/2308.08493v1)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mHuman\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAlice\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "human.initiate_chat(manager, message=\"find a latest paper about gpt-4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
